{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rough-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import \n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-intro",
   "metadata": {},
   "source": [
    "I assume that the candidate generation and feature genration has already be run on the training and dev tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accredited-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "##File paths\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/t2dv2-train-lof-original-tfidf\n",
    "train_path = '../data/t2dv2-train-lof-original-tfidf/'\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/t2dv2-dev-lof-original-tfidf\n",
    "dev_path = '../data/t2dv2-dev-lof-original-tfidf/'\n",
    "\n",
    "#Note: One would need just the train_path and dev_path to run this notebook. The below files are output after \n",
    "# running the notebook.\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/pos_features.pkl\n",
    "pos_output = '../data/training_data/pos_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/neg_features.pkl\n",
    "neg_output = '../data/training_data/neg_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/normalization_factor.pkl\n",
    "min_max_scaler_path = '../data/training_data/normalization_factor.pkl'\n",
    "dev_output_predictions = '../data/dev_predictions/'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/saved_models\n",
    "model_save_path = '../data/saved_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-melbourne",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "broke-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    eval_file_names = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(datapath):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = dirpath + fn\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    df_list = []\n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler\n",
    "\n",
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "            'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank','evaluation_label']\n",
    "    normalize_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(file_name)\n",
    "        d_sample = pd.read_csv(file)\n",
    "        grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            num_rows = 64\n",
    "            pos_row = a[features].drop('evaluation_label',axis=1)\n",
    "            negatives_filtered = cell[1][cell[1]['evaluation_label'] == -1]\n",
    "            sorted_df = negatives_filtered.sort_values('lof-graph-embedding-score',ascending=False)\n",
    "            sorted_df = sorted_df[features]\n",
    "            if 0 in sorted_df['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if sorted_df.empty:\n",
    "                continue\n",
    "            neg_list = []\n",
    "            if num_rows < len(sorted_df):\n",
    "                sorted_df = sorted_df[sorted_df['evaluation_label'] == -1]\n",
    "                neg_list.append(sorted_df[:2])\n",
    "                retrieval_score_df = sorted_df[2:].sort_values('retrieval_score',ascending=False)\n",
    "                neg_list.append(retrieval_score_df[:2])\n",
    "                pagerank_score_df = retrieval_score_df[2:].sort_values('pagerank', ascending=False)\n",
    "                neg_list.append(pagerank_score_df[:2])\n",
    "                class_count_score_df = pagerank_score_df[2:].sort_values('lof_class_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(class_count_score_df[:2])\n",
    "                prop_count_score_df = class_count_score_df[2:].sort_values('lof_property_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(prop_count_score_df[:2])\n",
    "                monge_elkan_score_df = prop_count_score_df[2:].sort_values('monge_elkan', ascending=False)\n",
    "                neg_list.append(monge_elkan_score_df[:2])\n",
    "                jaro_winkler_score_df = monge_elkan_score_df[2:].sort_values('jaro_winkler', ascending=False)\n",
    "                neg_list.append(jaro_winkler_score_df[:2])\n",
    "                top_sample_df = jaro_winkler_score_df.sample(n=50)\n",
    "                neg_list.append(top_sample_df)\n",
    "                top_sample_df = pd.concat(neg_list)\n",
    "                top_sample_df.drop('evaluation_label', inplace=True, axis=1)\n",
    "                top_sample_arr = top_sample_df.to_numpy()\n",
    "            \n",
    "            for i in range(num_rows):\n",
    "                neg_features.append(top_sample_arr[i])\n",
    "            random.shuffle(neg_features)\n",
    "            for i in range(num_rows):\n",
    "                pos_row_sample = pos_row.sample(n=1)\n",
    "                ar = pos_row_sample.to_numpy()\n",
    "                for ps_ar in ar:\n",
    "                    pos_features.append(ps_ar)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "    print(len(positive_features_final), len(positive_features_final[37]))\n",
    "    print(len(negative_features_final), len(negative_features_final[37]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cordless-premises",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n",
      "5617 64\n",
      "5617 64\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "generate_train_data(gen_training_data_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-piece",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ambient-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-interaction",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unsigned-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']\n",
    "    for file in glob.glob(input_table_path + '*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name != '52299421_0_4473286348258170200.csv':\n",
    "            print(file_name)\n",
    "            d_sample = pd.read_csv(file)\n",
    "            grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "            new_df_list = []\n",
    "            pred = []\n",
    "            for cell in grouped_obj:\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                sorted_df = cell[1].sort_values('lof-graph-embedding-score',ascending=False)[:64]\n",
    "                sorted_df_features = sorted_df[normalize_features]\n",
    "                new_df_list.append(sorted_df)\n",
    "                arr = sorted_df_features.to_numpy()\n",
    "                test_inp = []\n",
    "                for a in arr:\n",
    "                    test_inp.append(a)\n",
    "                test_tensor = torch.tensor(test_inp).float()\n",
    "                scores = model.predict(test_tensor)\n",
    "                pred.extend(torch.squeeze(scores).tolist())\n",
    "            test_df = pd.concat(new_df_list)\n",
    "            test_df['siamese_pred'] = pred\n",
    "            test_df.to_csv(os.path.join(output_table_path, file_name), index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(14).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, 'siamese_pred')\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "infrared-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if fn != '52299421_0_4473286348258170200.csv':\n",
    "                if \"csv\" not in fn:\n",
    "                    continue\n",
    "                abs_fn = os.path.join(dirpath, fn)\n",
    "                assert os.path.isfile(abs_fn)\n",
    "                if os.path.getsize(abs_fn) == 0:\n",
    "                    continue\n",
    "                eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'row', 'column'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "        s_data = c_e_data.sort_values(by=['lof-graph-embedding-score'], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_graph_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "        cf_e_data = c_e_data.copy()\n",
    "        cf_e_data['lof-graph-embedding-score'] = cf_e_data['lof-graph-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "    candidate_eval_data['lof-graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "    candidate_eval_data['lof-graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['lof-graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "    res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "    res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "    res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "educational-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "architectural-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "5617it [00:08, 684.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "122it [00:00, 612.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.13559077680110931, epoch top1 0.806497175141243, max top1 0.806497175141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 665.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "60it [00:00, 593.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.09761583060026169, epoch top1 0.8983050847457628, max top1 0.8983050847457628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 696.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "63it [00:00, 622.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.09150885790586472, epoch top1 0.9067796610169492, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 678.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "62it [00:00, 614.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss is 0.08903484046459198, epoch top1 0.8954802259887006, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 723.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "110it [00:00, 547.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.08610525727272034, epoch top1 0.8615819209039548, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 708.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "60it [00:00, 594.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.08848699927330017, epoch top1 0.8898305084745762, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 629.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "146it [00:00, 725.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.0917152687907219, epoch top1 0.827683615819209, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 716.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "142it [00:00, 712.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.0888967290520668, epoch top1 0.8912429378531074, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 734.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "67it [00:00, 666.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.0880441963672638, epoch top1 0.7146892655367232, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 711.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "131it [00:00, 658.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.08709308505058289, epoch top1 0.8375706214689266, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 724.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "73it [00:00, 723.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.0919279009103775, epoch top1 0.7556497175141242, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 730.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "68it [00:00, 672.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.09227693825960159, epoch top1 0.7966101694915254, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 730.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "63it [00:00, 622.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.08993757516145706, epoch top1 0.7401129943502824, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 664.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "112it [00:00, 560.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.08395981043577194, epoch top1 0.7274011299435028, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 702.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "149it [00:00, 740.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.09642603993415833, epoch top1 0.7415254237288136, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 722.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "56it [00:00, 559.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.08883008360862732, epoch top1 0.6850282485875706, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:09, 587.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "72it [00:00, 713.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.08726709336042404, epoch top1 0.7740112994350282, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:09, 619.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "67it [00:00, 666.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.09002918004989624, epoch top1 0.7259887005649718, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:07, 726.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "142it [00:00, 709.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.09428320080041885, epoch top1 0.730225988700565, max top1 0.9067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [00:08, 663.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 19, Avg Loss is 0.09280379116535187, epoch top1 0.7556497175141242, max top1 0.9067796610169492\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "train(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-small",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
