{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "concrete-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-binding",
   "metadata": {},
   "source": [
    "I assume that the candidate generation and feature genration has already be run on the training and dev tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "israeli-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented'\n",
    "\n",
    "# Input Paths\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = \"/Users/amandeep/Github/table-linker/data/SemTabR4_T2dv2/train-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = \"/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-dev-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = \"/Users/amandeep/Github/table-linker/data/SemTabR4_T2dv2/GT\"\n",
    "\n",
    "# can be downloaded from https://github.com/usc-isi-i2/table-linker-pipelines/blob/main/table-linker-full-pipeline/models/weighted_lr.pkl\n",
    "classifier_model_path = '/Users/amandeep/Github/table-linker-pipelines/table-linker-full-pipeline/models/weighted_lr.pkl'\n",
    "\n",
    "\n",
    "# OUTPUT PATHS\n",
    "output_path = \"/Users/amandeep/Github/table-linker/data/SemTabR4_T2dv2/table-linker\"\n",
    "train_output_path = f'{output_path}/train1-output'\n",
    "dev_output_path = f'{output_path}/dev-output'\n",
    "\n",
    "# increase version to create a new folder for an experiment\n",
    "VERSION = \"7_0\"\n",
    "\n",
    "train_candidate_path = f'{train_output_path}/{VERSION}/candidates'\n",
    "train_feature_path = f'{train_output_path}/{VERSION}/features'\n",
    "\n",
    "dev_candidate_path = f'{dev_output_path}/{VERSION}/candidates'\n",
    "dev_feature_path = f'{dev_output_path}/{VERSION}/features'\n",
    "dev_output_predictions = f'{dev_output_path}/{VERSION}/dev_predictions'\n",
    "dev_predictions_top_k = f'{dev_output_path}/{VERSION}/dev_predictions_top_k'\n",
    "dev_colorized_path = f'{dev_output_path}/{VERSION}/dev_predictions_colorized'\n",
    "dev_metrics_path = f'{dev_output_path}/{VERSION}/dev_predictions_metrics'\n",
    "\n",
    "aux_field = 'graph_embedding_complex,class_count,property_count,context'\n",
    "\n",
    "\n",
    "train_prop_count = f'{train_output_path}/{VERSION}/train_prop_count' \n",
    "train_class_count = f'{train_output_path}/{VERSION}/train_class_count'\n",
    "train_context_path = f'{train_output_path}/{VERSION}/train_context'\n",
    "train_graph_embedding = f'{train_output_path}/{VERSION}/train_graph_embedding'\n",
    "\n",
    "dev_prop_count = f'{dev_output_path}/{VERSION}/dev_prop_count'\n",
    "dev_class_count = f'{dev_output_path}/{VERSION}/dev_class_count'\n",
    "dev_context_path = f'{dev_output_path}/{VERSION}/dev_context'\n",
    "dev_graph_embedding = f'{dev_output_path}/{VERSION}/dev_graph_embedding'\n",
    "\n",
    "temp_dir = f'{output_path}/temp'\n",
    "\n",
    "pos_output = f'{temp_dir}/training_data/pos_features.pkl'\n",
    "neg_output = f'{temp_dir}/training_data/neg_features.pkl'\n",
    "min_max_scaler_path = f'{temp_dir}/training_data/normalization_factor.pkl'\n",
    "\n",
    "final_score_column = 'siamese_prediction'\n",
    "\n",
    "model_save_path = f'{dev_output_path}/{VERSION}/saved_models'\n",
    "best_model_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "behind-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$temp_dir\"\n",
    "\n",
    "!mkdir -p \"$train_prop_count\"\n",
    "!mkdir -p \"$dev_prop_count\"\n",
    "!mkdir -p \"$train_class_count\"\n",
    "!mkdir -p \"$dev_class_count\"\n",
    "!mkdir -p \"$train_graph_embedding\"\n",
    "!mkdir -p \"$dev_graph_embedding\"\n",
    "!mkdir -p \"$train_context_path\"\n",
    "!mkdir -p \"$dev_context_path\"\n",
    "\n",
    "!mkdir -p \"$train_candidate_path\"\n",
    "!mkdir -p \"$dev_candidate_path\"\n",
    "\n",
    "!mkdir -p \"$train_feature_path\"\n",
    "!mkdir -p \"$dev_feature_path\"\n",
    "\n",
    "!mkdir -p \"$temp_dir/training_data\"\n",
    "!mkdir -p \"$dev_output_predictions\"\n",
    "!mkdir -p \"$model_save_path\"\n",
    "!mkdir -p \"$dev_predictions_top_k\"\n",
    "!mkdir -p \"$dev_colorized_path\"\n",
    "!mkdir -p \"$dev_metrics_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "played-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pagerank','retrieval_score','monge_elkan','monge_elkan_aliases','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank', 'context_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "round-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_features = ['aligned_pagerank', 'smallest_qnode_number', 'monge_elkan', 'des_cont_jaccard_normalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-disorder",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electronic-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    file_list = glob.glob(path + '/*.csv')\n",
    "    for i, file in enumerate(file_list):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        print(f\"{filename}: {i+1} of {len(file_list)}\")\n",
    "        gt_file = f\"{ground_truth_files}/{filename}\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "        !tl clean -c label -o label_clean \"$file\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        ground-truth-labeler --gt-file \"$gt_file\" > \"$output_file\"\n",
    "        \n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = f\"{class_count_path}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = f\"{prop_count_path}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            elif field == 'context':\n",
    "                context_file = f\"{context_path}/{filename.strip('.csv')}_context.tsv\"\n",
    "                aux_df.to_csv(context_file, sep='\\t', index=False)\n",
    "            else:\n",
    "                graph_embedding_file = f\"{graph_embedding}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        \n",
    "        print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pharmaceutical-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv: 1 of 144\n",
      "17.910139799118042\n",
      "ZX8GERJC.csv: 2 of 144\n",
      "10.2502760887146\n",
      "8ZD74BO9.csv: 3 of 144\n",
      "10.760577917098999\n",
      "W0ZNF869.csv: 4 of 144\n",
      "13.971844911575317\n",
      "AM1UELOJ.csv: 5 of 144\n",
      "15.298548936843872\n",
      "39173938_0_7916056990138658530.csv: 6 of 144\n",
      "17.793717861175537\n",
      "5IXA0RAI.csv: 7 of 144\n",
      "9.488677978515625\n",
      "8EFC5XVR.csv: 8 of 144\n",
      "13.608717918395996\n",
      "DPUA686B.csv: 9 of 144\n",
      "11.279143810272217\n",
      "UMMA6HQO.csv: 10 of 144\n",
      "10.795647859573364\n",
      "ERPSWFMM.csv: 11 of 144\n",
      "10.068063020706177\n",
      "ZDAZ5PQ5.csv: 12 of 144\n",
      "12.091837167739868\n",
      "XF412HIL.csv: 13 of 144\n",
      "11.77834701538086\n",
      "BQ36GYQE.csv: 14 of 144\n",
      "12.048776865005493\n",
      "CKRLO13X.csv: 15 of 144\n",
      "17.457324981689453\n",
      "L5LFLQIN.csv: 16 of 144\n",
      "13.57371711730957\n",
      "J6SSKET3.csv: 17 of 144\n",
      "15.519392967224121\n",
      "10579449_0_1681126353774891032.csv: 18 of 144\n",
      "12.090851068496704\n",
      "T8SL8HGK.csv: 19 of 144\n",
      "13.934154987335205\n",
      "JUFYSXYP.csv: 20 of 144\n",
      "11.202039003372192\n",
      "CYYO69JB.csv: 21 of 144\n",
      "11.311702013015747\n",
      "YMHERMQV.csv: 22 of 144\n",
      "16.690694093704224\n",
      "6XCOGRWM.csv: 23 of 144\n",
      "14.148561954498291\n",
      "WNKF57RH.csv: 24 of 144\n",
      "9.032230854034424\n",
      "33401079_0_9127583903019856402.csv: 25 of 144\n",
      "21.992264986038208\n",
      "21362676_0_6854186738074119688.csv: 26 of 144\n",
      "22.436158895492554\n",
      "38428277_0_1311643810102462607.csv: 27 of 144\n",
      "22.827793836593628\n",
      "OMJX8TT6.csv: 28 of 144\n",
      "14.595085144042969\n",
      "IUBTQXYO.csv: 29 of 144\n",
      "11.43275499343872\n",
      "0XXGVKA8.csv: 30 of 144\n",
      "11.815274000167847\n",
      "57681CMM.csv: 31 of 144\n",
      "14.628245115280151\n",
      "VE3T1LHT.csv: 32 of 144\n",
      "13.595762252807617\n",
      "4KGRZFTI.csv: 33 of 144\n",
      "8.963340997695923\n",
      "UU8Q91MG.csv: 34 of 144\n",
      "13.99090313911438\n",
      "75MLA4XJ.csv: 35 of 144\n",
      "13.46780014038086\n",
      "U5L8U1OL.csv: 36 of 144\n",
      "16.10128378868103\n",
      "QDJ86U5I.csv: 37 of 144\n",
      "13.668212890625\n",
      "384SR1N3.csv: 38 of 144\n",
      "12.165132999420166\n",
      "PG0TP6O0.csv: 39 of 144\n",
      "15.507015943527222\n",
      "VFVMRNF9.csv: 40 of 144\n",
      "11.570871829986572\n",
      "CCCNRESE.csv: 41 of 144\n",
      "10.065340042114258\n",
      "7ZQB5C2O.csv: 42 of 144\n",
      "14.196595907211304\n",
      "LTZQIN2R.csv: 43 of 144\n",
      "19.117419719696045\n",
      "QID3PSI3.csv: 44 of 144\n",
      "15.768701076507568\n",
      "NXBVTACX.csv: 45 of 144\n",
      "16.29235005378723\n",
      "DKRE7U28.csv: 46 of 144\n",
      "13.773412704467773\n",
      "91959037_0_7907661684242014480.csv: 47 of 144\n",
      "53.18866729736328\n",
      "U8BHYWZ7.csv: 48 of 144\n",
      "15.064220190048218\n",
      "2389HYHH.csv: 49 of 144\n",
      "10.679088354110718\n",
      "2LM6W2JV.csv: 50 of 144\n",
      "6.621946096420288\n",
      "B735JU5L.csv: 51 of 144\n",
      "15.207002878189087\n",
      "20135078_0_7570343137119682530.csv: 52 of 144\n",
      "20.37350082397461\n",
      "6LSIYDYN.csv: 53 of 144\n",
      "15.212893962860107\n",
      "S2LGMOGC.csv: 54 of 144\n",
      "13.511904954910278\n",
      "SVPAISS7.csv: 55 of 144\n",
      "14.19212293624878\n",
      "0H0U54UZ.csv: 56 of 144\n",
      "11.766384840011597\n",
      "4W3919II.csv: 57 of 144\n",
      "10.448611974716187\n",
      "AGCHY9JJ.csv: 58 of 144\n",
      "10.414583206176758\n",
      "G2K4GSYB.csv: 59 of 144\n",
      "13.089877843856812\n",
      "RBIVOB6Q.csv: 60 of 144\n",
      "15.947922945022583\n",
      "2JN1R1VW.csv: 61 of 144\n",
      "13.318329811096191\n",
      "6NO3AH02.csv: 62 of 144\n",
      "13.179803848266602\n",
      "ABRT6AWH.csv: 63 of 144\n",
      "10.540617942810059\n",
      "KUN2Y3DX.csv: 64 of 144\n",
      "15.568418979644775\n",
      "35188621_0_6058553107571275232.csv: 65 of 144\n",
      "19.584126234054565\n",
      "24VU5BR7.csv: 66 of 144\n",
      "16.43827486038208\n",
      "PQN3CY7B.csv: 67 of 144\n",
      "16.15336513519287\n",
      "NA24I27F.csv: 68 of 144\n",
      "12.74208116531372\n",
      "9SERGNIZ.csv: 69 of 144\n",
      "15.69934892654419\n",
      "R4K6322V.csv: 70 of 144\n",
      "10.270381212234497\n",
      "5W99BCM4.csv: 71 of 144\n",
      "11.241222858428955\n",
      "7XB008OM.csv: 72 of 144\n",
      "14.043193101882935\n",
      "6VLKFW8J.csv: 73 of 144\n",
      "11.896378993988037\n",
      "HFRDW66L.csv: 74 of 144\n",
      "11.251761198043823\n",
      "54719588_0_8417197176086756912.csv: 75 of 144\n",
      "40.58233714103699\n",
      "3IB68W0T.csv: 76 of 144\n",
      "18.747722864151\n",
      "21245481_0_8730460088443117515.csv: 77 of 144\n",
      "24.403494834899902\n",
      "JZ9RW99R.csv: 78 of 144\n",
      "12.179041862487793\n",
      "RDNZHHGI.csv: 79 of 144\n",
      "13.01440691947937\n",
      "MOX6MBH5.csv: 80 of 144\n",
      "18.4753258228302\n",
      "0ZH7HCT0.csv: 81 of 144\n",
      "12.44511103630066\n",
      "OJEI7G4L.csv: 82 of 144\n",
      "15.542546033859253\n",
      "6ED9WFUN.csv: 83 of 144\n",
      "15.823859930038452\n",
      "71840765_0_6664391841933033844.csv: 84 of 144\n",
      "12.976773262023926\n",
      "E0LR4TZL.csv: 85 of 144\n",
      "12.581295013427734\n",
      "YS86QOSL.csv: 86 of 144\n",
      "15.041555881500244\n",
      "T3W112BN.csv: 87 of 144\n",
      "12.647341012954712\n",
      "RLMB7HEB.csv: 88 of 144\n",
      "12.817743062973022\n",
      "GX0WEFG7.csv: 89 of 144\n",
      "13.266595125198364\n",
      "8468806_0_4382447409703007384.csv: 90 of 144\n",
      "19.561516046524048\n",
      "5DNR9AW5.csv: 91 of 144\n",
      "12.166912078857422\n",
      "88523363_0_8180214313099580515.csv: 92 of 144\n",
      "59.826717376708984\n",
      "3TIMOEBD.csv: 93 of 144\n",
      "12.712872982025146\n",
      "OXLTY5IY.csv: 94 of 144\n",
      "7.451818943023682\n",
      "29414811_13_8724394428539174350.csv: 95 of 144\n",
      "8.211944818496704\n",
      "MBM31U4C.csv: 96 of 144\n",
      "15.25664210319519\n",
      "0MZX65PH.csv: 97 of 144\n",
      "11.872192859649658\n",
      "Y3XLQHGC.csv: 98 of 144\n",
      "17.591650009155273\n",
      "2UETZ4XK.csv: 99 of 144\n",
      "14.437839031219482\n",
      "OAWHF5BM.csv: 100 of 144\n",
      "12.612752914428711\n",
      "3YNPHVPV.csv: 101 of 144\n",
      "13.215863943099976\n",
      "YXYVNO79.csv: 102 of 144\n",
      "11.695104122161865\n",
      "1XNHBBRZ.csv: 103 of 144\n",
      "10.931010961532593\n",
      "M0XIN8I8.csv: 104 of 144\n",
      "11.024953126907349\n",
      "QIBT0IBA.csv: 105 of 144\n",
      "6.63821816444397\n",
      "SY4CRLEA.csv: 106 of 144\n",
      "15.953338146209717\n",
      "SU8B1A6K.csv: 107 of 144\n",
      "18.38055396080017\n",
      "XVMPJ993.csv: 108 of 144\n",
      "13.10664701461792\n",
      "I7JP3F91.csv: 109 of 144\n",
      "11.915621280670166\n",
      "M73R9KGL.csv: 110 of 144\n",
      "13.895318031311035\n",
      "7YVY712V.csv: 111 of 144\n",
      "11.870040893554688\n",
      "A3UJTT4U.csv: 112 of 144\n",
      "14.384582042694092\n",
      "TZ10O44M.csv: 113 of 144\n",
      "12.252969980239868\n",
      "USXB8M5L.csv: 114 of 144\n",
      "10.749431848526001\n",
      "S6RYCPCW.csv: 115 of 144\n",
      "14.17525577545166\n",
      "99070098_0_2074872741302696997.csv: 116 of 144\n",
      "23.110596895217896\n",
      "43237185_1_3636357855502246981.csv: 117 of 144\n",
      "18.556032180786133\n",
      "46671561_0_6122315295162029872.csv: 118 of 144\n",
      "25.832144021987915\n",
      "53989675_0_8697482470743954630.csv: 119 of 144\n",
      "14.377196788787842\n",
      "25404227_0_2240631045609013057.csv: 120 of 144\n",
      "19.185588121414185\n",
      "9834884_0_3871985887467090123.csv: 121 of 144\n",
      "48.66048526763916\n",
      "63450419_0_8012592961815711786.csv: 122 of 144\n",
      "21.166095733642578\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv: 123 of 144\n",
      "11.286492824554443\n",
      "22864497_0_8632623712684511496.csv: 124 of 144\n",
      "47.97562003135681\n",
      "53822652_0_5767892317858575530.csv: 125 of 144\n",
      "59.96814203262329\n",
      "37856682_0_6818907050314633217.csv: 126 of 144\n",
      "47.077033281326294\n",
      "26310680_0_5150772059999313798.csv: 127 of 144\n",
      "43.91697096824646\n",
      "29414811_12_251152470253168163.csv: 128 of 144\n",
      "15.055560111999512\n",
      "69537082_0_7789694313271016902.csv: 129 of 144\n",
      "42.3447699546814\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv: 130 of 144\n",
      "8.623080015182495\n",
      "60319454_0_3938426910282115527.csv: 131 of 144\n",
      "13.642040729522705\n",
      "16767252_0_2409448375013995751.csv: 132 of 144\n",
      "15.771263122558594\n",
      "84548468_0_5955155464119382182.csv: 133 of 144\n",
      "19.607301235198975\n",
      "80588006_0_6965325215443683359.csv: 134 of 144\n",
      "12.419893980026245\n",
      "39650055_5_7135804139753401681.csv: 135 of 144\n",
      "20.22871994972229\n",
      "40534006_0_4617468856744635526.csv: 136 of 144\n",
      "15.50412106513977\n",
      "90196673_0_5458330029110291950.csv: 137 of 144\n",
      "40.8723828792572\n",
      "24036779_0_5608105867560183058.csv: 138 of 144\n",
      "18.330065727233887\n",
      "9567241_0_5666388268510912770.csv: 139 of 144\n",
      "12.651001930236816\n",
      "41480166_0_6681239260286218499.csv: 140 of 144\n",
      "29.992806911468506\n",
      "77694908_0_6083291340991074532.csv: 141 of 144\n",
      "22.228206157684326\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv: 142 of 144\n",
      "13.005837678909302\n",
      "39107734_2_2329160387535788734.csv: 143 of 144\n",
      "13.952759981155396\n",
      "50245608_0_871275842592178099.csv: 144 of 144\n",
      "37.84526801109314\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_context_path,train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unauthorized-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv: 1 of 9\n",
      "16.595498085021973\n",
      "45073662_0_3179937335063201739.csv: 2 of 9\n",
      "16.528332948684692\n",
      "29414811_2_4773219892816395776.csv: 3 of 9\n",
      "11.15304708480835\n",
      "84575189_0_6365692015941409487.csv: 4 of 9\n",
      "17.38416600227356\n",
      "14380604_4_3329235705746762392.csv: 5 of 9\n",
      "13.114023923873901\n",
      "52299421_0_4473286348258170200.csv: 6 of 9\n",
      "20.181124925613403\n",
      "50270082_0_444360818941411589.csv: 7 of 9\n",
      "28.23505997657776\n",
      "28086084_0_3127660530989916727.csv: 8 of 9\n",
      "31.82511591911316\n",
      "14067031_0_559833072073397908.csv: 9 of 9\n",
      "17.163708925247192\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_context_path, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-nature",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liquid-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(candidate_dir, embedding_dir, class_count_dir, property_count_dir, context_path, output_path):\n",
    "    file_list = glob.glob(candidate_dir + '/*.csv')\n",
    "    for i, file in enumerate(file_list):\n",
    "        filename = file.split('/')[-1]\n",
    "        print(f\"{filename}: {i+1} of {len(file_list)}\")\n",
    "        embedding_file = f\"{embedding_dir}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "        class_count_file = f\"{class_count_dir}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "        context_file = f\"{context_path}/{filename.strip('.csv')}_context.tsv\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        classifier_features_str = \",\".join(classifier_features)\n",
    "        !time tl align-page-rank $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein \\\n",
    "            / string-similarity -i --method jaccard:tokenizer=word -c kg_descriptions context -o des_cont_jaccard \\\n",
    "            / normalize-scores -c des_cont_jaccard / smallest-qnode-number \\\n",
    "            / mosaic-features -c kg_labels --num-char --num-tokens \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            / vote-by-classifier  \\\n",
    "            --prob-threshold 0.995 \\\n",
    "            --features $classifier_features_str \\\n",
    "            --model $classifier_model_path \\\n",
    "            / score-using-embedding \\\n",
    "            --column-vector-strategy centroid-of-lof \\\n",
    "            --lof-strategy ems-mv \\\n",
    "            -o lof-graph-embedding-score \\\n",
    "            --embedding-file $embedding_file \\\n",
    "            / generate-reciprocal-rank  \\\n",
    "            -c lof-graph-embedding-score \\\n",
    "            -o lof-reciprocal-rank \\\n",
    "            / compute-tf-idf  \\\n",
    "            --feature-file $class_count_file \\\n",
    "            --feature-name class_count \\\n",
    "            --singleton-column is_lof \\\n",
    "            -o lof_class_count_tf_idf_score \\\n",
    "            / compute-tf-idf \\\n",
    "            --feature-file $property_count_file \\\n",
    "            --feature-name property_count \\\n",
    "            --singleton-column is_lof \\\n",
    "            -o lof_property_count_tf_idf_score \\\n",
    "            / context-match --context-file $context_file \\\n",
    "            -o context_score \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "steady-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv: 1 of 144\n",
      "Qnodes to lookup: 10717\n",
      "Qnodes from file: 10399\n",
      "Outlier removal generates 87 lof-voted candidates\n",
      "\n",
      "real\t2m0.707s\n",
      "user\t2m10.583s\n",
      "sys\t0m11.802s\n",
      "ZX8GERJC.csv: 2 of 144\n",
      "Qnodes to lookup: 2913\n",
      "Qnodes from file: 2816\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t1m37.843s\n",
      "user\t1m50.926s\n",
      "sys\t0m11.158s\n",
      "8ZD74BO9.csv: 3 of 144\n",
      "Qnodes to lookup: 2824\n",
      "Qnodes from file: 2630\n",
      "Outlier removal generates 21 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 30\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "\n",
      "real\t3m6.760s\n",
      "user\t3m14.816s\n",
      "sys\t0m10.593s\n",
      "W0ZNF869.csv: 4 of 144\n",
      "Qnodes to lookup: 4683\n",
      "Qnodes from file: 4612\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t1m9.241s\n",
      "user\t1m22.880s\n",
      "sys\t0m10.679s\n",
      "AM1UELOJ.csv: 5 of 144\n",
      "Qnodes to lookup: 3502\n",
      "Qnodes from file: 3339\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "Outlier removal generates 64 lof-voted candidates\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t2m15.830s\n",
      "user\t2m27.191s\n",
      "sys\t0m10.331s\n",
      "39173938_0_7916056990138658530.csv: 6 of 144\n",
      "Qnodes to lookup: 9986\n",
      "Qnodes from file: 9718\n",
      "Outlier removal generates 83 lof-voted candidates\n",
      "\n",
      "real\t1m47.364s\n",
      "user\t1m57.420s\n",
      "sys\t0m10.951s\n",
      "5IXA0RAI.csv: 7 of 144\n",
      "Qnodes to lookup: 257\n",
      "Qnodes from file: 247\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m22.002s\n",
      "user\t0m35.123s\n",
      "sys\t0m10.919s\n",
      "8EFC5XVR.csv: 8 of 144\n",
      "Qnodes to lookup: 1428\n",
      "Qnodes from file: 1365\n",
      "_centroid_of_lof: Missing 6 of 12\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "Outlier removal generates 34 lof-voted candidates\n",
      "_centroid_of_lof: Missing 12 of 34\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 38\n",
      "Outlier removal generates 34 lof-voted candidates\n",
      "\n",
      "real\t1m44.250s\n",
      "user\t1m54.020s\n",
      "sys\t0m9.898s\n",
      "DPUA686B.csv: 9 of 144\n",
      "Qnodes to lookup: 2351\n",
      "Qnodes from file: 2187\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t0m41.409s\n",
      "user\t0m51.609s\n",
      "sys\t0m10.112s\n",
      "UMMA6HQO.csv: 10 of 144\n",
      "Qnodes to lookup: 3328\n",
      "Qnodes from file: 3158\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 47\n",
      "Outlier removal generates 28 lof-voted candidates\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "\n",
      "real\t1m55.391s\n",
      "user\t2m9.745s\n",
      "sys\t0m10.718s\n",
      "ERPSWFMM.csv: 11 of 144\n",
      "Qnodes to lookup: 4244\n",
      "Qnodes from file: 4157\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t1m30.775s\n",
      "user\t1m41.825s\n",
      "sys\t0m10.140s\n",
      "ZDAZ5PQ5.csv: 12 of 144\n",
      "Qnodes to lookup: 6389\n",
      "Qnodes from file: 6094\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 25\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 32 lof-voted candidates\n",
      "Outlier removal generates 29 lof-voted candidates\n",
      "\n",
      "real\t3m11.705s\n",
      "user\t3m20.400s\n",
      "sys\t0m11.749s\n",
      "XF412HIL.csv: 13 of 144\n",
      "Qnodes to lookup: 1091\n",
      "Qnodes from file: 1079\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t0m26.890s\n",
      "user\t0m37.547s\n",
      "sys\t0m10.762s\n",
      "BQ36GYQE.csv: 14 of 144\n",
      "Qnodes to lookup: 757\n",
      "Qnodes from file: 750\n",
      "Outlier removal generates 1642 lof-voted candidates\n",
      "Outlier removal generates 1642 lof-voted candidates\n",
      "Outlier removal generates 100 lof-voted candidates\n",
      "Outlier removal generates 1419 lof-voted candidates\n",
      "Outlier removal generates 1644 lof-voted candidates\n",
      "\n",
      "real\t2m23.546s\n",
      "user\t2m34.247s\n",
      "sys\t0m11.224s\n",
      "CKRLO13X.csv: 15 of 144\n",
      "Qnodes to lookup: 3967\n",
      "Qnodes from file: 3889\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 30 lof-voted candidates\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t2m18.812s\n",
      "user\t2m26.558s\n",
      "sys\t0m10.998s\n",
      "L5LFLQIN.csv: 16 of 144\n",
      "Qnodes to lookup: 1834\n",
      "Qnodes from file: 1768\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m32.406s\n",
      "user\t0m41.217s\n",
      "sys\t0m13.419s\n",
      "J6SSKET3.csv: 17 of 144\n",
      "Qnodes to lookup: 4085\n",
      "Qnodes from file: 4045\n",
      "Outlier removal generates 48 lof-voted candidates\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "Outlier removal generates 71 lof-voted candidates\n",
      "\n",
      "real\t2m25.046s\n",
      "user\t2m37.476s\n",
      "sys\t0m14.108s\n",
      "10579449_0_1681126353774891032.csv: 18 of 144\n",
      "Qnodes to lookup: 1706\n",
      "Qnodes from file: 1652\n",
      "_centroid_of_lof: Missing 1 of 14\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "\n",
      "real\t0m25.198s\n",
      "user\t0m37.754s\n",
      "sys\t0m11.831s\n",
      "T8SL8HGK.csv: 19 of 144\n",
      "Qnodes to lookup: 4225\n",
      "Qnodes from file: 4162\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t1m44.200s\n",
      "user\t1m50.934s\n",
      "sys\t0m11.826s\n",
      "JUFYSXYP.csv: 20 of 144\n",
      "Qnodes to lookup: 1412\n",
      "Qnodes from file: 1279\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "\n",
      "real\t0m31.389s\n",
      "user\t0m38.236s\n",
      "sys\t0m12.261s\n",
      "CYYO69JB.csv: 21 of 144\n",
      "Qnodes to lookup: 1202\n",
      "Qnodes from file: 1186\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m26.452s\n",
      "user\t0m40.424s\n",
      "sys\t0m10.978s\n",
      "YMHERMQV.csv: 22 of 144\n",
      "Qnodes to lookup: 4907\n",
      "Qnodes from file: 4833\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t1m35.977s\n",
      "user\t1m43.749s\n",
      "sys\t0m11.686s\n",
      "6XCOGRWM.csv: 23 of 144\n",
      "Qnodes to lookup: 4969\n",
      "Qnodes from file: 4390\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 31\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "\n",
      "real\t1m34.385s\n",
      "user\t1m44.452s\n",
      "sys\t0m12.695s\n",
      "WNKF57RH.csv: 24 of 144\n",
      "Qnodes to lookup: 1614\n",
      "Qnodes from file: 1580\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m28.243s\n",
      "user\t0m36.464s\n",
      "sys\t0m11.588s\n",
      "33401079_0_9127583903019856402.csv: 25 of 144\n",
      "Qnodes to lookup: 8276\n",
      "Qnodes from file: 8183\n",
      "Outlier removal generates 60 lof-voted candidates\n",
      "\n",
      "real\t4m8.736s\n",
      "user\t4m15.612s\n",
      "sys\t0m13.288s\n",
      "21362676_0_6854186738074119688.csv: 26 of 144\n",
      "Qnodes to lookup: 10157\n",
      "Qnodes from file: 9877\n",
      "Outlier removal generates 109 lof-voted candidates\n",
      "\n",
      "real\t2m8.405s\n",
      "user\t2m16.497s\n",
      "sys\t0m13.601s\n",
      "38428277_0_1311643810102462607.csv: 27 of 144\n",
      "Qnodes to lookup: 12595\n",
      "Qnodes from file: 12188\n",
      "Outlier removal generates 119 lof-voted candidates\n",
      "\n",
      "real\t1m58.664s\n",
      "user\t2m13.421s\n",
      "sys\t0m11.252s\n",
      "OMJX8TT6.csv: 28 of 144\n",
      "Qnodes to lookup: 1209\n",
      "Qnodes from file: 1195\n",
      "Outlier removal generates 16 lof-voted candidates\n",
      "\n",
      "real\t0m28.006s\n",
      "user\t0m38.015s\n",
      "sys\t0m10.986s\n",
      "IUBTQXYO.csv: 29 of 144\n",
      "Qnodes to lookup: 1685\n",
      "Qnodes from file: 1627\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "\n",
      "real\t0m30.759s\n",
      "user\t0m44.157s\n",
      "sys\t0m10.909s\n",
      "0XXGVKA8.csv: 30 of 144\n",
      "Qnodes to lookup: 2898\n",
      "Qnodes from file: 2874\n",
      "Outlier removal generates 50 lof-voted candidates\n",
      "Outlier removal generates 49 lof-voted candidates\n",
      "\n",
      "real\t0m47.633s\n",
      "user\t0m56.443s\n",
      "sys\t0m11.713s\n",
      "57681CMM.csv: 31 of 144\n",
      "Qnodes to lookup: 6220\n",
      "Qnodes from file: 6111\n",
      "Outlier removal generates 6 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 43\n",
      "Outlier removal generates 25 lof-voted candidates\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t1m48.661s\n",
      "user\t1m57.606s\n",
      "sys\t0m13.206s\n",
      "VE3T1LHT.csv: 32 of 144\n",
      "Qnodes to lookup: 1602\n",
      "Qnodes from file: 1529\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "\n",
      "real\t1m28.653s\n",
      "user\t1m39.555s\n",
      "sys\t0m11.455s\n",
      "4KGRZFTI.csv: 33 of 144\n",
      "Qnodes to lookup: 756\n",
      "Qnodes from file: 743\n",
      "Outlier removal generates 28 lof-voted candidates\n",
      "\n",
      "real\t0m33.078s\n",
      "user\t0m40.592s\n",
      "sys\t0m12.562s\n",
      "UU8Q91MG.csv: 34 of 144\n",
      "Qnodes to lookup: 1609\n",
      "Qnodes from file: 1539\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t0m40.416s\n",
      "user\t0m51.889s\n",
      "sys\t0m11.807s\n",
      "75MLA4XJ.csv: 35 of 144\n",
      "Qnodes to lookup: 313\n",
      "Qnodes from file: 310\n",
      "Outlier removal generates 1160 lof-voted candidates\n",
      "Outlier removal generates 24 lof-voted candidates\n",
      "Outlier removal generates 80 lof-voted candidates\n",
      "\n",
      "real\t1m0.698s\n",
      "user\t1m11.628s\n",
      "sys\t0m12.530s\n",
      "U5L8U1OL.csv: 36 of 144\n",
      "Qnodes to lookup: 3748\n",
      "Qnodes from file: 3714\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "Outlier removal generates 32 lof-voted candidates\n",
      "\n",
      "real\t4m3.009s\n",
      "user\t4m9.557s\n",
      "sys\t0m13.938s\n",
      "QDJ86U5I.csv: 37 of 144\n",
      "Qnodes to lookup: 2444\n",
      "Qnodes from file: 2399\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t0m29.737s\n",
      "user\t0m39.379s\n",
      "sys\t0m12.514s\n",
      "384SR1N3.csv: 38 of 144\n",
      "Qnodes to lookup: 1667\n",
      "Qnodes from file: 1644\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m34.374s\n",
      "user\t0m45.395s\n",
      "sys\t0m12.028s\n",
      "PG0TP6O0.csv: 39 of 144\n",
      "Qnodes to lookup: 3679\n",
      "Qnodes from file: 3615\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "Outlier removal generates 16 lof-voted candidates\n",
      "\n",
      "real\t3m14.759s\n",
      "user\t3m19.290s\n",
      "sys\t0m12.990s\n",
      "VFVMRNF9.csv: 40 of 144\n",
      "Qnodes to lookup: 455\n",
      "Qnodes from file: 428\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t0m50.697s\n",
      "user\t0m59.760s\n",
      "sys\t0m11.673s\n",
      "CCCNRESE.csv: 41 of 144\n",
      "Qnodes to lookup: 1610\n",
      "Qnodes from file: 1557\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m27.205s\n",
      "user\t0m35.169s\n",
      "sys\t0m11.049s\n",
      "7ZQB5C2O.csv: 42 of 144\n",
      "Qnodes to lookup: 2134\n",
      "Qnodes from file: 2113\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 236 lof-voted candidates\n",
      "Outlier removal generates 80 lof-voted candidates\n",
      "Outlier removal generates 580 lof-voted candidates\n",
      "\n",
      "real\t4m53.366s\n",
      "user\t5m1.422s\n",
      "sys\t0m11.879s\n",
      "LTZQIN2R.csv: 43 of 144\n",
      "Qnodes to lookup: 4417\n",
      "Qnodes from file: 4030\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "_centroid_of_lof: Missing 1 of 35\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "\n",
      "real\t2m15.114s\n",
      "user\t2m25.748s\n",
      "sys\t0m11.803s\n",
      "QID3PSI3.csv: 44 of 144\n",
      "Qnodes to lookup: 2708\n",
      "Qnodes from file: 2683\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t0m27.150s\n",
      "user\t0m37.795s\n",
      "sys\t0m11.675s\n",
      "NXBVTACX.csv: 45 of 144\n",
      "Qnodes to lookup: 1820\n",
      "Qnodes from file: 1736\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t0m47.132s\n",
      "user\t0m55.648s\n",
      "sys\t0m10.216s\n",
      "DKRE7U28.csv: 46 of 144\n",
      "Qnodes to lookup: 4227\n",
      "Qnodes from file: 4082\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "Outlier removal generates 34 lof-voted candidates\n",
      "\n",
      "real\t1m8.652s\n",
      "user\t1m17.863s\n",
      "sys\t0m12.657s\n",
      "91959037_0_7907661684242014480.csv: 47 of 144\n",
      "Qnodes to lookup: 22297\n",
      "Qnodes from file: 21621\n",
      "Outlier removal generates 89 lof-voted candidates\n",
      "\n",
      "real\t4m2.766s\n",
      "user\t4m15.757s\n",
      "sys\t0m12.724s\n",
      "U8BHYWZ7.csv: 48 of 144\n",
      "Qnodes to lookup: 1789\n",
      "Qnodes from file: 1771\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m30.333s\n",
      "user\t0m39.335s\n",
      "sys\t0m11.473s\n",
      "2389HYHH.csv: 49 of 144\n",
      "Qnodes to lookup: 358\n",
      "Qnodes from file: 357\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t0m44.093s\n",
      "user\t0m59.329s\n",
      "sys\t0m11.134s\n",
      "2LM6W2JV.csv: 50 of 144\n",
      "Qnodes to lookup: 748\n",
      "Qnodes from file: 735\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m25.445s\n",
      "user\t0m34.364s\n",
      "sys\t0m10.879s\n",
      "B735JU5L.csv: 51 of 144\n",
      "Qnodes to lookup: 4675\n",
      "Qnodes from file: 4590\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "\n",
      "real\t2m44.594s\n",
      "user\t2m51.883s\n",
      "sys\t0m13.018s\n",
      "20135078_0_7570343137119682530.csv: 52 of 144\n",
      "Qnodes to lookup: 11208\n",
      "Qnodes from file: 10916\n",
      "Outlier removal generates 104 lof-voted candidates\n",
      "\n",
      "real\t1m56.164s\n",
      "user\t2m4.682s\n",
      "sys\t0m11.277s\n",
      "6LSIYDYN.csv: 53 of 144\n",
      "Qnodes to lookup: 2975\n",
      "Qnodes from file: 2944\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "\n",
      "real\t0m33.599s\n",
      "user\t0m44.947s\n",
      "sys\t0m11.719s\n",
      "S2LGMOGC.csv: 54 of 144\n",
      "Qnodes to lookup: 1757\n",
      "Qnodes from file: 1721\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t0m27.594s\n",
      "user\t0m36.035s\n",
      "sys\t0m11.407s\n",
      "SVPAISS7.csv: 55 of 144\n",
      "Qnodes to lookup: 1787\n",
      "Qnodes from file: 1717\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t0m27.614s\n",
      "user\t0m39.546s\n",
      "sys\t0m10.806s\n",
      "0H0U54UZ.csv: 56 of 144\n",
      "Qnodes to lookup: 1040\n",
      "Qnodes from file: 1020\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t0m28.931s\n",
      "user\t0m36.449s\n",
      "sys\t0m10.549s\n",
      "4W3919II.csv: 57 of 144\n",
      "Qnodes to lookup: 1481\n",
      "Qnodes from file: 1441\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m25.665s\n",
      "user\t0m35.723s\n",
      "sys\t0m10.760s\n",
      "AGCHY9JJ.csv: 58 of 144\n",
      "Qnodes to lookup: 2607\n",
      "Qnodes from file: 2572\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "\n",
      "real\t0m51.195s\n",
      "user\t0m58.812s\n",
      "sys\t0m12.410s\n",
      "G2K4GSYB.csv: 59 of 144\n",
      "Qnodes to lookup: 4564\n",
      "Qnodes from file: 4401\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "Outlier removal generates 26 lof-voted candidates\n",
      "Outlier removal generates 29 lof-voted candidates\n",
      "\n",
      "real\t3m25.198s\n",
      "user\t3m34.161s\n",
      "sys\t0m12.015s\n",
      "RBIVOB6Q.csv: 60 of 144\n",
      "Qnodes to lookup: 2256\n",
      "Qnodes from file: 2229\n",
      "Outlier removal generates 43 lof-voted candidates\n",
      "Outlier removal generates 33 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t2m56.249s\n",
      "user\t3m7.304s\n",
      "sys\t0m11.581s\n",
      "2JN1R1VW.csv: 61 of 144\n",
      "Qnodes to lookup: 3669\n",
      "Qnodes from file: 3606\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "\n",
      "real\t1m15.200s\n",
      "user\t1m28.858s\n",
      "sys\t0m11.279s\n",
      "6NO3AH02.csv: 62 of 144\n",
      "Qnodes to lookup: 1687\n",
      "Qnodes from file: 1632\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m28.301s\n",
      "user\t0m37.328s\n",
      "sys\t0m10.694s\n",
      "ABRT6AWH.csv: 63 of 144\n",
      "Qnodes to lookup: 482\n",
      "Qnodes from file: 473\n",
      "Outlier removal generates 29 lof-voted candidates\n",
      "\n",
      "real\t0m29.079s\n",
      "user\t0m37.339s\n",
      "sys\t0m10.560s\n",
      "KUN2Y3DX.csv: 64 of 144\n",
      "Qnodes to lookup: 2748\n",
      "Qnodes from file: 2719\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "Outlier removal generates 50 lof-voted candidates\n",
      "\n",
      "real\t1m47.789s\n",
      "user\t1m59.579s\n",
      "sys\t0m10.757s\n",
      "35188621_0_6058553107571275232.csv: 65 of 144\n",
      "Qnodes to lookup: 11833\n",
      "Qnodes from file: 11460\n",
      "_centroid_of_lof: Missing 1 of 175\n",
      "Outlier removal generates 104 lof-voted candidates\n",
      "\n",
      "real\t1m55.946s\n",
      "user\t2m6.117s\n",
      "sys\t0m11.328s\n",
      "24VU5BR7.csv: 66 of 144\n",
      "Qnodes to lookup: 4146\n",
      "Qnodes from file: 4049\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "\n",
      "real\t0m59.379s\n",
      "user\t1m10.219s\n",
      "sys\t0m10.767s\n",
      "PQN3CY7B.csv: 67 of 144\n",
      "Qnodes to lookup: 3137\n",
      "Qnodes from file: 2982\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t1m5.792s\n",
      "user\t1m17.046s\n",
      "sys\t0m11.259s\n",
      "NA24I27F.csv: 68 of 144\n",
      "Qnodes to lookup: 2582\n",
      "Qnodes from file: 2541\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t0m45.972s\n",
      "user\t1m0.584s\n",
      "sys\t0m9.929s\n",
      "9SERGNIZ.csv: 69 of 144\n",
      "Qnodes to lookup: 5199\n",
      "Qnodes from file: 4935\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "Outlier removal generates 16 lof-voted candidates\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "\n",
      "real\t1m30.504s\n",
      "user\t1m39.330s\n",
      "sys\t0m12.338s\n",
      "R4K6322V.csv: 70 of 144\n",
      "Qnodes to lookup: 1744\n",
      "Qnodes from file: 1651\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m30.154s\n",
      "user\t0m44.675s\n",
      "sys\t0m10.741s\n",
      "5W99BCM4.csv: 71 of 144\n",
      "Qnodes to lookup: 1783\n",
      "Qnodes from file: 1725\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t0m27.940s\n",
      "user\t0m37.211s\n",
      "sys\t0m10.600s\n",
      "7XB008OM.csv: 72 of 144\n",
      "Qnodes to lookup: 3870\n",
      "Qnodes from file: 3796\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 32 lof-voted candidates\n",
      "\n",
      "real\t2m59.252s\n",
      "user\t3m7.323s\n",
      "sys\t0m10.787s\n",
      "6VLKFW8J.csv: 73 of 144\n",
      "Qnodes to lookup: 215\n",
      "Qnodes from file: 215\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t0m48.822s\n",
      "user\t0m57.341s\n",
      "sys\t0m10.306s\n",
      "HFRDW66L.csv: 74 of 144\n",
      "Qnodes to lookup: 354\n",
      "Qnodes from file: 343\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t2m9.114s\n",
      "user\t2m19.987s\n",
      "sys\t0m11.013s\n",
      "54719588_0_8417197176086756912.csv: 75 of 144\n",
      "Qnodes to lookup: 24211\n",
      "Qnodes from file: 23278\n",
      "Outlier removal generates 77 lof-voted candidates\n",
      "\n",
      "real\t5m35.741s\n",
      "user\t5m45.560s\n",
      "sys\t0m12.481s\n",
      "3IB68W0T.csv: 76 of 144\n",
      "Qnodes to lookup: 1809\n",
      "Qnodes from file: 1786\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "Outlier removal generates 38 lof-voted candidates\n",
      "\n",
      "real\t1m33.013s\n",
      "user\t1m42.549s\n",
      "sys\t0m11.371s\n",
      "21245481_0_8730460088443117515.csv: 77 of 144\n",
      "Qnodes to lookup: 11848\n",
      "Qnodes from file: 11696\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t2m13.149s\n",
      "user\t2m23.465s\n",
      "sys\t0m12.400s\n",
      "JZ9RW99R.csv: 78 of 144\n",
      "Qnodes to lookup: 2969\n",
      "Qnodes from file: 2880\n",
      "_centroid_of_lof: Missing 1 of 39\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "\n",
      "real\t1m36.533s\n",
      "user\t1m46.220s\n",
      "sys\t0m10.704s\n",
      "RDNZHHGI.csv: 79 of 144\n",
      "Qnodes to lookup: 985\n",
      "Qnodes from file: 969\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m21.763s\n",
      "user\t0m34.993s\n",
      "sys\t0m10.908s\n",
      "MOX6MBH5.csv: 80 of 144\n",
      "Qnodes to lookup: 6484\n",
      "Qnodes from file: 6362\n",
      "_centroid_of_lof: Missing 1 of 6\n",
      "Outlier removal generates 3 lof-voted candidates\n",
      "_centroid_of_lof: Missing 5 of 38\n",
      "Outlier removal generates 25 lof-voted candidates\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t2m35.998s\n",
      "user\t2m45.297s\n",
      "sys\t0m11.530s\n",
      "0ZH7HCT0.csv: 81 of 144\n",
      "Qnodes to lookup: 1099\n",
      "Qnodes from file: 1075\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m22.757s\n",
      "user\t0m35.085s\n",
      "sys\t0m10.580s\n",
      "OJEI7G4L.csv: 82 of 144\n",
      "Qnodes to lookup: 5662\n",
      "Qnodes from file: 5601\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "\n",
      "real\t2m55.726s\n",
      "user\t3m5.303s\n",
      "sys\t0m10.797s\n",
      "6ED9WFUN.csv: 83 of 144\n",
      "Qnodes to lookup: 5289\n",
      "Qnodes from file: 5172\n",
      "Outlier removal generates 15 lof-voted candidates\n",
      "Outlier removal generates 21 lof-voted candidates\n",
      "Outlier removal generates 29 lof-voted candidates\n",
      "\n",
      "real\t1m58.702s\n",
      "user\t2m7.454s\n",
      "sys\t0m10.590s\n",
      "71840765_0_6664391841933033844.csv: 84 of 144\n",
      "Qnodes to lookup: 1100\n",
      "Qnodes from file: 1094\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t0m26.621s\n",
      "user\t0m36.097s\n",
      "sys\t0m10.900s\n",
      "E0LR4TZL.csv: 85 of 144\n",
      "Qnodes to lookup: 3508\n",
      "Qnodes from file: 3323\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "Outlier removal generates 25 lof-voted candidates\n",
      "\n",
      "real\t1m24.615s\n",
      "user\t1m32.672s\n",
      "sys\t0m11.292s\n",
      "YS86QOSL.csv: 86 of 144\n",
      "Qnodes to lookup: 3417\n",
      "Qnodes from file: 3352\n",
      "Outlier removal generates 16 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "\n",
      "real\t2m33.208s\n",
      "user\t2m43.449s\n",
      "sys\t0m10.931s\n",
      "T3W112BN.csv: 87 of 144\n",
      "Qnodes to lookup: 3391\n",
      "Qnodes from file: 3313\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "Outlier removal generates 28 lof-voted candidates\n",
      "Outlier removal generates 30 lof-voted candidates\n",
      "\n",
      "real\t1m37.238s\n",
      "user\t1m48.047s\n",
      "sys\t0m10.549s\n",
      "RLMB7HEB.csv: 88 of 144\n",
      "Qnodes to lookup: 2139\n",
      "Qnodes from file: 2101\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m27.269s\n",
      "user\t0m38.329s\n",
      "sys\t0m10.194s\n",
      "GX0WEFG7.csv: 89 of 144\n",
      "Qnodes to lookup: 129\n",
      "Qnodes from file: 129\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t0m31.646s\n",
      "user\t0m39.261s\n",
      "sys\t0m10.114s\n",
      "8468806_0_4382447409703007384.csv: 90 of 144\n",
      "Qnodes to lookup: 6738\n",
      "Qnodes from file: 6671\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "\n",
      "real\t1m27.286s\n",
      "user\t1m37.357s\n",
      "sys\t0m10.631s\n",
      "5DNR9AW5.csv: 91 of 144\n",
      "Qnodes to lookup: 271\n",
      "Qnodes from file: 264\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t0m56.540s\n",
      "user\t1m6.846s\n",
      "sys\t0m10.879s\n",
      "88523363_0_8180214313099580515.csv: 92 of 144\n",
      "Qnodes to lookup: 42234\n",
      "Qnodes from file: 41968\n",
      "Outlier removal generates 349 lof-voted candidates\n",
      "\n",
      "real\t6m9.319s\n",
      "user\t6m23.711s\n",
      "sys\t0m13.904s\n",
      "3TIMOEBD.csv: 93 of 144\n",
      "Qnodes to lookup: 1384\n",
      "Qnodes from file: 1356\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "\n",
      "real\t0m23.467s\n",
      "user\t0m35.477s\n",
      "sys\t0m9.819s\n",
      "OXLTY5IY.csv: 94 of 144\n",
      "Qnodes to lookup: 1425\n",
      "Qnodes from file: 1367\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m25.660s\n",
      "user\t0m34.694s\n",
      "sys\t0m10.234s\n",
      "29414811_13_8724394428539174350.csv: 95 of 144\n",
      "Qnodes to lookup: 1140\n",
      "Qnodes from file: 1102\n",
      "Outlier removal generates 6 lof-voted candidates\n",
      "\n",
      "real\t0m55.353s\n",
      "user\t1m2.881s\n",
      "sys\t0m10.860s\n",
      "MBM31U4C.csv: 96 of 144\n",
      "Qnodes to lookup: 4411\n",
      "Qnodes from file: 4188\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "Outlier removal generates 16 lof-voted candidates\n",
      "Outlier removal generates 15 lof-voted candidates\n",
      "Outlier removal generates 27 lof-voted candidates\n",
      "\n",
      "real\t2m33.039s\n",
      "user\t2m44.124s\n",
      "sys\t0m11.580s\n",
      "0MZX65PH.csv: 97 of 144\n",
      "Qnodes to lookup: 1084\n",
      "Qnodes from file: 994\n",
      "_centroid_of_lof: Missing 1 of 8\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t0m27.433s\n",
      "user\t0m35.648s\n",
      "sys\t0m10.556s\n",
      "Y3XLQHGC.csv: 98 of 144\n",
      "Qnodes to lookup: 2167\n",
      "Qnodes from file: 2038\n",
      "Outlier removal generates 24 lof-voted candidates\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "_centroid_of_lof: Missing 5 of 40\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "_centroid_of_lof: Missing 3 of 51\n",
      "Outlier removal generates 31 lof-voted candidates\n",
      "Outlier removal generates 72 lof-voted candidates\n",
      "\n",
      "real\t8m17.680s\n",
      "user\t8m27.036s\n",
      "sys\t0m10.678s\n",
      "2UETZ4XK.csv: 99 of 144\n",
      "Qnodes to lookup: 1597\n",
      "Qnodes from file: 1562\n",
      "Outlier removal generates 6 lof-voted candidates\n",
      "\n",
      "real\t0m25.384s\n",
      "user\t0m35.195s\n",
      "sys\t0m10.688s\n",
      "OAWHF5BM.csv: 100 of 144\n",
      "Qnodes to lookup: 763\n",
      "Qnodes from file: 751\n",
      "Outlier removal generates 27 lof-voted candidates\n",
      "\n",
      "real\t0m28.739s\n",
      "user\t0m37.902s\n",
      "sys\t0m10.424s\n",
      "3YNPHVPV.csv: 101 of 144\n",
      "Qnodes to lookup: 1560\n",
      "Qnodes from file: 1535\n",
      "Outlier removal generates 38 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t1m8.319s\n",
      "user\t1m16.509s\n",
      "sys\t0m10.391s\n",
      "YXYVNO79.csv: 102 of 144\n",
      "Qnodes to lookup: 1139\n",
      "Qnodes from file: 1122\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "\n",
      "real\t0m23.846s\n",
      "user\t0m37.244s\n",
      "sys\t0m10.353s\n",
      "1XNHBBRZ.csv: 103 of 144\n",
      "Qnodes to lookup: 1367\n",
      "Qnodes from file: 1322\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t1m54.791s\n",
      "user\t2m4.121s\n",
      "sys\t0m10.702s\n",
      "M0XIN8I8.csv: 104 of 144\n",
      "Qnodes to lookup: 3270\n",
      "Qnodes from file: 3148\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "\n",
      "real\t2m10.313s\n",
      "user\t2m22.799s\n",
      "sys\t0m11.383s\n",
      "QIBT0IBA.csv: 105 of 144\n",
      "Qnodes to lookup: 648\n",
      "Qnodes from file: 637\n",
      "No pseudo GT available\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: context-match\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/context-match.py\", line 44, in run\n",
      "    obj = MatchContext(input_file_path, context_file_path, kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/context_match.py\", line 8, in __init__\n",
      "    self.final_data = pd.read_csv(input_path, dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m26.628s\n",
      "user\t0m35.118s\n",
      "sys\t0m10.446s\n",
      "SY4CRLEA.csv: 106 of 144\n",
      "Qnodes to lookup: 4098\n",
      "Qnodes from file: 4013\n",
      "Outlier removal generates 26 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t4m0.369s\n",
      "user\t4m12.919s\n",
      "sys\t0m11.567s\n",
      "SU8B1A6K.csv: 107 of 144\n",
      "Qnodes to lookup: 4766\n",
      "Qnodes from file: 4685\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 14 lof-voted candidates\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t1m42.521s\n",
      "user\t1m51.951s\n",
      "sys\t0m10.996s\n",
      "XVMPJ993.csv: 108 of 144\n",
      "Qnodes to lookup: 1656\n",
      "Qnodes from file: 1632\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t0m23.717s\n",
      "user\t0m35.896s\n",
      "sys\t0m10.315s\n",
      "I7JP3F91.csv: 109 of 144\n",
      "Qnodes to lookup: 1752\n",
      "Qnodes from file: 1730\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t0m28.884s\n",
      "user\t0m38.486s\n",
      "sys\t0m10.760s\n",
      "M73R9KGL.csv: 110 of 144\n",
      "Qnodes to lookup: 3895\n",
      "Qnodes from file: 3841\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t1m33.540s\n",
      "user\t1m41.030s\n",
      "sys\t0m10.424s\n",
      "7YVY712V.csv: 111 of 144\n",
      "Qnodes to lookup: 1099\n",
      "Qnodes from file: 1076\n",
      "Outlier removal generates 346 lof-voted candidates\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t0m31.729s\n",
      "user\t0m45.252s\n",
      "sys\t0m10.599s\n",
      "A3UJTT4U.csv: 112 of 144\n",
      "Qnodes to lookup: 4511\n",
      "Qnodes from file: 4412\n",
      "Outlier removal generates 30 lof-voted candidates\n",
      "\n",
      "real\t0m31.938s\n",
      "user\t0m42.889s\n",
      "sys\t0m10.940s\n",
      "TZ10O44M.csv: 113 of 144\n",
      "Qnodes to lookup: 964\n",
      "Qnodes from file: 888\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "\n",
      "real\t1m11.944s\n",
      "user\t1m22.533s\n",
      "sys\t0m10.039s\n",
      "USXB8M5L.csv: 114 of 144\n",
      "Qnodes to lookup: 1444\n",
      "Qnodes from file: 1417\n",
      "Column_vector_stragtegy centroid_of_lof failed\n",
      "\n",
      "real\t0m24.276s\n",
      "user\t0m36.074s\n",
      "sys\t0m10.116s\n",
      "S6RYCPCW.csv: 115 of 144\n",
      "Qnodes to lookup: 2162\n",
      "Qnodes from file: 2135\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t0m30.604s\n",
      "user\t0m39.074s\n",
      "sys\t0m9.833s\n",
      "99070098_0_2074872741302696997.csv: 116 of 144\n",
      "Qnodes to lookup: 12078\n",
      "Qnodes from file: 11859\n",
      "_centroid_of_lof: Missing 1 of 172\n",
      "Outlier removal generates 103 lof-voted candidates\n",
      "\n",
      "real\t3m9.059s\n",
      "user\t3m19.032s\n",
      "sys\t0m10.752s\n",
      "43237185_1_3636357855502246981.csv: 117 of 144\n",
      "Qnodes to lookup: 2611\n",
      "Qnodes from file: 2573\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t1m46.971s\n",
      "user\t1m55.655s\n",
      "sys\t0m10.367s\n",
      "46671561_0_6122315295162029872.csv: 118 of 144\n",
      "Qnodes to lookup: 15698\n",
      "Qnodes from file: 15130\n",
      "Outlier removal generates 118 lof-voted candidates\n",
      "\n",
      "real\t3m23.565s\n",
      "user\t3m33.228s\n",
      "sys\t0m11.456s\n",
      "53989675_0_8697482470743954630.csv: 119 of 144\n",
      "Qnodes to lookup: 2161\n",
      "Qnodes from file: 2144\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "\n",
      "real\t0m38.258s\n",
      "user\t0m47.805s\n",
      "sys\t0m10.982s\n",
      "25404227_0_2240631045609013057.csv: 120 of 144\n",
      "Qnodes to lookup: 10026\n",
      "Qnodes from file: 9732\n",
      "_centroid_of_lof: Missing 1 of 171\n",
      "Outlier removal generates 103 lof-voted candidates\n",
      "\n",
      "real\t2m2.390s\n",
      "user\t2m14.071s\n",
      "sys\t0m11.895s\n",
      "9834884_0_3871985887467090123.csv: 121 of 144\n",
      "Qnodes to lookup: 22923\n",
      "Qnodes from file: 22574\n",
      "Outlier removal generates 156 lof-voted candidates\n",
      "\n",
      "real\t4m26.776s\n",
      "user\t4m37.848s\n",
      "sys\t0m12.480s\n",
      "63450419_0_8012592961815711786.csv: 122 of 144\n",
      "Qnodes to lookup: 13769\n",
      "Qnodes from file: 13691\n",
      "Outlier removal generates 116 lof-voted candidates\n",
      "\n",
      "real\t1m34.259s\n",
      "user\t1m47.063s\n",
      "sys\t0m11.647s\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv: 123 of 144\n",
      "Qnodes to lookup: 1237\n",
      "Qnodes from file: 1179\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t0m23.778s\n",
      "user\t0m35.693s\n",
      "sys\t0m9.763s\n",
      "22864497_0_8632623712684511496.csv: 124 of 144\n",
      "Qnodes to lookup: 29456\n",
      "Qnodes from file: 28330\n",
      "Outlier removal generates 116 lof-voted candidates\n",
      "\n",
      "real\t5m42.955s\n",
      "user\t5m54.833s\n",
      "sys\t0m11.915s\n",
      "53822652_0_5767892317858575530.csv: 125 of 144\n",
      "Qnodes to lookup: 43140\n",
      "Qnodes from file: 41723\n",
      "_centroid_of_lof: Missing 1 of 230\n",
      "Outlier removal generates 138 lof-voted candidates\n",
      "\n",
      "real\t7m16.818s\n",
      "user\t7m26.670s\n",
      "sys\t0m13.602s\n",
      "37856682_0_6818907050314633217.csv: 126 of 144\n",
      "Qnodes to lookup: 43381\n",
      "Qnodes from file: 42173\n",
      "Outlier removal generates 286 lof-voted candidates\n",
      "\n",
      "real\t2m53.495s\n",
      "user\t3m8.398s\n",
      "sys\t0m13.025s\n",
      "26310680_0_5150772059999313798.csv: 127 of 144\n",
      "Qnodes to lookup: 35193\n",
      "Qnodes from file: 34092\n",
      "Outlier removal generates 216 lof-voted candidates\n",
      "\n",
      "real\t6m8.675s\n",
      "user\t6m18.072s\n",
      "sys\t0m12.575s\n",
      "29414811_12_251152470253168163.csv: 128 of 144\n",
      "Qnodes to lookup: 1857\n",
      "Qnodes from file: 1790\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t1m17.841s\n",
      "user\t1m30.944s\n",
      "sys\t0m9.619s\n",
      "69537082_0_7789694313271016902.csv: 129 of 144\n",
      "Qnodes to lookup: 36385\n",
      "Qnodes from file: 35280\n",
      "Outlier removal generates 234 lof-voted candidates\n",
      "\n",
      "real\t2m7.232s\n",
      "user\t2m18.886s\n",
      "sys\t0m11.692s\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv: 130 of 144\n",
      "Qnodes to lookup: 980\n",
      "Qnodes from file: 976\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "\n",
      "real\t0m28.712s\n",
      "user\t0m36.666s\n",
      "sys\t0m10.268s\n",
      "60319454_0_3938426910282115527.csv: 131 of 144\n",
      "Qnodes to lookup: 5005\n",
      "Qnodes from file: 4886\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t1m4.790s\n",
      "user\t1m14.856s\n",
      "sys\t0m10.688s\n",
      "16767252_0_2409448375013995751.csv: 132 of 144\n",
      "Qnodes to lookup: 8870\n",
      "Qnodes from file: 8626\n",
      "Outlier removal generates 91 lof-voted candidates\n",
      "\n",
      "real\t1m36.963s\n",
      "user\t1m47.011s\n",
      "sys\t0m10.570s\n",
      "84548468_0_5955155464119382182.csv: 133 of 144\n",
      "Qnodes to lookup: 10574\n",
      "Qnodes from file: 10229\n",
      "Outlier removal generates 83 lof-voted candidates\n",
      "\n",
      "real\t1m42.397s\n",
      "user\t1m51.823s\n",
      "sys\t0m10.841s\n",
      "80588006_0_6965325215443683359.csv: 134 of 144\n",
      "Qnodes to lookup: 1855\n",
      "Qnodes from file: 1809\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m24.554s\n",
      "user\t0m36.377s\n",
      "sys\t0m9.679s\n",
      "39650055_5_7135804139753401681.csv: 135 of 144\n",
      "Qnodes to lookup: 13681\n",
      "Qnodes from file: 13068\n",
      "Outlier removal generates 40 lof-voted candidates\n",
      "\n",
      "real\t1m11.809s\n",
      "user\t1m20.613s\n",
      "sys\t0m10.837s\n",
      "40534006_0_4617468856744635526.csv: 136 of 144\n",
      "Qnodes to lookup: 4092\n",
      "Qnodes from file: 4034\n",
      "Outlier removal generates 25 lof-voted candidates\n",
      "\n",
      "real\t0m29.752s\n",
      "user\t0m41.282s\n",
      "sys\t0m10.079s\n",
      "90196673_0_5458330029110291950.csv: 137 of 144\n",
      "Qnodes to lookup: 22360\n",
      "Qnodes from file: 21926\n",
      "Outlier removal generates 144 lof-voted candidates\n",
      "\n",
      "real\t5m13.048s\n",
      "user\t5m25.480s\n",
      "sys\t0m12.201s\n",
      "24036779_0_5608105867560183058.csv: 138 of 144\n",
      "Qnodes to lookup: 13908\n",
      "Qnodes from file: 13592\n",
      "Outlier removal generates 116 lof-voted candidates\n",
      "\n",
      "real\t4m30.244s\n",
      "user\t4m42.416s\n",
      "sys\t0m11.615s\n",
      "9567241_0_5666388268510912770.csv: 139 of 144\n",
      "Qnodes to lookup: 1922\n",
      "Qnodes from file: 1861\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t0m22.390s\n",
      "user\t0m35.826s\n",
      "sys\t0m9.919s\n",
      "41480166_0_6681239260286218499.csv: 140 of 144\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Outlier removal generates 89 lof-voted candidates\n",
      "\n",
      "real\t5m47.533s\n",
      "user\t5m54.801s\n",
      "sys\t0m12.003s\n",
      "77694908_0_6083291340991074532.csv: 141 of 144\n",
      "Qnodes to lookup: 10760\n",
      "Qnodes from file: 10387\n",
      "Outlier removal generates 99 lof-voted candidates\n",
      "\n",
      "real\t1m52.806s\n",
      "user\t2m4.472s\n",
      "sys\t0m11.487s\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv: 142 of 144\n",
      "Qnodes to lookup: 1234\n",
      "Qnodes from file: 1218\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "\n",
      "real\t0m42.449s\n",
      "user\t0m55.185s\n",
      "sys\t0m10.447s\n",
      "39107734_2_2329160387535788734.csv: 143 of 144\n",
      "Qnodes to lookup: 3480\n",
      "Qnodes from file: 3403\n",
      "Outlier removal generates 52 lof-voted candidates\n",
      "\n",
      "real\t1m58.945s\n",
      "user\t2m9.038s\n",
      "sys\t0m10.657s\n",
      "50245608_0_871275842592178099.csv: 144 of 144\n",
      "Qnodes to lookup: 26520\n",
      "Qnodes from file: 25679\n",
      "_centroid_of_lof: Missing 1 of 267\n",
      "Outlier removal generates 160 lof-voted candidates\n",
      "\n",
      "real\t8m46.886s\n",
      "user\t8m56.538s\n",
      "sys\t0m12.991s\n"
     ]
    }
   ],
   "source": [
    "feature_generation(train_candidate_path, train_graph_embedding, train_class_count, train_prop_count, train_context_path, train_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "massive-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv: 1 of 9\n",
      "Qnodes to lookup: 10448\n",
      "Qnodes from file: 10120\n",
      "Outlier removal generates 105 lof-voted candidates\n",
      "\n",
      "real\t2m34.011s\n",
      "user\t2m40.644s\n",
      "sys\t0m11.761s\n",
      "45073662_0_3179937335063201739.csv: 2 of 9\n",
      "Qnodes to lookup: 3040\n",
      "Qnodes from file: 3004\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t0m33.086s\n",
      "user\t0m40.087s\n",
      "sys\t0m11.377s\n",
      "29414811_2_4773219892816395776.csv: 3 of 9\n",
      "Qnodes to lookup: 2106\n",
      "Qnodes from file: 2025\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t1m13.537s\n",
      "user\t1m26.295s\n",
      "sys\t0m12.591s\n",
      "84575189_0_6365692015941409487.csv: 4 of 9\n",
      "Qnodes to lookup: 8486\n",
      "Qnodes from file: 7897\n",
      "Outlier removal generates 46 lof-voted candidates\n",
      "\n",
      "real\t1m29.691s\n",
      "user\t1m38.799s\n",
      "sys\t0m16.526s\n",
      "14380604_4_3329235705746762392.csv: 5 of 9\n",
      "Qnodes to lookup: 2291\n",
      "Qnodes from file: 2226\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t0m52.430s\n",
      "user\t1m6.244s\n",
      "sys\t0m13.862s\n",
      "52299421_0_4473286348258170200.csv: 6 of 9\n",
      "Qnodes to lookup: 15531\n",
      "Qnodes from file: 15218\n",
      "Outlier removal generates 85 lof-voted candidates\n",
      "\n",
      "real\t4m22.400s\n",
      "user\t4m28.102s\n",
      "sys\t0m15.543s\n",
      "50270082_0_444360818941411589.csv: 7 of 9\n",
      "Qnodes to lookup: 16794\n",
      "Qnodes from file: 16683\n",
      "Outlier removal generates 50 lof-voted candidates\n",
      "\n",
      "real\t0m59.505s\n",
      "user\t1m11.734s\n",
      "sys\t0m11.830s\n",
      "28086084_0_3127660530989916727.csv: 8 of 9\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Outlier removal generates 89 lof-voted candidates\n",
      "\n",
      "real\t7m5.681s\n",
      "user\t7m9.106s\n",
      "sys\t0m16.574s\n",
      "14067031_0_559833072073397908.csv: 9 of 9\n",
      "Qnodes to lookup: 7690\n",
      "Qnodes from file: 7433\n",
      "Outlier removal generates 59 lof-voted candidates\n",
      "\n",
      "real\t2m23.614s\n",
      "user\t2m31.452s\n",
      "sys\t0m12.144s\n"
     ]
    }
   ],
   "source": [
    "feature_generation(dev_candidate_path, dev_graph_embedding, dev_class_count, dev_prop_count, dev_context_path, dev_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-invasion",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wanted-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    eval_file_names = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(datapath):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = f\"{dirpath}/{fn}\"\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    df_list = []\n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler\n",
    "\n",
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['evaluation_label']\n",
    "    print(sfeatures)\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '/*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(file_name)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        d_sample = pd.read_csv(file)\n",
    "#         grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            num_rows = 64\n",
    "            pos_row = a[sfeatures].drop('evaluation_label',axis=1)\n",
    "            negatives_filtered = cell[1][cell[1]['evaluation_label'] == -1]\n",
    "            sorted_df = negatives_filtered.sort_values('lof-graph-embedding-score',ascending=False)\n",
    "            sorted_df = sorted_df[sfeatures]\n",
    "            if 0 in sorted_df['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if sorted_df.empty:\n",
    "                continue\n",
    "            neg_list = []\n",
    "            if num_rows < len(sorted_df):\n",
    "                sorted_df = sorted_df[sorted_df['evaluation_label'] == -1]\n",
    "                neg_list.append(sorted_df[:2])\n",
    "                retrieval_score_df = sorted_df[2:].sort_values('retrieval_score',ascending=False)\n",
    "                neg_list.append(retrieval_score_df[:2])\n",
    "                pagerank_score_df = retrieval_score_df[2:].sort_values('pagerank', ascending=False)\n",
    "                neg_list.append(pagerank_score_df[:2])\n",
    "                class_count_score_df = pagerank_score_df[2:].sort_values('lof_class_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(class_count_score_df[:2])\n",
    "                prop_count_score_df = class_count_score_df[2:].sort_values('lof_property_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(prop_count_score_df[:2])\n",
    "                monge_elkan_score_df = prop_count_score_df[2:].sort_values('monge_elkan', ascending=False)\n",
    "                neg_list.append(monge_elkan_score_df[:2])\n",
    "                monge_elkan_alias_score_df = monge_elkan_score_df[2:].sort_values('monge_elkan_aliases', ascending=False)\n",
    "                neg_list.append(monge_elkan_alias_score_df[:2])\n",
    "                \n",
    "                context_score_df = monge_elkan_alias_score_df[2:].sort_values('context_score', ascending=False)\n",
    "                neg_list.append(context_score_df[:2])\n",
    "\n",
    "                jaro_winkler_score_df = monge_elkan_alias_score_df[2:].sort_values('jaro_winkler', ascending=False)\n",
    "                neg_list.append(jaro_winkler_score_df[:2])\n",
    "                \n",
    "                top_sample_df = jaro_winkler_score_df.sample(n=50)\n",
    "                neg_list.append(top_sample_df)\n",
    "                top_sample_df = pd.concat(neg_list)\n",
    "                top_sample_df.drop('evaluation_label', inplace=True, axis=1)\n",
    "                top_sample_arr = top_sample_df.to_numpy()\n",
    "\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                neg_features.append(top_sample_arr[i])\n",
    "            random.shuffle(neg_features)\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                pos_row_sample = pos_row.sample(n=1)\n",
    "                ar = pos_row_sample.to_numpy()\n",
    "                for ps_ar in ar:\n",
    "                    pos_features.append(ps_ar)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "    print(len(positive_features_final), len(positive_features_final[37]))\n",
    "    print(len(negative_features_final), len(negative_features_final[37]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "former-killer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pagerank', 'retrieval_score', 'monge_elkan', 'monge_elkan_aliases', 'des_cont_jaccard', 'jaro_winkler', 'levenshtein', 'singleton', 'num_char', 'num_tokens', 'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score', 'lof-graph-embedding-score', 'lof-reciprocal-rank', 'context_score', 'evaluation_label']\n",
      "58891288_0_1117541047012405958.csv\n",
      "ZX8GERJC.csv\n",
      "8ZD74BO9.csv\n",
      "W0ZNF869.csv\n",
      "AM1UELOJ.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "5IXA0RAI.csv\n",
      "8EFC5XVR.csv\n",
      "DPUA686B.csv\n",
      "UMMA6HQO.csv\n",
      "ERPSWFMM.csv\n",
      "ZDAZ5PQ5.csv\n",
      "XF412HIL.csv\n",
      "BQ36GYQE.csv\n",
      "CKRLO13X.csv\n",
      "L5LFLQIN.csv\n",
      "J6SSKET3.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "T8SL8HGK.csv\n",
      "JUFYSXYP.csv\n",
      "CYYO69JB.csv\n",
      "YMHERMQV.csv\n",
      "6XCOGRWM.csv\n",
      "WNKF57RH.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "OMJX8TT6.csv\n",
      "IUBTQXYO.csv\n",
      "0XXGVKA8.csv\n",
      "57681CMM.csv\n",
      "VE3T1LHT.csv\n",
      "4KGRZFTI.csv\n",
      "UU8Q91MG.csv\n",
      "75MLA4XJ.csv\n",
      "U5L8U1OL.csv\n",
      "QDJ86U5I.csv\n",
      "384SR1N3.csv\n",
      "PG0TP6O0.csv\n",
      "VFVMRNF9.csv\n",
      "CCCNRESE.csv\n",
      "7ZQB5C2O.csv\n",
      "LTZQIN2R.csv\n",
      "QID3PSI3.csv\n",
      "NXBVTACX.csv\n",
      "DKRE7U28.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "U8BHYWZ7.csv\n",
      "2389HYHH.csv\n",
      "2LM6W2JV.csv\n",
      "B735JU5L.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "6LSIYDYN.csv\n",
      "S2LGMOGC.csv\n",
      "SVPAISS7.csv\n",
      "0H0U54UZ.csv\n",
      "4W3919II.csv\n",
      "AGCHY9JJ.csv\n",
      "G2K4GSYB.csv\n",
      "RBIVOB6Q.csv\n",
      "2JN1R1VW.csv\n",
      "6NO3AH02.csv\n",
      "ABRT6AWH.csv\n",
      "KUN2Y3DX.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "24VU5BR7.csv\n",
      "PQN3CY7B.csv\n",
      "NA24I27F.csv\n",
      "9SERGNIZ.csv\n",
      "R4K6322V.csv\n",
      "5W99BCM4.csv\n",
      "7XB008OM.csv\n",
      "6VLKFW8J.csv\n",
      "HFRDW66L.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "3IB68W0T.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "JZ9RW99R.csv\n",
      "RDNZHHGI.csv\n",
      "MOX6MBH5.csv\n",
      "0ZH7HCT0.csv\n",
      "OJEI7G4L.csv\n",
      "6ED9WFUN.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "E0LR4TZL.csv\n",
      "YS86QOSL.csv\n",
      "T3W112BN.csv\n",
      "RLMB7HEB.csv\n",
      "GX0WEFG7.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "5DNR9AW5.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "3TIMOEBD.csv\n",
      "OXLTY5IY.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "MBM31U4C.csv\n",
      "0MZX65PH.csv\n",
      "Y3XLQHGC.csv\n",
      "2UETZ4XK.csv\n",
      "OAWHF5BM.csv\n",
      "3YNPHVPV.csv\n",
      "YXYVNO79.csv\n",
      "1XNHBBRZ.csv\n",
      "M0XIN8I8.csv\n",
      "QIBT0IBA.csv\n",
      "SY4CRLEA.csv\n",
      "SU8B1A6K.csv\n",
      "XVMPJ993.csv\n",
      "I7JP3F91.csv\n",
      "M73R9KGL.csv\n",
      "7YVY712V.csv\n",
      "A3UJTT4U.csv\n",
      "TZ10O44M.csv\n",
      "USXB8M5L.csv\n",
      "S6RYCPCW.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n",
      "9493 68\n",
      "9493 68\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_feature_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "generate_train_data(gen_training_data_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-sponsorship",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "apparent-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-annual",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "general-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = features\n",
    "    for file in glob.glob(input_table_path + '/*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        if file_name != '52299421_0_4473286348258170200.csv':\n",
    "            print(file_name)\n",
    "            d_sample = pd.read_csv(file)\n",
    "            grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "            new_df_list = []\n",
    "            pred = []\n",
    "            for cell in grouped_obj:\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                sorted_df = cell[1].sort_values('lof-graph-embedding-score',ascending=False)[:64]\n",
    "                sorted_df_features = sorted_df[normalize_features]\n",
    "                new_df_list.append(sorted_df)\n",
    "                arr = sorted_df_features.to_numpy()\n",
    "                test_inp = []\n",
    "                for a in arr:\n",
    "                    test_inp.append(a)\n",
    "                test_tensor = torch.tensor(test_inp).float()\n",
    "                scores = model.predict(test_tensor)\n",
    "                pred.extend(torch.squeeze(scores).tolist())\n",
    "            test_df = pd.concat(new_df_list)\n",
    "            test_df[final_score_column] = pred\n",
    "            test_df.to_csv(f\"{output_table_path}/{file_name}\", index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, final_score_column)\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            best_model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "public-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if fn != '52299421_0_4473286348258170200.csv':\n",
    "                if \"csv\" not in fn:\n",
    "                    continue\n",
    "                abs_fn = os.path.join(dirpath, fn)\n",
    "                assert os.path.isfile(abs_fn)\n",
    "                if os.path.getsize(abs_fn) == 0:\n",
    "                    continue\n",
    "                eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "        s_data = c_e_data.sort_values(by=['lof-graph-embedding-score'], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_graph_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "        cf_e_data = c_e_data.copy()\n",
    "        cf_e_data['lof-graph-embedding-score'] = cf_e_data['lof-graph-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "    candidate_eval_data['lof-graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "    candidate_eval_data['lof-graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['lof-graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "    res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "    res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "    res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fossil-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_feature_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "killing-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "10087it [00:19, 517.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "48it [00:00, 475.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.1427529752254486, epoch top1 0.8022598870056498, max top1 0.8022598870056498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 488.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "104it [00:00, 516.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.1053028553724289, epoch top1 0.5819209039548022, max top1 0.8022598870056498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:18, 531.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "103it [00:00, 512.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.10776866227388382, epoch top1 0.806497175141243, max top1 0.806497175141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 522.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 3, Avg Loss is 0.10439883917570114, epoch top1 0.7471751412429378, max top1 0.806497175141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "10087it [00:18, 538.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "95it [00:00, 482.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.11187434196472168, epoch top1 0.4661016949152542, max top1 0.806497175141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 523.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "45it [00:00, 445.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.11552786082029343, epoch top1 0.8728813559322034, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 510.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "48it [00:00, 471.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.10210538655519485, epoch top1 0.4463276836158192, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:22, 442.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "95it [00:00, 472.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.10487081855535507, epoch top1 0.4872881355932203, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 499.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "94it [00:00, 463.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.1108384057879448, epoch top1 0.731638418079096, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 524.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "98it [00:00, 487.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.10709555447101593, epoch top1 0.7951977401129944, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:22, 454.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "97it [00:00, 481.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.10172771662473679, epoch top1 0.5141242937853108, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 491.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "49it [00:00, 482.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.10210956633090973, epoch top1 0.7387005649717514, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 508.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "45it [00:00, 449.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.11249137669801712, epoch top1 0.53954802259887, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 490.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "50it [00:00, 495.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.11516843736171722, epoch top1 0.693502824858757, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 496.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "92it [00:00, 458.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.11205058544874191, epoch top1 0.8601694915254238, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 499.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "49it [00:00, 482.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.13062135875225067, epoch top1 0.7217514124293786, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:19, 509.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "100it [00:00, 499.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.10613536834716797, epoch top1 0.6779661016949152, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 494.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "98it [00:00, 486.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.10379068553447723, epoch top1 0.8686440677966102, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 492.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-15-7a187275f3b9>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-15-7a187275f3b9>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "75it [00:00, 371.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.10788590461015701, epoch top1 0.748587570621469, max top1 0.8728813559322034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10087it [00:20, 492.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 19, Avg Loss is 0.10643847286701202, epoch top1 0.6836158192090396, max top1 0.8728813559322034\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "best_model_path = train(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vertical-pocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amandeep/Github/table-linker/data/SemTabR4_T2dv2/table-linker/dev-output/7_0/saved_models/epoch_5_loss_0.11552786082029343_top1_0.8728813559322034.pth'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-consultation",
   "metadata": {},
   "source": [
    "## Dev Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sixth-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_prediction(dev_feature_path, dev_predictions_top_k, saved_model, output_column, min_max_scaler_path, k=5):\n",
    "    for file in glob.glob(dev_feature_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        feature_str =  \",\".join(features)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        # location where the output generated by the predictions wil be stored.\n",
    "        dev_output = f\"{dev_predictions_top_k}/{filename}\"\n",
    "        !tl predict-using-model $file -o $output_column \\\n",
    "            --features $feature_str \\\n",
    "            --ranking-model $saved_model \\\n",
    "            --normalization-factor $min_max_scaler_path \\\n",
    "            / get-kg-links -c $output_column -k $k --k-rows \\\n",
    "            > $dev_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "noted-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_color(dev_predictions_top_k, dev_colorized_path, score_column, k=5):\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_color_file = f\"{dev_colorized_path}/{filename.strip('.csv')}.xlsx\"\n",
    "        !tl add-color $file -c \"$score_column,evaluation_label\" -k $k --output $dev_color_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "narrative-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dev_predictions_top_k, dev_predictions_metrics, score_column, k=5):\n",
    "    df_list = []\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_metrics_file = f\"{dev_predictions_metrics}/{filename}\"\n",
    "        !tl metrics $file -k $k -c $score_column --tag $filename> $dev_metrics_file\n",
    "        df_list.append(pd.read_csv(dev_metrics_file))\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "finished-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "52299421_0_4473286348258170200.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "dev_prediction(dev_feature_path, dev_predictions_top_k, best_model_path, final_score_column, min_max_scaler_path, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "elder-junior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "52299421_0_4473286348258170200.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_df = compute_metrics(dev_predictions_top_k, dev_metrics_path, final_score_column, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abstract-review",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39759273_0_1427898308030295194.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45073662_0_3179937335063201739.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29414811_2_4773219892816395776.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>84575189_0_6365692015941409487.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>14380604_4_3329235705746762392.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>52299421_0_4473286348258170200.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50270082_0_444360818941411589.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.898695</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.959091</td>\n",
       "      <td>28086084_0_3127660530989916727.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>14067031_0_559833072073397908.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k        f1  precision    recall                                 tag\n",
       "0  200  0.994975   0.990000  1.000000  39759273_0_1427898308030295194.csv\n",
       "0  200  1.000000   1.000000  1.000000  45073662_0_3179937335063201739.csv\n",
       "0  200  0.926829   0.863636  1.000000  29414811_2_4773219892816395776.csv\n",
       "0  200  0.968421   0.938776  1.000000  84575189_0_6365692015941409487.csv\n",
       "0  200  0.950000   0.950000  0.950000  14380604_4_3329235705746762392.csv\n",
       "0  200  0.890110   0.890110  0.890110  52299421_0_4473286348258170200.csv\n",
       "0  200  0.994012   0.988095  1.000000   50270082_0_444360818941411589.csv\n",
       "0  200  0.898695   0.845455  0.959091  28086084_0_3127660530989916727.csv\n",
       "0  200  0.943396   0.943396  0.943396   14067031_0_559833072073397908.csv"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "prepared-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.971399669512877"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df['recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "looking-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{dev_metrics_path}/metrics_200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "transparent-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "52299421_0_4473286348258170200.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_df = compute_metrics(dev_predictions_top_k, dev_metrics_path, final_score_column, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "processed-verse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.969588</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>39759273_0_1427898308030295194.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>45073662_0_3179937335063201739.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.555195</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>29414811_2_4773219892816395776.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.890496</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>84575189_0_6365692015941409487.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>14380604_4_3329235705746762392.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.884581</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>52299421_0_4473286348258170200.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.976045</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>50270082_0_444360818941411589.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.843176</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>28086084_0_3127660530989916727.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>14067031_0_559833072073397908.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k        f1  precision    recall                                 tag\n",
       "0  1  0.969588   0.990000  0.950000  39759273_0_1427898308030295194.csv\n",
       "0  1  0.981132   1.000000  0.962963  45073662_0_3179937335063201739.csv\n",
       "0  1  0.555195   0.863636  0.409091  29414811_2_4773219892816395776.csv\n",
       "0  1  0.890496   0.938776  0.846939  84575189_0_6365692015941409487.csv\n",
       "0  1  0.950000   0.950000  0.950000  14380604_4_3329235705746762392.csv\n",
       "0  1  0.884581   0.890110  0.879121  52299421_0_4473286348258170200.csv\n",
       "0  1  0.976045   0.988095  0.964286   50270082_0_444360818941411589.csv\n",
       "0  1  0.843176   0.845455  0.840909  28086084_0_3127660530989916727.csv\n",
       "0  1  0.943396   0.943396  0.943396   14067031_0_559833072073397908.csv"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "incorporated-supervision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8881787342690379"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "legislative-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{dev_metrics_path}/metrics_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rapid-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "52299421_0_4473286348258170200.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "add_color(dev_predictions_top_k, dev_colorized_path, final_score_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-scheme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
