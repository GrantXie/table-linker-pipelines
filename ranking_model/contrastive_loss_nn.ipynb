{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprising-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import \n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import shutil\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-strain",
   "metadata": {},
   "source": [
    "I assume that the candidate generation and feature genration has already be run on the training and dev tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charming-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented'\n",
    "\n",
    "work_dir = '/Users/amandeep/Github/table-linker/data/t2dv2'\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = f'{work_dir}/t2dv2-train-canonical/'\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = f'{work_dir}/t2dv2-dev-canonical/'\n",
    "\n",
    "VERSION = \"1.0\"\n",
    "\n",
    "train_candidate_path = f'{train_path}{VERSION}/candidates/'\n",
    "train_feature_path = f'{train_path}{VERSION}/features/'\n",
    "\n",
    "dev_candidate_path = f'{dev_path}{VERSION}/candidates/'\n",
    "dev_feature_path = f'{dev_path}{VERSION}/features/'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = f'{work_dir}/round_1_GT/'\n",
    "classifier_model_path = '/Users/amandeep/Github/table-linker-pipelines/table-linker-full-pipeline/models/weighted_lr.pkl'\n",
    "\n",
    "\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/pos_features.pkl\n",
    "pos_output = f'{work_dir}training_data/pos_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/neg_features.pkl\n",
    "neg_output = f'{work_dir}training_data/neg_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/normalization_factor.pkl\n",
    "min_max_scaler_path = f'{work_dir}training_data/normalization_factor.pkl'\n",
    "dev_output_predictions = f'{work_dir}dev_predictions/'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/saved_models\n",
    "model_save_path = f'{work_dir}saved_models/'\n",
    "\n",
    "aux_field = 'graph_embedding_complex,class_count,property_count,context'\n",
    "temp_dir = f'{work_dir}temp/' #temp directory to store intermediate files\n",
    "\n",
    "#directory to store the property count file for each table. Can be directly used for computing the tf-idf features \n",
    "#without running the candidate generation process again which is expensive\n",
    "\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/train_prop_count/\n",
    "train_prop_count = f'{temp_dir}train_prop_count/' \n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/dev_prop_count/\n",
    "dev_prop_count = f'{temp_dir}dev_prop_count/'\n",
    "\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/train_class_count/\n",
    "train_class_count = f'{temp_dir}train_class_count/'\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/dev_class_count/\n",
    "dev_class_count = f'{temp_dir}dev_class_count/'\n",
    "\n",
    "train_context_path = f'{temp_dir}train_context/'\n",
    "dev_context_path = f'{temp_dir}dev_context/'\n",
    "\n",
    "train_graph_embedding = f'{temp_dir}train_graph_embedding/'\n",
    "dev_graph_embedding = f'{temp_dir}dev_graph_embedding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "minimal-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {dev_output_predictions}\n",
    "!mkdir -p {model_save_path}\n",
    "!mkdir -p {working_path}/training_data\n",
    "!mkdir -p $temp_dir\n",
    "!mkdir -p $train_prop_count\n",
    "!mkdir -p $dev_prop_count\n",
    "!mkdir -p $train_class_count\n",
    "!mkdir -p $dev_class_count\n",
    "!mkdir -p $train_graph_embedding\n",
    "!mkdir -p $dev_graph_embedding\n",
    "!mkdir -p $train_candidate_path\n",
    "!mkdir -p $dev_candidate_path\n",
    "!mkdir -p $train_feature_path\n",
    "!mkdir -p $dev_feature_path\n",
    "!mkdir -p $train_context_path\n",
    "!mkdir -p $dev_context_path\n",
    "!mkdir -p $work_dir/training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sophisticated-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pagerank','retrieval_score','monge_elkan','monge_elkan_aliases','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-prince",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rural-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    for i, file in enumerate(glob.glob(path + '*.csv')):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        print(filename)\n",
    "        gt_file = os.path.join(ground_truth_files, filename)\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "        \n",
    "        !tl clean -c label -o label_clean $file / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder $temp_dir / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder {temp_dir} / \\\n",
    "        ground-truth-labeler --gt-file $gt_file > $output_file\n",
    "        \n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = os.path.join(class_count_path, filename.strip('.csv') + '_class_count.tsv')\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = os.path.join(prop_count_path, filename.strip('.csv') + '_prop_count.tsv')\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            elif field == 'context':\n",
    "                context_file = os.path.join(context_path, filename.strip('.csv') + '_context.tsv')\n",
    "                aux_df.to_csv(context_file, sep='\\t', index=False)\n",
    "\n",
    "            else:\n",
    "                graph_embedding_file = os.path.join(graph_embedding, filename.strip('.csv') + '_graph_embedding_complex.tsv')\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        \n",
    "        print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preliminary-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "15.631978034973145\n",
      "39173938_0_7916056990138658530.csv\n",
      "17.997843980789185\n",
      "10579449_0_1681126353774891032.csv\n",
      "11.222554206848145\n",
      "33401079_0_9127583903019856402.csv\n",
      "20.534048795700073\n",
      "21362676_0_6854186738074119688.csv\n",
      "14.794913053512573\n",
      "38428277_0_1311643810102462607.csv\n",
      "16.37204599380493\n",
      "91959037_0_7907661684242014480.csv\n",
      "38.14062809944153\n",
      "20135078_0_7570343137119682530.csv\n",
      "16.241691827774048\n",
      "35188621_0_6058553107571275232.csv\n",
      "17.636343002319336\n",
      "54719588_0_8417197176086756912.csv\n",
      "30.55109405517578\n",
      "21245481_0_8730460088443117515.csv\n",
      "24.624454021453857\n",
      "71840765_0_6664391841933033844.csv\n",
      "7.3358399868011475\n",
      "8468806_0_4382447409703007384.csv\n",
      "12.98465895652771\n",
      "88523363_0_8180214313099580515.csv\n",
      "43.32491207122803\n",
      "29414811_13_8724394428539174350.csv\n",
      "6.715674161911011\n",
      "99070098_0_2074872741302696997.csv\n",
      "22.538585901260376\n",
      "43237185_1_3636357855502246981.csv\n",
      "11.891586065292358\n",
      "46671561_0_6122315295162029872.csv\n",
      "22.55510687828064\n",
      "53989675_0_8697482470743954630.csv\n",
      "11.430384159088135\n",
      "25404227_0_2240631045609013057.csv\n",
      "17.428587913513184\n",
      "9834884_0_3871985887467090123.csv\n",
      "39.01950287818909\n",
      "63450419_0_8012592961815711786.csv\n",
      "16.99641442298889\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "6.761589765548706\n",
      "22864497_0_8632623712684511496.csv\n",
      "37.556106090545654\n",
      "53822652_0_5767892317858575530.csv\n",
      "55.87173509597778\n",
      "37856682_0_6818907050314633217.csv\n",
      "41.487581968307495\n",
      "26310680_0_5150772059999313798.csv\n",
      "32.17906713485718\n",
      "29414811_12_251152470253168163.csv\n",
      "11.977322101593018\n",
      "69537082_0_7789694313271016902.csv\n",
      "34.699711084365845\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "12.276924848556519\n",
      "60319454_0_3938426910282115527.csv\n",
      "14.054846048355103\n",
      "16767252_0_2409448375013995751.csv\n",
      "12.745820760726929\n",
      "84548468_0_5955155464119382182.csv\n",
      "13.475652933120728\n",
      "80588006_0_6965325215443683359.csv\n",
      "6.662214040756226\n",
      "39650055_5_7135804139753401681.csv\n",
      "15.295292854309082\n",
      "40534006_0_4617468856744635526.csv\n",
      "10.859890937805176\n",
      "90196673_0_5458330029110291950.csv\n",
      "35.89137101173401\n",
      "24036779_0_5608105867560183058.csv\n",
      "16.88730478286743\n",
      "9567241_0_5666388268510912770.csv\n",
      "10.512878179550171\n",
      "41480166_0_6681239260286218499.csv\n",
      "28.989279985427856\n",
      "77694908_0_6083291340991074532.csv\n",
      "16.546757221221924\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "6.986270904541016\n",
      "39107734_2_2329160387535788734.csv\n",
      "9.951543807983398\n",
      "50245608_0_871275842592178099.csv\n",
      "30.853371143341064\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_context_path,train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recorded-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "14.00437617301941\n",
      "45073662_0_3179937335063201739.csv\n",
      "10.836252927780151\n",
      "29414811_2_4773219892816395776.csv\n",
      "10.595270156860352\n",
      "84575189_0_6365692015941409487.csv\n",
      "20.619489192962646\n",
      "14380604_4_3329235705746762392.csv\n",
      "11.689258098602295\n",
      "52299421_0_4473286348258170200.csv\n",
      "14.647533893585205\n",
      "50270082_0_444360818941411589.csv\n",
      "24.478726863861084\n",
      "28086084_0_3127660530989916727.csv\n",
      "25.04857301712036\n",
      "14067031_0_559833072073397908.csv\n",
      "13.739971160888672\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_context_path, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-testing",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(candidate_dir, embedding_dir, class_count_dir, property_count_dir, output_path):\n",
    "    print(candidate_dir)\n",
    "    print(embedding_dir)\n",
    "    print(class_count_dir)\n",
    "    print(property_count_dir)\n",
    "    print(output_path)\n",
    "    for file in glob.glob(candidate_dir + '*.csv'):\n",
    "        filename = file.split('/')[-1]\n",
    "        print(filename)\n",
    "        embedding_file = os.path.join(embedding_dir, filename.strip('.csv') + '_graph_embedding_complex.tsv')\n",
    "        class_count_file = f\"{class_count_dir}{filename.strip('.csv')}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}{filename.strip('.csv')}_prop_count.tsv\"\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "        !tl align-page-rank $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein \\\n",
    "            / string-similarity -i --method jaccard:tokenizer=word -c kg_descriptions context -o des_cont_jaccard \\\n",
    "            / normalize-scores -c des_cont_jaccard / smallest-qnode-number \\\n",
    "            / mosaic-features -c kg_labels --num-char --num-tokens \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            / vote-by-classifier  \\\n",
    "            --prob-threshold 0.995 \\\n",
    "            --model $classifier_model_path \\\n",
    "            / score-using-embedding \\\n",
    "            --column-vector-strategy centroid-of-lof \\\n",
    "            --lof-strategy ems-mv \\\n",
    "            -o lof-graph-embedding-score \\\n",
    "            --embedding-file $embedding_file \\\n",
    "            --embedding-url \"$es_url/$es_index/\" \\\n",
    "            / generate-reciprocal-rank  \\\n",
    "            -c lof-graph-embedding-score \\\n",
    "            -o lof-reciprocal-rank \\\n",
    "            / compute-tf-idf  \\\n",
    "            --feature-file $class_count_file \\\n",
    "            --feature-name class_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_class_count_tf_idf_score \\\n",
    "            / compute-tf-idf \\\n",
    "            --feature-file $property_count_file \\\n",
    "            --feature-name property_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_property_count_tf_idf_score \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tested-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-train-canonical/1.0/candidates/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_graph_embedding/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_class_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_prop_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-train-canonical/1.0/features/\n",
      "58891288_0_1117541047012405958.csv\n",
      "Qnodes to lookup: 10717\n",
      "Qnodes from file: 10399\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 86 lof-voted candidates\n",
      "39173938_0_7916056990138658530.csv\n",
      "Qnodes to lookup: 9986\n",
      "Qnodes from file: 9718\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 73 lof-voted candidates\n",
      "10579449_0_1681126353774891032.csv\n",
      "Qnodes to lookup: 1706\n",
      "Qnodes from file: 1652\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "33401079_0_9127583903019856402.csv\n",
      "Qnodes to lookup: 8276\n",
      "Qnodes from file: 8183\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 50 lof-voted candidates\n",
      "21362676_0_6854186738074119688.csv\n",
      "Qnodes to lookup: 10157\n",
      "Qnodes from file: 9877\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 104 lof-voted candidates\n",
      "38428277_0_1311643810102462607.csv\n",
      "Qnodes to lookup: 12595\n",
      "Qnodes from file: 12188\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 115 lof-voted candidates\n",
      "91959037_0_7907661684242014480.csv\n",
      "Qnodes to lookup: 22297\n",
      "Qnodes from file: 21621\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "20135078_0_7570343137119682530.csv\n",
      "Qnodes to lookup: 11208\n",
      "Qnodes from file: 10916\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "35188621_0_6058553107571275232.csv\n",
      "Qnodes to lookup: 11833\n",
      "Qnodes from file: 11460\n",
      "Qnodes from server: 0\n",
      "_centroid_of_lof: Missing 1 of 164\n",
      "Outlier removal generates 98 lof-voted candidates\n",
      "54719588_0_8417197176086756912.csv\n",
      "Qnodes to lookup: 24211\n",
      "Qnodes from file: 23278\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 45 lof-voted candidates\n",
      "21245481_0_8730460088443117515.csv\n",
      "Qnodes to lookup: 11848\n",
      "Qnodes from file: 11696\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "71840765_0_6664391841933033844.csv\n",
      "Qnodes to lookup: 1100\n",
      "Qnodes from file: 1094\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "8468806_0_4382447409703007384.csv\n",
      "Qnodes to lookup: 6738\n",
      "Qnodes from file: 6671\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "88523363_0_8180214313099580515.csv\n",
      "Qnodes to lookup: 42234\n",
      "Qnodes from file: 41968\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 35 lof-voted candidates\n",
      "29414811_13_8724394428539174350.csv\n",
      "Qnodes to lookup: 1140\n",
      "Qnodes from file: 1102\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "99070098_0_2074872741302696997.csv\n",
      "Qnodes to lookup: 12078\n",
      "Qnodes from file: 11859\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 15 lof-voted candidates\n",
      "43237185_1_3636357855502246981.csv\n",
      "Qnodes to lookup: 2611\n",
      "Qnodes from file: 2573\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "46671561_0_6122315295162029872.csv\n",
      "Qnodes to lookup: 15698\n",
      "Qnodes from file: 15130\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 66 lof-voted candidates\n",
      "53989675_0_8697482470743954630.csv\n",
      "Qnodes to lookup: 2161\n",
      "Qnodes from file: 2144\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "25404227_0_2240631045609013057.csv\n",
      "Qnodes to lookup: 10026\n",
      "Qnodes from file: 9732\n",
      "Qnodes from server: 0\n",
      "_centroid_of_lof: Missing 1 of 160\n",
      "Outlier removal generates 96 lof-voted candidates\n",
      "9834884_0_3871985887467090123.csv\n",
      "Qnodes to lookup: 22923\n",
      "Qnodes from file: 22574\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 56 lof-voted candidates\n",
      "63450419_0_8012592961815711786.csv\n",
      "Qnodes to lookup: 13769\n",
      "Qnodes from file: 13691\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "Qnodes to lookup: 1237\n",
      "Qnodes from file: 1179\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "22864497_0_8632623712684511496.csv\n",
      "Qnodes to lookup: 29456\n",
      "Qnodes from file: 28330\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 48 lof-voted candidates\n",
      "53822652_0_5767892317858575530.csv\n",
      "Qnodes to lookup: 43140\n",
      "Qnodes from file: 41723\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 76 lof-voted candidates\n",
      "37856682_0_6818907050314633217.csv\n",
      "Qnodes to lookup: 43381\n",
      "Qnodes from file: 42173\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 272 lof-voted candidates\n",
      "26310680_0_5150772059999313798.csv\n",
      "Qnodes to lookup: 35193\n",
      "Qnodes from file: 34092\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 200 lof-voted candidates\n",
      "29414811_12_251152470253168163.csv\n",
      "Qnodes to lookup: 1857\n",
      "Qnodes from file: 1790\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "69537082_0_7789694313271016902.csv\n",
      "Qnodes to lookup: 36385\n",
      "Qnodes from file: 35280\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 226 lof-voted candidates\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "Qnodes to lookup: 980\n",
      "Qnodes from file: 976\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "60319454_0_3938426910282115527.csv\n",
      "Qnodes to lookup: 5005\n",
      "Qnodes from file: 4886\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "16767252_0_2409448375013995751.csv\n",
      "Qnodes to lookup: 8870\n",
      "Qnodes from file: 8626\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 88 lof-voted candidates\n",
      "84548468_0_5955155464119382182.csv\n",
      "Qnodes to lookup: 10574\n",
      "Qnodes from file: 10229\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 78 lof-voted candidates\n",
      "80588006_0_6965325215443683359.csv\n",
      "Qnodes to lookup: 1855\n",
      "Qnodes from file: 1809\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "39650055_5_7135804139753401681.csv\n",
      "Qnodes to lookup: 13681\n",
      "Qnodes from file: 13068\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 30 lof-voted candidates\n",
      "40534006_0_4617468856744635526.csv\n",
      "Qnodes to lookup: 4092\n",
      "Qnodes from file: 4034\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "90196673_0_5458330029110291950.csv\n",
      "Qnodes to lookup: 22360\n",
      "Qnodes from file: 21926\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 21 lof-voted candidates\n",
      "24036779_0_5608105867560183058.csv\n",
      "Qnodes to lookup: 13908\n",
      "Qnodes from file: 13592\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 114 lof-voted candidates\n",
      "9567241_0_5666388268510912770.csv\n",
      "Qnodes to lookup: 1922\n",
      "Qnodes from file: 1861\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "41480166_0_6681239260286218499.csv\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "77694908_0_6083291340991074532.csv\n",
      "Qnodes to lookup: 10760\n",
      "Qnodes from file: 10387\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 96 lof-voted candidates\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "Qnodes to lookup: 1234\n",
      "Qnodes from file: 1218\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "39107734_2_2329160387535788734.csv\n",
      "Qnodes to lookup: 3480\n",
      "Qnodes from file: 3403\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 42 lof-voted candidates\n",
      "50245608_0_871275842592178099.csv\n",
      "Qnodes to lookup: 26520\n",
      "Qnodes from file: 25679\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 151 lof-voted candidates\n"
     ]
    }
   ],
   "source": [
    "feature_generation(train_candidate_path, train_graph_embedding, train_class_count, train_prop_count, train_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "graduate-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-dev-canonical/1.0/candidates/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_graph_embedding/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_class_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_prop_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-dev-canonical/1.0/features/\n",
      "39759273_0_1427898308030295194.csv\n",
      "Qnodes to lookup: 10448\n",
      "Qnodes from file: 10120\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "45073662_0_3179937335063201739.csv\n",
      "Qnodes to lookup: 3040\n",
      "Qnodes from file: 3004\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "29414811_2_4773219892816395776.csv\n",
      "Qnodes to lookup: 2106\n",
      "Qnodes from file: 2025\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "84575189_0_6365692015941409487.csv\n",
      "Qnodes to lookup: 8486\n",
      "Qnodes from file: 7897\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "14380604_4_3329235705746762392.csv\n",
      "Qnodes to lookup: 2291\n",
      "Qnodes from file: 2226\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "52299421_0_4473286348258170200.csv\n",
      "Qnodes to lookup: 15531\n",
      "Qnodes from file: 15218\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 83 lof-voted candidates\n",
      "50270082_0_444360818941411589.csv\n",
      "Qnodes to lookup: 16794\n",
      "Qnodes from file: 16683\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "28086084_0_3127660530989916727.csv\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "14067031_0_559833072073397908.csv\n",
      "Qnodes to lookup: 7690\n",
      "Qnodes from file: 7433\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 56 lof-voted candidates\n"
     ]
    }
   ],
   "source": [
    "feature_generation(dev_candidate_path, dev_graph_embedding, dev_class_count, dev_prop_count, dev_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-scale",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fundamental-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    eval_file_names = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(datapath):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = dirpath + fn\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    df_list = []\n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler\n",
    "\n",
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['evaluation_label']\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '/*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(file_name)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        d_sample = pd.read_csv(file)\n",
    "#         grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            num_rows = 64\n",
    "            pos_row = a[sfeatures].drop('evaluation_label',axis=1)\n",
    "            negatives_filtered = cell[1][cell[1]['evaluation_label'] == -1]\n",
    "            sorted_df = negatives_filtered.sort_values('lof-graph-embedding-score',ascending=False)\n",
    "            sorted_df = sorted_df[sfeatures]\n",
    "            if 0 in sorted_df['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if sorted_df.empty:\n",
    "                continue\n",
    "            neg_list = []\n",
    "            if num_rows < len(sorted_df):\n",
    "                sorted_df = sorted_df[sorted_df['evaluation_label'] == -1]\n",
    "                neg_list.append(sorted_df[:2])\n",
    "                retrieval_score_df = sorted_df[2:].sort_values('retrieval_score',ascending=False)\n",
    "                neg_list.append(retrieval_score_df[:2])\n",
    "                pagerank_score_df = retrieval_score_df[2:].sort_values('pagerank', ascending=False)\n",
    "                neg_list.append(pagerank_score_df[:2])\n",
    "                class_count_score_df = pagerank_score_df[2:].sort_values('lof_class_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(class_count_score_df[:2])\n",
    "                prop_count_score_df = class_count_score_df[2:].sort_values('lof_property_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(prop_count_score_df[:2])\n",
    "                monge_elkan_score_df = prop_count_score_df[2:].sort_values('monge_elkan', ascending=False)\n",
    "                neg_list.append(monge_elkan_score_df[:2])\n",
    "                monge_elkan_alias_score_df = monge_elkan_score_df[2:].sort_values('monge_elkan_aliases', ascending=False)\n",
    "                neg_list.append(monge_elkan_alias_score_df[:2])\n",
    "\n",
    "                jaro_winkler_score_df = monge_elkan_alias_score_df[2:].sort_values('jaro_winkler', ascending=False)\n",
    "                neg_list.append(jaro_winkler_score_df[:2])\n",
    "                top_sample_df = jaro_winkler_score_df.sample(n=50)\n",
    "                neg_list.append(top_sample_df)\n",
    "                top_sample_df = pd.concat(neg_list)\n",
    "                top_sample_df.drop('evaluation_label', inplace=True, axis=1)\n",
    "                top_sample_arr = top_sample_df.to_numpy()\n",
    "\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                neg_features.append(top_sample_arr[i])\n",
    "            random.shuffle(neg_features)\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                pos_row_sample = pos_row.sample(n=1)\n",
    "                ar = pos_row_sample.to_numpy()\n",
    "                for ps_ar in ar:\n",
    "                    pos_features.append(ps_ar)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "    print(len(positive_features_final), len(positive_features_final[37]))\n",
    "    print(len(negative_features_final), len(negative_features_final[37]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exact-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n",
      "5578 66\n",
      "5578 66\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_feature_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "generate_train_data(gen_training_data_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-sustainability",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "serial-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-pixel",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "atmospheric-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = features\n",
    "    for file in glob.glob(input_table_path + '*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        if file_name != '52299421_0_4473286348258170200.csv':\n",
    "            print(file_name)\n",
    "            d_sample = pd.read_csv(file)\n",
    "            grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "            new_df_list = []\n",
    "            pred = []\n",
    "            for cell in grouped_obj:\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                sorted_df = cell[1].sort_values('lof-graph-embedding-score',ascending=False)[:64]\n",
    "                sorted_df_features = sorted_df[normalize_features]\n",
    "                new_df_list.append(sorted_df)\n",
    "                arr = sorted_df_features.to_numpy()\n",
    "                test_inp = []\n",
    "                for a in arr:\n",
    "                    test_inp.append(a)\n",
    "                test_tensor = torch.tensor(test_inp).float()\n",
    "                scores = model.predict(test_tensor)\n",
    "                pred.extend(torch.squeeze(scores).tolist())\n",
    "            test_df = pd.concat(new_df_list)\n",
    "            test_df['siamese_pred'] = pred\n",
    "            test_df.to_csv(os.path.join(output_table_path, file_name), index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, 'siamese_pred')\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "monetary-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if fn != '52299421_0_4473286348258170200.csv':\n",
    "                if \"csv\" not in fn:\n",
    "                    continue\n",
    "                abs_fn = os.path.join(dirpath, fn)\n",
    "                assert os.path.isfile(abs_fn)\n",
    "                if os.path.getsize(abs_fn) == 0:\n",
    "                    continue\n",
    "                eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "        s_data = c_e_data.sort_values(by=['lof-graph-embedding-score'], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_graph_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "        cf_e_data = c_e_data.copy()\n",
    "        cf_e_data['lof-graph-embedding-score'] = cf_e_data['lof-graph-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "    candidate_eval_data['lof-graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "    candidate_eval_data['lof-graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['lof-graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "    res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "    res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "    res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "productive-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_feature_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "little-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "5753it [00:08, 661.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "100it [00:00, 495.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.11848242580890656, epoch top1 0.8737151248164464, max top1 0.8737151248164464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 668.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "55it [00:00, 549.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.09182703495025635, epoch top1 0.8854625550660793, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 690.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "62it [00:00, 614.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.09619525820016861, epoch top1 0.9001468428781204, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 685.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "61it [00:00, 602.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss is 0.0911773219704628, epoch top1 0.7826725403817915, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 660.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "61it [00:00, 604.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.09382644295692444, epoch top1 0.8340675477239354, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 648.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "55it [00:00, 548.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.10200232267379761, epoch top1 0.8237885462555066, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 698.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "54it [00:00, 535.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.08929978311061859, epoch top1 0.8237885462555066, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 644.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "114it [00:00, 572.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.08926733583211899, epoch top1 0.8795888399412628, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 630.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "112it [00:00, 560.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.09745410829782486, epoch top1 0.6666666666666666, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 632.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "114it [00:00, 569.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.0961243137717247, epoch top1 0.8149779735682819, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 575.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 560.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.09719926118850708, epoch top1 0.7812041116005873, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 584.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "118it [00:00, 590.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.09049894660711288, epoch top1 0.7033773861967695, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 592.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "52it [00:00, 514.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.09249014407396317, epoch top1 0.7738619676945668, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 580.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 562.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.0904867947101593, epoch top1 0.8340675477239354, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 586.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "120it [00:00, 602.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.0908413976430893, epoch top1 0.73568281938326, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 622.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "59it [00:00, 583.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.09629981219768524, epoch top1 0.8032305433186491, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 640.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "58it [00:00, 576.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.10847587883472443, epoch top1 0.6108663729809104, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 624.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 558.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.10744363069534302, epoch top1 0.8281938325991189, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 585.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-af7e7c173294>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-20-af7e7c173294>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "55it [00:00, 540.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.0977567657828331, epoch top1 0.7870778267254038, max top1 0.9001468428781204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 580.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 19, Avg Loss is 0.09634328633546829, epoch top1 0.7900146842878121, max top1 0.9001468428781204\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "train(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-longitude",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
