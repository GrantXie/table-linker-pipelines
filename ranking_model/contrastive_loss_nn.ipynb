{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "grateful-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-thumbnail",
   "metadata": {},
   "source": [
    "I assume that the candidate generation and feature genration has already be run on the training and dev tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reasonable-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented'\n",
    "\n",
    "# Input Paths\n",
    "work_dir = '/Users/amandeep/Github/table-linker/data/t2dv2'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = f'{work_dir}/t2dv2-train-canonical'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = f'{work_dir}/t2dv2-dev-canonical'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = f'{work_dir}/round_1_GT'\n",
    "\n",
    "# can be downloaded from https://github.com/usc-isi-i2/table-linker-pipelines/blob/main/table-linker-full-pipeline/models/weighted_lr.pkl\n",
    "classifier_model_path = '/Users/amandeep/Github/table-linker-pipelines/table-linker-full-pipeline/models/weighted_lr.pkl'\n",
    "\n",
    "\n",
    "# OUTPUT PATHS\n",
    "output_path = '/Users/amandeep/Github/table-linker/data/t2dv2'\n",
    "train_output_path = f'{output_path}/t2dv2-train-canonical-output'\n",
    "dev_output_path = f'{output_path}/t2dv2-dev-canonical-output'\n",
    "\n",
    "# increase version to create a new folder for an experiment\n",
    "VERSION = \"2.0\"\n",
    "\n",
    "train_candidate_path = f'{train_output_path}/{VERSION}/candidates'\n",
    "train_feature_path = f'{train_output_path}/{VERSION}/features'\n",
    "\n",
    "dev_candidate_path = f'{dev_output_path}/{VERSION}/candidates'\n",
    "dev_feature_path = f'{dev_output_path}/{VERSION}/features'\n",
    "\n",
    "aux_field = 'graph_embedding_complex,class_count,property_count,context'\n",
    "\n",
    "\n",
    "train_prop_count = f'{train_output_path}/{VERSION}/train_prop_count' \n",
    "train_class_count = f'{train_output_path}/{VERSION}/train_class_count'\n",
    "train_context_path = f'{train_output_path}/{VERSION}/train_context'\n",
    "train_graph_embedding = f'{train_output_path}/{VERSION}/train_graph_embedding'\n",
    "\n",
    "dev_prop_count = f'{dev_output_path}/{VERSION}/dev_prop_count'\n",
    "dev_class_count = f'{dev_output_path}/{VERSION}/dev_class_count'\n",
    "dev_context_path = f'{dev_output_path}/{VERSION}/dev_context'\n",
    "dev_graph_embedding = f'{dev_output_path}/{VERSION}/dev_graph_embedding'\n",
    "\n",
    "temp_dir = f'{output_path}/temp'\n",
    "\n",
    "\n",
    "\n",
    "pos_output = f'{temp_dir}/training_data/pos_features.pkl'\n",
    "neg_output = f'{temp_dir}/training_data/neg_features.pkl'\n",
    "min_max_scaler_path = f'{temp_dir}/training_data/normalization_factor.pkl'\n",
    "\n",
    "dev_output_predictions = f'{output_path}/dev_predictions'\n",
    "dev_predictions_top_k = f'{output_path}/dev_predictions_top_k'\n",
    "model_save_path = f'{output_path}/saved_models'\n",
    "best_model_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tender-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $temp_dir\n",
    "\n",
    "!mkdir -p $train_prop_count\n",
    "!mkdir -p $dev_prop_count\n",
    "!mkdir -p $train_class_count\n",
    "!mkdir -p $dev_class_count\n",
    "!mkdir -p $train_graph_embedding\n",
    "!mkdir -p $dev_graph_embedding\n",
    "!mkdir -p $train_context_path\n",
    "!mkdir -p $dev_context_path\n",
    "\n",
    "!mkdir -p $train_candidate_path\n",
    "!mkdir -p $dev_candidate_path\n",
    "\n",
    "!mkdir -p $train_feature_path\n",
    "!mkdir -p $dev_feature_path\n",
    "\n",
    "!mkdir -p $temp_dir/training_data\n",
    "!mkdir -p $dev_output_predictions\n",
    "!mkdir -p $model_save_path\n",
    "!mkdir -p $dev_predictions_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modified-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pagerank','retrieval_score','monge_elkan','monge_elkan_aliases','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-plane",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dependent-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    file_list = glob.glob(path + '/*.csv')\n",
    "    for i, file in enumerate(file_list):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        print(f\"{filename}: {i+1} of {len(file_list)}\")\n",
    "        gt_file = f\"{ground_truth_files}/{filename}\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "        !tl clean -c label -o label_clean $file / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder $temp_dir / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder {temp_dir} / \\\n",
    "        ground-truth-labeler --gt-file $gt_file > $output_file\n",
    "        \n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = f\"{class_count_path}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = f\"{prop_count_path}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            elif field == 'context':\n",
    "                context_file = f\"{context_path}/{filename.strip('.csv')}_context.tsv\"\n",
    "                aux_df.to_csv(context_file, sep='\\t', index=False)\n",
    "            else:\n",
    "                graph_embedding_file = f\"{graph_embedding}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        \n",
    "        print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confirmed-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv: 1 of 44\n",
      "18.18380904197693\n",
      "39173938_0_7916056990138658530.csv: 2 of 44\n",
      "16.239722967147827\n",
      "10579449_0_1681126353774891032.csv: 3 of 44\n",
      "8.634151935577393\n",
      "33401079_0_9127583903019856402.csv: 4 of 44\n",
      "16.86306619644165\n",
      "21362676_0_6854186738074119688.csv: 5 of 44\n",
      "16.79141902923584\n",
      "38428277_0_1311643810102462607.csv: 6 of 44\n",
      "18.03566598892212\n",
      "91959037_0_7907661684242014480.csv: 7 of 44\n",
      "32.608768939971924\n",
      "20135078_0_7570343137119682530.csv: 8 of 44\n",
      "18.32354974746704\n",
      "35188621_0_6058553107571275232.csv: 9 of 44\n",
      "19.000466346740723\n",
      "54719588_0_8417197176086756912.csv: 10 of 44\n",
      "30.85878086090088\n",
      "21245481_0_8730460088443117515.csv: 11 of 44\n",
      "26.574946880340576\n",
      "71840765_0_6664391841933033844.csv: 12 of 44\n",
      "14.153718948364258\n",
      "8468806_0_4382447409703007384.csv: 13 of 44\n",
      "14.317156076431274\n",
      "88523363_0_8180214313099580515.csv: 14 of 44\n",
      "56.520403146743774\n",
      "29414811_13_8724394428539174350.csv: 15 of 44\n",
      "14.20040512084961\n",
      "99070098_0_2074872741302696997.csv: 16 of 44\n",
      "28.81235933303833\n",
      "43237185_1_3636357855502246981.csv: 17 of 44\n",
      "15.427056074142456\n",
      "46671561_0_6122315295162029872.csv: 18 of 44\n",
      "24.16020894050598\n",
      "53989675_0_8697482470743954630.csv: 19 of 44\n",
      "15.165349960327148\n",
      "25404227_0_2240631045609013057.csv: 20 of 44\n",
      "22.728625059127808\n",
      "9834884_0_3871985887467090123.csv: 21 of 44\n",
      "54.91386795043945\n",
      "63450419_0_8012592961815711786.csv: 22 of 44\n",
      "20.578903913497925\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv: 23 of 44\n",
      "11.260174751281738\n",
      "22864497_0_8632623712684511496.csv: 24 of 44\n",
      "45.902360916137695\n",
      "53822652_0_5767892317858575530.csv: 25 of 44\n",
      "74.33613991737366\n",
      "37856682_0_6818907050314633217.csv: 26 of 44\n",
      "52.85862588882446\n",
      "26310680_0_5150772059999313798.csv: 27 of 44\n",
      "42.75602579116821\n",
      "29414811_12_251152470253168163.csv: 28 of 44\n",
      "13.776190996170044\n",
      "69537082_0_7789694313271016902.csv: 29 of 44\n",
      "44.67486906051636\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv: 30 of 44\n",
      "16.001067876815796\n",
      "60319454_0_3938426910282115527.csv: 31 of 44\n",
      "12.165462017059326\n",
      "16767252_0_2409448375013995751.csv: 32 of 44\n",
      "16.646509885787964\n",
      "84548468_0_5955155464119382182.csv: 33 of 44\n",
      "17.53990602493286\n",
      "80588006_0_6965325215443683359.csv: 34 of 44\n",
      "12.634064197540283\n",
      "39650055_5_7135804139753401681.csv: 35 of 44\n",
      "20.456315994262695\n",
      "40534006_0_4617468856744635526.csv: 36 of 44\n",
      "13.869087934494019\n",
      "90196673_0_5458330029110291950.csv: 37 of 44\n",
      "40.99371814727783\n",
      "24036779_0_5608105867560183058.csv: 38 of 44\n",
      "22.532912254333496\n",
      "9567241_0_5666388268510912770.csv: 39 of 44\n",
      "11.270158052444458\n",
      "41480166_0_6681239260286218499.csv: 40 of 44\n",
      "28.8023738861084\n",
      "77694908_0_6083291340991074532.csv: 41 of 44\n",
      "23.57937216758728\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv: 42 of 44\n",
      "13.198241233825684\n",
      "39107734_2_2329160387535788734.csv: 43 of 44\n",
      "12.594287872314453\n",
      "50245608_0_871275842592178099.csv: 44 of 44\n",
      "38.101136922836304\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_context_path,train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pleased-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv: 1 of 9\n",
      "20.344475030899048\n",
      "45073662_0_3179937335063201739.csv: 2 of 9\n",
      "11.884235858917236\n",
      "29414811_2_4773219892816395776.csv: 3 of 9\n",
      "11.353357076644897\n",
      "84575189_0_6365692015941409487.csv: 4 of 9\n",
      "16.01444411277771\n",
      "14380604_4_3329235705746762392.csv: 5 of 9\n",
      "11.758607864379883\n",
      "52299421_0_4473286348258170200.csv: 6 of 9\n",
      "22.64367413520813\n",
      "50270082_0_444360818941411589.csv: 7 of 9\n",
      "24.196305990219116\n",
      "28086084_0_3127660530989916727.csv: 8 of 9\n",
      "32.01168513298035\n",
      "14067031_0_559833072073397908.csv: 9 of 9\n",
      "16.88125991821289\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_context_path, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-grain",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civil-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(candidate_dir, embedding_dir, class_count_dir, property_count_dir, output_path):\n",
    "    file_list = glob.glob(candidate_dir + '/*.csv')\n",
    "    for i, file in enumerate(file_list):\n",
    "        filename = file.split('/')[-1]\n",
    "        print(f\"{filename}: {i+1} of {len(file_list)}\")\n",
    "        embedding_file = f\"{embedding_dir}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "        class_count_file = f\"{class_count_dir}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        !time tl align-page-rank $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein \\\n",
    "            / string-similarity -i --method jaccard:tokenizer=word -c kg_descriptions context -o des_cont_jaccard \\\n",
    "            / normalize-scores -c des_cont_jaccard / smallest-qnode-number \\\n",
    "            / mosaic-features -c kg_labels --num-char --num-tokens \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            / vote-by-classifier  \\\n",
    "            --prob-threshold 0.995 \\\n",
    "            --model $classifier_model_path \\\n",
    "            / score-using-embedding \\\n",
    "            --column-vector-strategy centroid-of-lof \\\n",
    "            --lof-strategy ems-mv \\\n",
    "            -o lof-graph-embedding-score \\\n",
    "            --embedding-file $embedding_file \\\n",
    "            / generate-reciprocal-rank  \\\n",
    "            -c lof-graph-embedding-score \\\n",
    "            -o lof-reciprocal-rank \\\n",
    "            / compute-tf-idf  \\\n",
    "            --feature-file $class_count_file \\\n",
    "            --feature-name class_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_class_count_tf_idf_score \\\n",
    "            / compute-tf-idf \\\n",
    "            --feature-file $property_count_file \\\n",
    "            --feature-name property_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_property_count_tf_idf_score \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "going-ancient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv: 1 of 44\n",
      "Qnodes to lookup: 10717\n",
      "Qnodes from file: 10399\n",
      "Outlier removal generates 86 lof-voted candidates\n",
      "\n",
      "real\t0m49.415s\n",
      "user\t0m57.723s\n",
      "sys\t0m9.136s\n",
      "39173938_0_7916056990138658530.csv: 2 of 44\n",
      "Qnodes to lookup: 9986\n",
      "Qnodes from file: 9718\n",
      "Outlier removal generates 74 lof-voted candidates\n",
      "\n",
      "real\t0m42.454s\n",
      "user\t0m53.124s\n",
      "sys\t0m8.777s\n",
      "10579449_0_1681126353774891032.csv: 3 of 44\n",
      "Qnodes to lookup: 1706\n",
      "Qnodes from file: 1652\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m18.026s\n",
      "user\t0m30.589s\n",
      "sys\t0m8.073s\n",
      "33401079_0_9127583903019856402.csv: 4 of 44\n",
      "Qnodes to lookup: 8276\n",
      "Qnodes from file: 8183\n",
      "Outlier removal generates 52 lof-voted candidates\n",
      "\n",
      "real\t0m44.369s\n",
      "user\t0m52.694s\n",
      "sys\t0m9.077s\n",
      "21362676_0_6854186738074119688.csv: 5 of 44\n",
      "Qnodes to lookup: 10157\n",
      "Qnodes from file: 9877\n",
      "Outlier removal generates 105 lof-voted candidates\n",
      "\n",
      "real\t0m55.598s\n",
      "user\t1m4.863s\n",
      "sys\t0m9.036s\n",
      "38428277_0_1311643810102462607.csv: 6 of 44\n",
      "Qnodes to lookup: 12595\n",
      "Qnodes from file: 12188\n",
      "Outlier removal generates 115 lof-voted candidates\n",
      "\n",
      "real\t0m50.100s\n",
      "user\t1m1.328s\n",
      "sys\t0m8.442s\n",
      "91959037_0_7907661684242014480.csv: 7 of 44\n",
      "Qnodes to lookup: 22297\n",
      "Qnodes from file: 21621\n",
      "Outlier removal generates 22 lof-voted candidates\n",
      "\n",
      "real\t2m18.243s\n",
      "user\t2m32.330s\n",
      "sys\t0m10.583s\n",
      "20135078_0_7570343137119682530.csv: 8 of 44\n",
      "Qnodes to lookup: 11208\n",
      "Qnodes from file: 10916\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "\n",
      "real\t0m43.090s\n",
      "user\t0m53.052s\n",
      "sys\t0m8.468s\n",
      "35188621_0_6058553107571275232.csv: 9 of 44\n",
      "Qnodes to lookup: 11833\n",
      "Qnodes from file: 11460\n",
      "_centroid_of_lof: Missing 1 of 169\n",
      "Outlier removal generates 102 lof-voted candidates\n",
      "\n",
      "real\t0m44.177s\n",
      "user\t0m54.334s\n",
      "sys\t0m9.085s\n",
      "54719588_0_8417197176086756912.csv: 10 of 44\n",
      "Qnodes to lookup: 24211\n",
      "Qnodes from file: 23278\n",
      "Outlier removal generates 44 lof-voted candidates\n",
      "\n",
      "real\t1m11.467s\n",
      "user\t1m23.550s\n",
      "sys\t0m8.808s\n",
      "21245481_0_8730460088443117515.csv: 11 of 44\n",
      "Qnodes to lookup: 11848\n",
      "Qnodes from file: 11696\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "\n",
      "real\t1m3.881s\n",
      "user\t1m14.088s\n",
      "sys\t0m9.219s\n",
      "71840765_0_6664391841933033844.csv: 12 of 44\n",
      "Qnodes to lookup: 1100\n",
      "Qnodes from file: 1094\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "\n",
      "real\t0m24.951s\n",
      "user\t0m31.152s\n",
      "sys\t0m7.577s\n",
      "8468806_0_4382447409703007384.csv: 13 of 44\n",
      "Qnodes to lookup: 6738\n",
      "Qnodes from file: 6671\n",
      "Outlier removal generates 18 lof-voted candidates\n",
      "\n",
      "real\t0m26.312s\n",
      "user\t0m40.337s\n",
      "sys\t0m8.377s\n",
      "88523363_0_8180214313099580515.csv: 14 of 44\n",
      "Qnodes to lookup: 42234\n",
      "Qnodes from file: 41968\n",
      "Outlier removal generates 35 lof-voted candidates\n",
      "\n",
      "real\t1m55.637s\n",
      "user\t2m8.686s\n",
      "sys\t0m11.540s\n",
      "29414811_13_8724394428539174350.csv: 15 of 44\n",
      "Qnodes to lookup: 1140\n",
      "Qnodes from file: 1102\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "\n",
      "real\t0m24.907s\n",
      "user\t0m34.917s\n",
      "sys\t0m7.587s\n",
      "99070098_0_2074872741302696997.csv: 16 of 44\n",
      "Qnodes to lookup: 12078\n",
      "Qnodes from file: 11859\n",
      "_centroid_of_lof: Missing 1 of 28\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "\n",
      "real\t0m43.280s\n",
      "user\t0m58.944s\n",
      "sys\t0m8.646s\n",
      "43237185_1_3636357855502246981.csv: 17 of 44\n",
      "Qnodes to lookup: 2611\n",
      "Qnodes from file: 2573\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "\n",
      "real\t0m32.228s\n",
      "user\t0m37.550s\n",
      "sys\t0m8.363s\n",
      "46671561_0_6122315295162029872.csv: 18 of 44\n",
      "Qnodes to lookup: 15698\n",
      "Qnodes from file: 15130\n",
      "Outlier removal generates 84 lof-voted candidates\n",
      "\n",
      "real\t1m13.721s\n",
      "user\t1m24.412s\n",
      "sys\t0m9.009s\n",
      "53989675_0_8697482470743954630.csv: 19 of 44\n",
      "Qnodes to lookup: 2161\n",
      "Qnodes from file: 2144\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m22.802s\n",
      "user\t0m34.575s\n",
      "sys\t0m8.715s\n",
      "25404227_0_2240631045609013057.csv: 20 of 44\n",
      "Qnodes to lookup: 10026\n",
      "Qnodes from file: 9732\n",
      "_centroid_of_lof: Missing 1 of 166\n",
      "Outlier removal generates 100 lof-voted candidates\n",
      "\n",
      "real\t1m1.526s\n",
      "user\t1m9.957s\n",
      "sys\t0m9.536s\n",
      "9834884_0_3871985887467090123.csv: 21 of 44\n",
      "Qnodes to lookup: 22923\n",
      "Qnodes from file: 22574\n",
      "Outlier removal generates 72 lof-voted candidates\n",
      "\n",
      "real\t2m32.399s\n",
      "user\t2m45.138s\n",
      "sys\t0m10.526s\n",
      "63450419_0_8012592961815711786.csv: 22 of 44\n",
      "Qnodes to lookup: 13769\n",
      "Qnodes from file: 13691\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "\n",
      "real\t0m50.513s\n",
      "user\t0m59.661s\n",
      "sys\t0m8.964s\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv: 23 of 44\n",
      "Qnodes to lookup: 1237\n",
      "Qnodes from file: 1179\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "\n",
      "real\t0m17.816s\n",
      "user\t0m29.863s\n",
      "sys\t0m8.178s\n",
      "22864497_0_8632623712684511496.csv: 24 of 44\n",
      "Qnodes to lookup: 29456\n",
      "Qnodes from file: 28330\n",
      "Outlier removal generates 48 lof-voted candidates\n",
      "\n",
      "real\t1m52.482s\n",
      "user\t2m4.105s\n",
      "sys\t0m10.577s\n",
      "53822652_0_5767892317858575530.csv: 25 of 44\n",
      "Qnodes to lookup: 43140\n",
      "Qnodes from file: 41723\n",
      "_centroid_of_lof: Missing 1 of 131\n",
      "Outlier removal generates 78 lof-voted candidates\n",
      "\n",
      "real\t1m50.459s\n",
      "user\t2m4.569s\n",
      "sys\t0m10.451s\n",
      "37856682_0_6818907050314633217.csv: 26 of 44\n",
      "Qnodes to lookup: 43381\n",
      "Qnodes from file: 42173\n",
      "Outlier removal generates 280 lof-voted candidates\n",
      "\n",
      "real\t1m57.784s\n",
      "user\t2m10.827s\n",
      "sys\t0m9.797s\n",
      "26310680_0_5150772059999313798.csv: 27 of 44\n",
      "Qnodes to lookup: 35193\n",
      "Qnodes from file: 34092\n",
      "Outlier removal generates 207 lof-voted candidates\n",
      "\n",
      "real\t1m49.583s\n",
      "user\t2m1.337s\n",
      "sys\t0m10.274s\n",
      "29414811_12_251152470253168163.csv: 28 of 44\n",
      "Qnodes to lookup: 1857\n",
      "Qnodes from file: 1790\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "\n",
      "real\t0m27.479s\n",
      "user\t0m39.609s\n",
      "sys\t0m8.253s\n",
      "69537082_0_7789694313271016902.csv: 29 of 44\n",
      "Qnodes to lookup: 36385\n",
      "Qnodes from file: 35280\n",
      "Outlier removal generates 229 lof-voted candidates\n",
      "\n",
      "real\t1m51.084s\n",
      "user\t2m1.591s\n",
      "sys\t0m10.082s\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv: 30 of 44\n",
      "Qnodes to lookup: 980\n",
      "Qnodes from file: 976\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "\n",
      "real\t0m22.254s\n",
      "user\t0m30.657s\n",
      "sys\t0m8.374s\n",
      "60319454_0_3938426910282115527.csv: 31 of 44\n",
      "Qnodes to lookup: 5005\n",
      "Qnodes from file: 4886\n",
      "Outlier removal generates 13 lof-voted candidates\n",
      "\n",
      "real\t0m24.262s\n",
      "user\t0m36.344s\n",
      "sys\t0m8.455s\n",
      "16767252_0_2409448375013995751.csv: 32 of 44\n",
      "Qnodes to lookup: 8870\n",
      "Qnodes from file: 8626\n",
      "Outlier removal generates 90 lof-voted candidates\n",
      "\n",
      "real\t0m39.472s\n",
      "user\t0m48.683s\n",
      "sys\t0m9.255s\n",
      "84548468_0_5955155464119382182.csv: 33 of 44\n",
      "Qnodes to lookup: 10574\n",
      "Qnodes from file: 10229\n",
      "Outlier removal generates 80 lof-voted candidates\n",
      "\n",
      "real\t0m42.680s\n",
      "user\t0m51.325s\n",
      "sys\t0m8.160s\n",
      "80588006_0_6965325215443683359.csv: 34 of 44\n",
      "Qnodes to lookup: 1855\n",
      "Qnodes from file: 1809\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m25.689s\n",
      "user\t0m31.498s\n",
      "sys\t0m8.617s\n",
      "39650055_5_7135804139753401681.csv: 35 of 44\n",
      "Qnodes to lookup: 13681\n",
      "Qnodes from file: 13068\n",
      "Outlier removal generates 36 lof-voted candidates\n",
      "\n",
      "real\t0m33.264s\n",
      "user\t0m45.448s\n",
      "sys\t0m9.060s\n",
      "40534006_0_4617468856744635526.csv: 36 of 44\n",
      "Qnodes to lookup: 4092\n",
      "Qnodes from file: 4034\n",
      "Outlier removal generates 12 lof-voted candidates\n",
      "\n",
      "real\t0m23.491s\n",
      "user\t0m34.823s\n",
      "sys\t0m8.228s\n",
      "90196673_0_5458330029110291950.csv: 37 of 44\n",
      "Qnodes to lookup: 22360\n",
      "Qnodes from file: 21926\n",
      "Outlier removal generates 21 lof-voted candidates\n",
      "\n",
      "real\t1m42.650s\n",
      "user\t1m54.834s\n",
      "sys\t0m10.053s\n",
      "24036779_0_5608105867560183058.csv: 38 of 44\n",
      "Qnodes to lookup: 13908\n",
      "Qnodes from file: 13592\n",
      "Outlier removal generates 115 lof-voted candidates\n",
      "\n",
      "real\t0m50.223s\n",
      "user\t1m2.645s\n",
      "sys\t0m8.228s\n",
      "9567241_0_5666388268510912770.csv: 39 of 44\n",
      "Qnodes to lookup: 1922\n",
      "Qnodes from file: 1861\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m24.550s\n",
      "user\t0m31.927s\n",
      "sys\t0m8.276s\n",
      "41480166_0_6681239260286218499.csv: 40 of 44\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "\n",
      "real\t1m26.052s\n",
      "user\t1m36.258s\n",
      "sys\t0m9.521s\n",
      "77694908_0_6083291340991074532.csv: 41 of 44\n",
      "Qnodes to lookup: 10760\n",
      "Qnodes from file: 10387\n",
      "Outlier removal generates 98 lof-voted candidates\n",
      "\n",
      "real\t0m46.861s\n",
      "user\t0m57.874s\n",
      "sys\t0m8.277s\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv: 42 of 44\n",
      "Qnodes to lookup: 1234\n",
      "Qnodes from file: 1218\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m24.444s\n",
      "user\t0m31.643s\n",
      "sys\t0m8.783s\n",
      "39107734_2_2329160387535788734.csv: 43 of 44\n",
      "Qnodes to lookup: 3480\n",
      "Qnodes from file: 3403\n",
      "Outlier removal generates 42 lof-voted candidates\n",
      "\n",
      "real\t0m43.691s\n",
      "user\t0m52.104s\n",
      "sys\t0m8.518s\n",
      "50245608_0_871275842592178099.csv: 44 of 44\n",
      "Qnodes to lookup: 26520\n",
      "Qnodes from file: 25679\n",
      "_centroid_of_lof: Missing 1 of 255\n",
      "Outlier removal generates 152 lof-voted candidates\n",
      "\n",
      "real\t1m35.961s\n",
      "user\t1m47.325s\n",
      "sys\t0m9.491s\n"
     ]
    }
   ],
   "source": [
    "feature_generation(train_candidate_path, train_graph_embedding, train_class_count, train_prop_count, train_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rising-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv: 1 of 9\n",
      "Qnodes to lookup: 10448\n",
      "Qnodes from file: 10120\n",
      "Outlier removal generates 103 lof-voted candidates\n",
      "\n",
      "real\t0m50.393s\n",
      "user\t1m2.170s\n",
      "sys\t0m8.076s\n",
      "45073662_0_3179937335063201739.csv: 2 of 9\n",
      "Qnodes to lookup: 3040\n",
      "Qnodes from file: 3004\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 38, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 22, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "\n",
      "real\t0m28.954s\n",
      "user\t0m34.822s\n",
      "sys\t0m9.684s\n",
      "29414811_2_4773219892816395776.csv: 3 of 9\n",
      "Qnodes to lookup: 2106\n",
      "Qnodes from file: 2025\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "\n",
      "real\t0m31.595s\n",
      "user\t0m38.278s\n",
      "sys\t0m9.084s\n",
      "84575189_0_6365692015941409487.csv: 4 of 9\n",
      "Qnodes to lookup: 8486\n",
      "Qnodes from file: 7897\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "\n",
      "real\t1m0.271s\n",
      "user\t1m8.454s\n",
      "sys\t0m9.253s\n",
      "14380604_4_3329235705746762392.csv: 5 of 9\n",
      "Qnodes to lookup: 2291\n",
      "Qnodes from file: 2226\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "\n",
      "real\t0m26.487s\n",
      "user\t0m33.806s\n",
      "sys\t0m9.413s\n",
      "52299421_0_4473286348258170200.csv: 6 of 9\n",
      "Qnodes to lookup: 15531\n",
      "Qnodes from file: 15218\n",
      "Outlier removal generates 85 lof-voted candidates\n",
      "\n",
      "real\t1m0.657s\n",
      "user\t1m8.742s\n",
      "sys\t0m9.257s\n",
      "50270082_0_444360818941411589.csv: 7 of 9\n",
      "Qnodes to lookup: 16794\n",
      "Qnodes from file: 16683\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "\n",
      "real\t0m47.168s\n",
      "user\t0m57.537s\n",
      "sys\t0m9.349s\n",
      "28086084_0_3127660530989916727.csv: 8 of 9\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Outlier removal generates 23 lof-voted candidates\n",
      "\n",
      "real\t1m34.342s\n",
      "user\t1m47.484s\n",
      "sys\t0m9.317s\n",
      "14067031_0_559833072073397908.csv: 9 of 9\n",
      "Qnodes to lookup: 7690\n",
      "Qnodes from file: 7433\n",
      "Outlier removal generates 58 lof-voted candidates\n",
      "\n",
      "real\t0m43.494s\n",
      "user\t0m50.430s\n",
      "sys\t0m9.584s\n"
     ]
    }
   ],
   "source": [
    "feature_generation(dev_candidate_path, dev_graph_embedding, dev_class_count, dev_prop_count, dev_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-chair",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protected-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    eval_file_names = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(datapath):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = f\"{dirpath}/{fn}\"\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    df_list = []\n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler\n",
    "\n",
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['evaluation_label']\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '/*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(file_name)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        d_sample = pd.read_csv(file)\n",
    "#         grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            num_rows = 64\n",
    "            pos_row = a[sfeatures].drop('evaluation_label',axis=1)\n",
    "            negatives_filtered = cell[1][cell[1]['evaluation_label'] == -1]\n",
    "            sorted_df = negatives_filtered.sort_values('lof-graph-embedding-score',ascending=False)\n",
    "            sorted_df = sorted_df[sfeatures]\n",
    "            if 0 in sorted_df['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if sorted_df.empty:\n",
    "                continue\n",
    "            neg_list = []\n",
    "            if num_rows < len(sorted_df):\n",
    "                sorted_df = sorted_df[sorted_df['evaluation_label'] == -1]\n",
    "                neg_list.append(sorted_df[:2])\n",
    "                retrieval_score_df = sorted_df[2:].sort_values('retrieval_score',ascending=False)\n",
    "                neg_list.append(retrieval_score_df[:2])\n",
    "                pagerank_score_df = retrieval_score_df[2:].sort_values('pagerank', ascending=False)\n",
    "                neg_list.append(pagerank_score_df[:2])\n",
    "                class_count_score_df = pagerank_score_df[2:].sort_values('lof_class_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(class_count_score_df[:2])\n",
    "                prop_count_score_df = class_count_score_df[2:].sort_values('lof_property_count_tf_idf_score', ascending=False)\n",
    "                neg_list.append(prop_count_score_df[:2])\n",
    "                monge_elkan_score_df = prop_count_score_df[2:].sort_values('monge_elkan', ascending=False)\n",
    "                neg_list.append(monge_elkan_score_df[:2])\n",
    "                monge_elkan_alias_score_df = monge_elkan_score_df[2:].sort_values('monge_elkan_aliases', ascending=False)\n",
    "                neg_list.append(monge_elkan_alias_score_df[:2])\n",
    "\n",
    "                jaro_winkler_score_df = monge_elkan_alias_score_df[2:].sort_values('jaro_winkler', ascending=False)\n",
    "                neg_list.append(jaro_winkler_score_df[:2])\n",
    "                top_sample_df = jaro_winkler_score_df.sample(n=50)\n",
    "                neg_list.append(top_sample_df)\n",
    "                top_sample_df = pd.concat(neg_list)\n",
    "                top_sample_df.drop('evaluation_label', inplace=True, axis=1)\n",
    "                top_sample_arr = top_sample_df.to_numpy()\n",
    "\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                neg_features.append(top_sample_arr[i])\n",
    "            random.shuffle(neg_features)\n",
    "            for i in range(len(top_sample_arr)):\n",
    "                pos_row_sample = pos_row.sample(n=1)\n",
    "                ar = pos_row_sample.to_numpy()\n",
    "                for ps_ar in ar:\n",
    "                    pos_features.append(ps_ar)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "    print(len(positive_features_final), len(positive_features_final[37]))\n",
    "    print(len(negative_features_final), len(negative_features_final[37]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indirect-adrian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n",
      "5578 66\n",
      "5578 66\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_feature_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "generate_train_data(gen_training_data_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-chick",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "painted-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-dylan",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dietary-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = features\n",
    "    for file in glob.glob(input_table_path + '/*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        if file_name != '52299421_0_4473286348258170200.csv':\n",
    "            print(file_name)\n",
    "            d_sample = pd.read_csv(file)\n",
    "            grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "            new_df_list = []\n",
    "            pred = []\n",
    "            for cell in grouped_obj:\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                sorted_df = cell[1].sort_values('lof-graph-embedding-score',ascending=False)[:64]\n",
    "                sorted_df_features = sorted_df[normalize_features]\n",
    "                new_df_list.append(sorted_df)\n",
    "                arr = sorted_df_features.to_numpy()\n",
    "                test_inp = []\n",
    "                for a in arr:\n",
    "                    test_inp.append(a)\n",
    "                test_tensor = torch.tensor(test_inp).float()\n",
    "                scores = model.predict(test_tensor)\n",
    "                pred.extend(torch.squeeze(scores).tolist())\n",
    "            test_df = pd.concat(new_df_list)\n",
    "            test_df['siamese_pred'] = pred\n",
    "            test_df.to_csv(f\"{output_table_path}/{file_name}\", index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, 'siamese_pred')\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            best_model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "numerical-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if fn != '52299421_0_4473286348258170200.csv':\n",
    "                if \"csv\" not in fn:\n",
    "                    continue\n",
    "                abs_fn = os.path.join(dirpath, fn)\n",
    "                assert os.path.isfile(abs_fn)\n",
    "                if os.path.getsize(abs_fn) == 0:\n",
    "                    continue\n",
    "                eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'column', 'row']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "        s_data = c_e_data.sort_values(by=['lof-graph-embedding-score'], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_graph_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "        cf_e_data = c_e_data.copy()\n",
    "        cf_e_data['lof-graph-embedding-score'] = cf_e_data['lof-graph-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "    candidate_eval_data['lof-graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "    candidate_eval_data['lof-graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['lof-graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "    res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "    res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "    res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "everyday-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_feature_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "exterior-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "5753it [00:08, 682.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "123it [00:00, 624.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.1252160221338272, epoch top1 0.869309838472834, max top1 0.869309838472834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 651.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "127it [00:00, 633.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.09705191850662231, epoch top1 0.8854625550660793, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 704.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "133it [00:00, 669.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.09554126113653183, epoch top1 0.8355359765051396, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 630.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 559.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss is 0.09343326091766357, epoch top1 0.8370044052863436, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 618.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 551.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.09539840370416641, epoch top1 0.6725403817914831, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 632.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 562.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.09158875048160553, epoch top1 0.8516886930983847, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 642.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "126it [00:00, 626.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.09025537967681885, epoch top1 0.697503671071953, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 648.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "58it [00:00, 573.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.08919765800237656, epoch top1 0.8487518355359766, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 640.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 555.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.08174728602170944, epoch top1 0.856093979441997, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 626.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 552.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.08591412007808685, epoch top1 0.8164464023494861, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 644.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "64it [00:00, 632.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.08845736086368561, epoch top1 0.7562408223201175, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 632.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 569.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.08996590226888657, epoch top1 0.7856093979441997, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 626.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "58it [00:00, 577.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.0938536673784256, epoch top1 0.7914831130690162, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 614.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "120it [00:00, 598.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.09170757979154587, epoch top1 0.7650513950073421, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 612.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 562.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.08713185787200928, epoch top1 0.8002936857562408, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 628.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "118it [00:00, 587.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.08577883988618851, epoch top1 0.8414096916299559, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:09, 620.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "57it [00:00, 562.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.08639060705900192, epoch top1 0.7841409691629956, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 654.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "56it [00:00, 555.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.08659988641738892, epoch top1 0.8076358296622613, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 643.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-18-333dfe1ca43c>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "<ipython-input-18-333dfe1ca43c>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "59it [00:00, 586.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.08667615056037903, epoch top1 0.6328928046989721, max top1 0.8854625550660793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [00:08, 658.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 19, Avg Loss is 0.08539833873510361, epoch top1 0.7723935389133627, max top1 0.8854625550660793\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "best_model_path = train(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-costa",
   "metadata": {},
   "source": [
    "## Dev Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "exceptional-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_prediction(dev_feature_path, dev_predictions_top_k, saved_model, output_column, min_max_scaler_path, k=5):\n",
    "    for file in glob.glob(dev_feature_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        # location where the output generated by the predictions wil be stored.\n",
    "        dev_output = f\"{dev_predictions_top_k}/{filename.strip('.csv')}.xlsx\"\n",
    "        !tl predict-using-model $file -o $output_column \\\n",
    "            --features {\",\".join(features)} \\\n",
    "            --ranking-model $saved_model \\\n",
    "            --normalization-factor $min_max_scaler_path \\\n",
    "            / get-kg-links -c $output_column -k $k --k-rows \\\n",
    "            / add-color -c $output_column -k $k --output $dev_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "little-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "52299421_0_4473286348258170200.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "dev_prediction(dev_feature_path, dev_predictions_top_k, best_model_path, 'siamese_pred', min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-albania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
