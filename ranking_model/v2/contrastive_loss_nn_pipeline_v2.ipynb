{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented'\n",
    "\n",
    "# Input Paths\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/train1-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/dev-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/round4_gt_with_labels\"\n",
    "\n",
    "\n",
    "# OUTPUT PATHS\n",
    "output_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker\"\n",
    "train_output_path = f'{output_path}/train1-output'\n",
    "dev_output_path = f'{output_path}/dev-output'\n",
    "\n",
    "# increase version to create a new folder for an experiment\n",
    "PREVIOUS_VERSION = \"v15\"\n",
    "VERSION = \"v15_reverse_context\"\n",
    "\n",
    "train_candidate_path = f'{train_output_path}/{VERSION}/candidates'\n",
    "train_string_feature_path = f'{train_output_path}/{VERSION}/string_features'\n",
    "train_feature_path = f'{train_output_path}/{VERSION}/features'\n",
    "train_context_feature_path = f'{train_output_path}/{VERSION}/context_features'\n",
    "train_missing_candidates_path = f'{train_output_path}/{VERSION}/train_missing_candidates_path'\n",
    "\n",
    "dev_candidate_path = f'{dev_output_path}/{VERSION}/candidates'\n",
    "dev_feature_path = f'{dev_output_path}/{VERSION}/features'\n",
    "dev_string_feature_path = f'{dev_output_path}/{VERSION}/string_features'\n",
    "dev_context_feature_path = f'{dev_output_path}/{VERSION}/context_features'\n",
    "dev_output_predictions = f'{dev_output_path}/{VERSION}/dev_predictions'\n",
    "dev_predictions_top_k = f'{dev_output_path}/{VERSION}/dev_predictions_top_k'\n",
    "dev_colorized_path = f'{dev_output_path}/{VERSION}/dev_predictions_colorized'\n",
    "dev_metrics_path = f'{dev_output_path}/{VERSION}/dev_predictions_metrics'\n",
    "dev_missing_candidates_path = f'{dev_output_path}/{VERSION}/dev_missing_candidates_path'\n",
    "\n",
    "aux_field = 'graph_embedding_complex,class_count,property_count,context'\n",
    "\n",
    "\n",
    "train_prop_count = f'{train_output_path}/{VERSION}/train_prop_count' \n",
    "train_class_count = f'{train_output_path}/{VERSION}/train_class_count'\n",
    "train_context_path = f'{train_output_path}/{VERSION}/train_context'\n",
    "train_graph_embedding = f'{train_output_path}/{VERSION}/train_graph_embedding'\n",
    "\n",
    "dev_prop_count = f'{dev_output_path}/{VERSION}/dev_prop_count'\n",
    "dev_class_count = f'{dev_output_path}/{VERSION}/dev_class_count'\n",
    "dev_context_path = f'{dev_output_path}/{VERSION}/dev_context'\n",
    "dev_graph_embedding = f'{dev_output_path}/{VERSION}/dev_graph_embedding'\n",
    "\n",
    "temp_dir = f'{output_path}/temp'\n",
    "\n",
    "tl_log_file =f'{temp_dir}/tl_log.txt'\n",
    "\n",
    "pos_output = f'{temp_dir}/training_data/pos_features.pkl'\n",
    "neg_output = f'{temp_dir}/training_data/neg_features.pkl'\n",
    "min_max_scaler_path = f'{temp_dir}/training_data/normalization_factor.pkl'\n",
    "\n",
    "final_score_column = 'siamese_prediction'\n",
    "threshold = final_score_column+\":median\"\n",
    "\n",
    "model_save_path = f'{dev_output_path}/{VERSION}/saved_models'\n",
    "best_model_path = ''\n",
    "\n",
    "copy_candidates_from_previous_version = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sensitive-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$temp_dir\"\n",
    "\n",
    "!mkdir -p \"$train_prop_count\"\n",
    "!mkdir -p \"$dev_prop_count\"\n",
    "!mkdir -p \"$train_class_count\"\n",
    "!mkdir -p \"$dev_class_count\"\n",
    "!mkdir -p \"$train_graph_embedding\"\n",
    "!mkdir -p \"$dev_graph_embedding\"\n",
    "!mkdir -p \"$train_context_path\"\n",
    "!mkdir -p \"$dev_context_path\"\n",
    "\n",
    "!mkdir -p \"$train_candidate_path\"\n",
    "!mkdir -p \"$dev_candidate_path\"\n",
    "\n",
    "!mkdir -p \"$train_feature_path\"\n",
    "!mkdir -p \"$train_string_feature_path\"\n",
    "!mkdir -p \"$train_context_feature_path\"\n",
    "!mkdir -p \"$dev_feature_path\"\n",
    "!mkdir -p \"$dev_string_feature_path\"\n",
    "!mkdir -p \"$dev_context_feature_path\"\n",
    "\n",
    "!mkdir -p \"$temp_dir/training_data\"\n",
    "!mkdir -p \"$dev_output_predictions\"\n",
    "!mkdir -p \"$model_save_path\"\n",
    "!mkdir -p \"$dev_predictions_top_k\"\n",
    "!mkdir -p \"$dev_colorized_path\"\n",
    "!mkdir -p \"$dev_metrics_path\"\n",
    "!mkdir -p \"$dev_missing_candidates_path\"\n",
    "!mkdir -p \"$train_missing_candidates_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "if copy_candidates_from_previous_version:\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/candidates/*csv $dev_output_path/$VERSION/candidates\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_prop_count/* $dev_output_path/$VERSION/dev_prop_count\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_class_count/* $dev_output_path/$VERSION/dev_class_count\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_context/* $dev_output_path/$VERSION/dev_context\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_graph_embedding/* $dev_output_path/$VERSION/dev_graph_embedding\n",
    "    \n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/candidates/*csv $train_output_path/$VERSION/candidates\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_prop_count/* $train_output_path/$VERSION/train_prop_count\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_class_count/* $train_output_path/$VERSION/train_class_count\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_context/* $train_output_path/$VERSION/train_context\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_graph_embedding/* $train_output_path/$VERSION/train_graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e96e759-e94d-40a6-8e77-7412edaf22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"monge_elkan\",\"monge_elkan_aliases\",\"jaro_winkler\",\"levenshtein\",\"singleton\",\"pgr_rts\",\"context_score\",\"smc_class_score\",\"smc_property_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-tradition",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    file_list = glob.glob(path + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        gt_file = f\"{ground_truth_files}/{filename}\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "        !tl --log-file $tl_log_file clean -c label -o label_clean \"$file\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        / get-ngram-matches -c label_clean  \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" \\\n",
    "        / ground-truth-labeler --gt-file \"$gt_file\" > \"$output_file\"\n",
    "        \n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = f\"{class_count_path}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = f\"{prop_count_path}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            elif field == 'context':\n",
    "                context_file = f\"{context_path}/{filename.strip('.csv')}_context.tsv\"\n",
    "                aux_df.to_csv(context_file, sep='\\t', index=False)\n",
    "            else:\n",
    "                graph_embedding_file = f\"{graph_embedding}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not copy_candidates_from_previous_version:\n",
    "    candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_context_path,train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not copy_candidates_from_previous_version:\n",
    "    candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_context_path, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-visitor",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_feature_generation(candidate_dir, output_path):\n",
    "    file_list = glob.glob(candidate_dir + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        !tl --log-file $tl_log_file string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae15e57-2833-4b56-bc3d-dfdcbc5d9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $dev_output_path/$PREVIOUS_VERSION/string_features/* $dev_output_path/$VERSION/string_features   \n",
    "!cp $train_output_path/$PREVIOUS_VERSION/string_features/*csv $train_output_path/$VERSION/string_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_feature_generation(train_candidate_path, train_string_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_feature_generation(dev_candidate_path, dev_string_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2df8835-2a07-4e3d-9de9-d3b73fcaec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pseudo_gt_feature_generation(string_features_path, context_path, output_path):\n",
    "    file_list = glob.glob(string_features_path + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        filename = file.split('/')[-1]\n",
    "        context_file = f\"{context_path}/{filename[:-4]}_context.tsv\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        !tl --log-file $tl_log_file pick-hc-candidates -o ignore_candidate $file \\\n",
    "        -s monge_elkan,monge_elkan_aliases,jaro_winkler,levenshtein \\\n",
    "        / context-match --debug --context-file $context_file --ignore-column-name ignore_candidate -o context_score \\\n",
    "        / kth-percentile -c context_score -o kth_percenter --ignore-column ignore_candidate --k-percentile 0.75  --minimum-cells 10 > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bdf64df-aacb-4936-b3d6-8261b8cf0a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [30:45, 42.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/kth-percentile.py:44: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  kp = KthPercentile(input_file=kwargs['input_file'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [1:26:20, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/kth-percentile.py:44: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  kp = KthPercentile(input_file=kwargs['input_file'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [2:58:28, 36.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/kth-percentile.py:44: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  kp = KthPercentile(input_file=kwargs['input_file'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [3:18:00, 40.00s/it]\n"
     ]
    }
   ],
   "source": [
    "pre_pseudo_gt_feature_generation(train_string_feature_path,train_context_path, train_context_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07385b31-e3c8-48cf-89ac-d9fe81d95b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/train1-output/v15_reverse_context/string_features/2OYAB7YM.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff29ac2-9f64-443a-a161-6daedcdf903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [24:17, 29.14s/it]\n"
     ]
    }
   ],
   "source": [
    "pre_pseudo_gt_feature_generation(dev_string_feature_path,dev_context_path, dev_context_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84be2bd3-f84b-41f6-9526-7f7cb8cb2928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/v15_reverse_context/context_features'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_string_feature_path\n",
    "dev_context_path\n",
    "dev_context_feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac336e4-38d4-46c2-a0d4-24bf5a0fb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_of_features_generation(context_features_path, class_count_dir, property_count_dir, output_path):\n",
    "    file_list = glob.glob(context_features_path + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        filename = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "\n",
    "        class_count_file = f\"{class_count_dir}/{filename[:-4]}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}/{filename[:-4]}_prop_count.tsv\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        !tl  --log-file $tl_log_file pgt-semantic-tf-idf $file \\\n",
    "            -o smc_class_score \\\n",
    "            --pagerank-column pagerank \\\n",
    "            --retrieval-score-column retrieval_score \\\n",
    "            --feature-file \"$class_count_file\" \\\n",
    "            --feature-name class_count \\\n",
    "            --high-confidence-column kth_percenter \\\n",
    "            / pgt-semantic-tf-idf \\\n",
    "            -o smc_property_score \\\n",
    "            --pagerank-column pagerank \\\n",
    "            --retrieval-score-column retrieval_score \\\n",
    "            --feature-file \"$property_count_file\" \\\n",
    "            --feature-name property_count \\\n",
    "            --high-confidence-column kth_percenter \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4384ae2-5ebc-4774-aa28-16cf973076c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [07:37, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n",
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (25,26,27,30,31,32,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [24:05, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n",
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [53:51, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n",
      "/Users/amandeep/Github/table-linker/tl/cli/pgt-semantic-tf-idf.py:46: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  tfidf_unit = SemanticsFeature(kwargs['output_column_name'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [1:01:31, 12.43s/it]\n"
     ]
    }
   ],
   "source": [
    "rest_of_features_generation(train_context_feature_path, train_class_count, train_prop_count, train_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2657676b-d125-4898-96c7-a5def6183212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [11:04, 13.29s/it]\n"
     ]
    }
   ],
   "source": [
    "rest_of_features_generation(dev_context_feature_path, dev_class_count, dev_prop_count, dev_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-carrier",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tested-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    df_list  = []\n",
    "    for fn in glob.glob(f\"{datapath}/*csv\"):\n",
    "        if os.path.getsize(fn) == 0:\n",
    "            continue\n",
    "        fid = fn.split('/')[-1][:-4]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df['context_score'].fillna(0.0, inplace=True)\n",
    "        df_list.append(df)            \n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cebe14-8fd3-4c52-bf00-79f9a96752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['evaluation_label']\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '/*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        d_sample = pd.read_csv(file)\n",
    "        d_sample['context_score'].fillna(0.0, inplace=True)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            pos_rows = cell[1][(cell[1]['evaluation_label'].astype(int) == 1) & (cell[1]['ignore_candidate'].astype(int) == 0)][features].to_numpy()\n",
    "            for i in range(len(pos_rows)):\n",
    "                pos_features.append(pos_rows[i])\n",
    "            neg_rows = cell[1][(cell[1]['evaluation_label'].astype(int) == -1) & (cell[1]['ignore_candidate'].astype(int) == 0)][features].to_numpy()\n",
    "            for i in range(min(50,len(neg_rows))):\n",
    "                neg_features.append(neg_rows[i])\n",
    "            random.shuffle(pos_features)\n",
    "            random.shuffle(neg_features)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "            \n",
    "    print(len(positive_features_final), len(positive_features_final[3]))\n",
    "    print(len(negative_features_final), len(negative_features_final[3]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "younger-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/1529062613.py:3: DtypeWarning: Columns (25,26,27,30,31,32,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  all_data = merge_files(gen_training_data_args)\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/1529062613.py:3: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  all_data = merge_files(gen_training_data_args)\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/1529062613.py:3: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  all_data = merge_files(gen_training_data_args)\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_feature_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3ebe59-63d5-4516-bdc9-334a3e0b48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1743455"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb42916-c088-4de8-ae14-29fe3f78d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>row</th>\n",
       "      <th>label</th>\n",
       "      <th>context</th>\n",
       "      <th>filename</th>\n",
       "      <th>column-id</th>\n",
       "      <th>label_clean</th>\n",
       "      <th>kg_id</th>\n",
       "      <th>kg_labels</th>\n",
       "      <th>kg_aliases</th>\n",
       "      <th>...</th>\n",
       "      <th>reverse_context_property</th>\n",
       "      <th>reverse_context_similarity</th>\n",
       "      <th>reverse_context_property_similarity_q_node</th>\n",
       "      <th>kth_percenter</th>\n",
       "      <th>pgr_rts</th>\n",
       "      <th>smc_class_score</th>\n",
       "      <th>top5_smc_class_score</th>\n",
       "      <th>smc_property_score</th>\n",
       "      <th>top5_smc_property_score</th>\n",
       "      <th>table_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Shalygino</td>\n",
       "      <td>2363|160|12</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Shalygino</td>\n",
       "      <td>Q4519780</td>\n",
       "      <td>Schalyhyne|Shalygino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.445500e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Chernelytsia</td>\n",
       "      <td>1564|305|31.4</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Chernelytsia</td>\n",
       "      <td>Q4513379</td>\n",
       "      <td>Chernelytsia|Tschernelyzja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.545824e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.295434</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Stanytsia Luhanska</td>\n",
       "      <td>14543|40|14.6</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Stanytsia Luhanska</td>\n",
       "      <td>Q4439422</td>\n",
       "      <td>Stanytsia Luhanska|Stanyzja Luhanska|Luganska</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8.201895e-07</td>\n",
       "      <td>0.482599</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.357818</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>6483|129|7.53</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>Q4423206</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.040340e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.339590</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Mykhailo-Kotsiubynske</td>\n",
       "      <td>3028|133|5.5</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Mykhailo-Kotsiubynske</td>\n",
       "      <td>Q4297269</td>\n",
       "      <td>Mychajlo-Kozjubynske|Mykhailo-Kotsiubynske</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.859499e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.310358</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   column  row                  label        context      filename  \\\n",
       "0       0    1              Shalygino    2363|160|12  042AKDN1.csv   \n",
       "1       0    2           Chernelytsia  1564|305|31.4  042AKDN1.csv   \n",
       "2       0    4     Stanytsia Luhanska  14543|40|14.6  042AKDN1.csv   \n",
       "3       0    5                Slatyne  6483|129|7.53  042AKDN1.csv   \n",
       "4       0    8  Mykhailo-Kotsiubynske   3028|133|5.5  042AKDN1.csv   \n",
       "\n",
       "        column-id            label_clean     kg_id  \\\n",
       "0  042AKDN1.csv-0              Shalygino  Q4519780   \n",
       "1  042AKDN1.csv-0           Chernelytsia  Q4513379   \n",
       "2  042AKDN1.csv-0     Stanytsia Luhanska  Q4439422   \n",
       "3  042AKDN1.csv-0                Slatyne  Q4423206   \n",
       "4  042AKDN1.csv-0  Mykhailo-Kotsiubynske  Q4297269   \n",
       "\n",
       "                                       kg_labels kg_aliases  ...  \\\n",
       "0                           Schalyhyne|Shalygino        NaN  ...   \n",
       "1                     Chernelytsia|Tschernelyzja        NaN  ...   \n",
       "2  Stanytsia Luhanska|Stanyzja Luhanska|Luganska        NaN  ...   \n",
       "3                                        Slatyne        NaN  ...   \n",
       "4     Mychajlo-Kozjubynske|Mykhailo-Kotsiubynske        NaN  ...   \n",
       "\n",
       "  reverse_context_property reverse_context_similarity  \\\n",
       "0                      NaN                        NaN   \n",
       "1                      NaN                        NaN   \n",
       "2                      NaN                        NaN   \n",
       "3                      NaN                        NaN   \n",
       "4                      NaN                        NaN   \n",
       "\n",
       "   reverse_context_property_similarity_q_node  kth_percenter       pgr_rts  \\\n",
       "0                                         NaN              1  1.445500e-07   \n",
       "1                                         NaN              1  1.545824e-07   \n",
       "2                                         NaN              1  8.201895e-07   \n",
       "3                                         NaN              1  1.040340e-07   \n",
       "4                                         NaN              1  1.859499e-07   \n",
       "\n",
       "  smc_class_score                               top5_smc_class_score  \\\n",
       "0        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "1        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "2        0.482599  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "3        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "4        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "\n",
       "   smc_property_score                            top5_smc_property_score  \\\n",
       "0            0.345722  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "1            0.295434  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "2            0.357818  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "3            0.339590  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "4            0.310358  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "\n",
       "   table_id  \n",
       "0  042AKDN1  \n",
       "1  042AKDN1  \n",
       "2  042AKDN1  \n",
       "3  042AKDN1  \n",
       "4  042AKDN1  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3bb53c2-55d4-4b7c-9759-99ada234606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13960\n",
      "59259\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>row</th>\n",
       "      <th>label</th>\n",
       "      <th>context</th>\n",
       "      <th>filename</th>\n",
       "      <th>column-id</th>\n",
       "      <th>label_clean</th>\n",
       "      <th>kg_id</th>\n",
       "      <th>kg_labels</th>\n",
       "      <th>kg_aliases</th>\n",
       "      <th>...</th>\n",
       "      <th>reverse_context_property</th>\n",
       "      <th>reverse_context_similarity</th>\n",
       "      <th>reverse_context_property_similarity_q_node</th>\n",
       "      <th>kth_percenter</th>\n",
       "      <th>pgr_rts</th>\n",
       "      <th>smc_class_score</th>\n",
       "      <th>top5_smc_class_score</th>\n",
       "      <th>smc_property_score</th>\n",
       "      <th>top5_smc_property_score</th>\n",
       "      <th>table_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Shalygino</td>\n",
       "      <td>2363|160|12</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Shalygino</td>\n",
       "      <td>Q4519780</td>\n",
       "      <td>Schalyhyne|Shalygino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.445500e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Chernelytsia</td>\n",
       "      <td>1564|305|31.4</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Chernelytsia</td>\n",
       "      <td>Q4513379</td>\n",
       "      <td>Chernelytsia|Tschernelyzja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.545824e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.295434</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Stanytsia Luhanska</td>\n",
       "      <td>14543|40|14.6</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Stanytsia Luhanska</td>\n",
       "      <td>Q4439422</td>\n",
       "      <td>Stanytsia Luhanska|Stanyzja Luhanska|Luganska</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8.201895e-07</td>\n",
       "      <td>0.482599</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.357818</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>6483|129|7.53</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>Q4423206</td>\n",
       "      <td>Slatyne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.040340e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.339590</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Mykhailo-Kotsiubynske</td>\n",
       "      <td>3028|133|5.5</td>\n",
       "      <td>042AKDN1.csv</td>\n",
       "      <td>042AKDN1.csv-0</td>\n",
       "      <td>Mykhailo-Kotsiubynske</td>\n",
       "      <td>Q4297269</td>\n",
       "      <td>Mychajlo-Kozjubynske|Mykhailo-Kotsiubynske</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.859499e-07</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...</td>\n",
       "      <td>0.310358</td>\n",
       "      <td>P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...</td>\n",
       "      <td>042AKDN1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   column  row                  label        context      filename  \\\n",
       "0       0    1              Shalygino    2363|160|12  042AKDN1.csv   \n",
       "1       0    2           Chernelytsia  1564|305|31.4  042AKDN1.csv   \n",
       "2       0    4     Stanytsia Luhanska  14543|40|14.6  042AKDN1.csv   \n",
       "3       0    5                Slatyne  6483|129|7.53  042AKDN1.csv   \n",
       "4       0    8  Mykhailo-Kotsiubynske   3028|133|5.5  042AKDN1.csv   \n",
       "\n",
       "        column-id            label_clean     kg_id  \\\n",
       "0  042AKDN1.csv-0              Shalygino  Q4519780   \n",
       "1  042AKDN1.csv-0           Chernelytsia  Q4513379   \n",
       "2  042AKDN1.csv-0     Stanytsia Luhanska  Q4439422   \n",
       "3  042AKDN1.csv-0                Slatyne  Q4423206   \n",
       "4  042AKDN1.csv-0  Mykhailo-Kotsiubynske  Q4297269   \n",
       "\n",
       "                                       kg_labels kg_aliases  ...  \\\n",
       "0                           Schalyhyne|Shalygino        NaN  ...   \n",
       "1                     Chernelytsia|Tschernelyzja        NaN  ...   \n",
       "2  Stanytsia Luhanska|Stanyzja Luhanska|Luganska        NaN  ...   \n",
       "3                                        Slatyne        NaN  ...   \n",
       "4     Mychajlo-Kozjubynske|Mykhailo-Kotsiubynske        NaN  ...   \n",
       "\n",
       "  reverse_context_property reverse_context_similarity  \\\n",
       "0                      NaN                        NaN   \n",
       "1                      NaN                        NaN   \n",
       "2                      NaN                        NaN   \n",
       "3                      NaN                        NaN   \n",
       "4                      NaN                        NaN   \n",
       "\n",
       "   reverse_context_property_similarity_q_node  kth_percenter       pgr_rts  \\\n",
       "0                                         NaN              1  1.445500e-07   \n",
       "1                                         NaN              1  1.545824e-07   \n",
       "2                                         NaN              1  8.201895e-07   \n",
       "3                                         NaN              1  1.040340e-07   \n",
       "4                                         NaN              1  1.859499e-07   \n",
       "\n",
       "  smc_class_score                               top5_smc_class_score  \\\n",
       "0        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "1        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "2        0.482599  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "3        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "4        0.480665  Q7216840:0.102|Q2989457:0.090|Q203323:0.066|Q1...   \n",
       "\n",
       "   smc_property_score                            top5_smc_property_score  \\\n",
       "0            0.345722  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "1            0.295434  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "2            0.357818  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "3            0.339590  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "4            0.310358  P1077:0.092|P1376:0.068|P2046:0.060|P1082:0.05...   \n",
       "\n",
       "   table_id  \n",
       "0  042AKDN1  \n",
       "1  042AKDN1  \n",
       "2  042AKDN1  \n",
       "3  042AKDN1  \n",
       "4  042AKDN1  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = all_data[(all_data['evaluation_label'] == 1) & (all_data['ignore_candidate'] == 0)]\n",
    "neg = all_data[(all_data['evaluation_label'] == -1) & (all_data['ignore_candidate'] == 0)]\n",
    "print(len(pos))\n",
    "print(len(neg))\n",
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26866d65-3a57-49d9-808d-024d9c6427b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3563378694.py:1: DtypeWarning: Columns (25,26,27,30,31,32,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  generate_train_data(gen_training_data_args)\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3563378694.py:1: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  generate_train_data(gen_training_data_args)\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3563378694.py:1: DtypeWarning: Columns (25,26,27,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  generate_train_data(gen_training_data_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12744 0\n",
      "12744 0\n"
     ]
    }
   ],
   "source": [
    "generate_train_data(gen_training_data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aaa0d-754f-481f-8913-dde5b9ea0dd0",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collectible-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-lying",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "widespread-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = features\n",
    "    for file in glob.glob(input_table_path + '/*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "                \n",
    "        d_sample = pd.read_csv(file)\n",
    "        d_sample['context_score'].fillna(0.0, inplace=True)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        new_df_list = []\n",
    "        pred = []\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            sorted_df = cell[1].sort_values('context_score', ascending=False)\n",
    "            sorted_df_features = sorted_df[normalize_features]\n",
    "            new_df_list.append(sorted_df)\n",
    "            arr = sorted_df_features.to_numpy()\n",
    "            test_inp = []\n",
    "            for a in arr:\n",
    "                test_inp.append(a)\n",
    "            test_tensor = torch.tensor(test_inp).float()\n",
    "            scores = model.predict(test_tensor)\n",
    "            scores_list = torch.squeeze(scores).tolist()\n",
    "            if not type(scores_list) is list:\n",
    "                pred.append(scores_list)\n",
    "            else:\n",
    "                pred.extend(scores_list)\n",
    "        test_df = pd.concat(new_df_list)\n",
    "        test_df[final_score_column] = pred\n",
    "        test_df.to_csv(f\"{output_table_path}/{file_name}\", index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, final_score_column)\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            best_model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infectious-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = os.path.join(dirpath, fn)\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "                    \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impressed-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_feature_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chronic-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 292.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.8735578060150146, epoch top1 0.6990701606086221, max top1 0.6990701606086221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 293.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.333856463432312, epoch top1 0.761200338123415, max top1 0.761200338123415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 434.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.2301730513572693, epoch top1 0.760777683854607, max top1 0.761200338123415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 316.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss is 0.2006962150335312, epoch top1 0.7637362637362637, max top1 0.7637362637362637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 164.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.18470695614814758, epoch top1 0.7633136094674556, max top1 0.7637362637362637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 159.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.17467385530471802, epoch top1 0.7637362637362637, max top1 0.7637362637362637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 164.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.16805782914161682, epoch top1 0.76458157227388, max top1 0.76458157227388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 149.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.16309846937656403, epoch top1 0.7696534234995773, max top1 0.7696534234995773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 162.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.158919557929039, epoch top1 0.7700760777683855, max top1 0.7700760777683855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 133.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.15551936626434326, epoch top1 0.775993237531699, max top1 0.775993237531699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 134.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.15285538136959076, epoch top1 0.7823330515638208, max top1 0.7823330515638208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 185.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.15067775547504425, epoch top1 0.7844463229078613, max top1 0.7844463229078613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 154.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.1489744484424591, epoch top1 0.7865595942519019, max top1 0.7865595942519019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:01, 169.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.14766743779182434, epoch top1 0.7899408284023669, max top1 0.7899408284023669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 228.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.14663995802402496, epoch top1 0.7895181741335587, max top1 0.7899408284023669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 342.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.14571425318717957, epoch top1 0.7869822485207101, max top1 0.7899408284023669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 294.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.1449698656797409, epoch top1 0.7882502113271344, max top1 0.7899408284023669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 341.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.14420084655284882, epoch top1 0.7924767540152156, max top1 0.7924767540152156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 283.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.14357593655586243, epoch top1 0.7958579881656804, max top1 0.7958579881656804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_feat = torch.tensor(batch[0].float())\n",
      "/var/folders/qv/cxzpwz3j29x7n79vwpw253v80000gn/T/ipykernel_46590/3218613351.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_feat = torch.tensor(batch[1].float())\n",
      "219it [00:00, 358.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss is 0.14290618896484375, epoch top1 0.794167371090448, max top1 0.7958579881656804\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "best_model_path = train(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "valuable-grain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/v15_reverse_context/saved_models/epoch_18_loss_0.14357593655586243_top1_0.7958579881656804.pth'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-holocaust",
   "metadata": {},
   "source": [
    "## Dev Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "surprising-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_prediction(dev_feature_path, dev_predictions_top_k, saved_model, output_column, min_max_scaler_path, k=5):\n",
    "    for file in glob.glob(dev_feature_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        feature_str =  \",\".join(features)\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        # location where the output generated by the predictions wil be stored.\n",
    "        dev_output = f\"{dev_predictions_top_k}/{filename}\"\n",
    "        !tl --log-file $tl_log_file predict-using-model $file -o $output_column \\\n",
    "            --features $feature_str \\\n",
    "            --ranking-model $saved_model \\\n",
    "            --normalization-factor $min_max_scaler_path \\\n",
    "            / create-pseudo-gt \\\n",
    "            --column-thresholds $threshold \\\n",
    "            --filter smc_class_score:0 \\\n",
    "            / get-kg-links -c pseudo_gt -k $k --k-rows \\\n",
    "            > $dev_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "funny-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_color(dev_predictions_top_k, dev_colorized_path, score_column, k=5):\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_color_file = f\"{dev_colorized_path}/{filename[:-4]}.xlsx\"\n",
    "        !tl add-color $file -c \"$score_column,evaluation_label\" -k $k --output $dev_color_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "guided-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dev_predictions_top_k, dev_predictions_metrics, score_column, k=5):\n",
    "    df_list = []\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_metrics_file = f\"{dev_predictions_metrics}/{filename}\"\n",
    "        !tl --log-file $tl_log_file metrics $file -k $k -c $score_column --tag $filename> $dev_metrics_file\n",
    "        df_list.append(pd.read_csv(dev_metrics_file))\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1686c3e4-a7c0-49aa-9a7e-f9d80a2f6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_custom_metrics(dev_predictions_top_k):\n",
    "    df_list = []\n",
    "    for file in glob.glob(dev_predictions_top_k+\"/*.csv\"):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        df = pd.read_csv(file)\n",
    "        col_df = []\n",
    "        for col, coldf in df.groupby(by=[\"column\"]):\n",
    "            rows = 0\n",
    "            pgt_rows = 0\n",
    "            pgt_recall = 0\n",
    "            unignored_rows = 0\n",
    "            unignored_candidates = 0\n",
    "            unignored_correct = 0\n",
    "            ignored_correct = 0\n",
    "            kth_perc_rows = 0\n",
    "            kth_perc_correct = 0\n",
    "            kth_perc_candidates = 0\n",
    "            for row, rowdf in coldf.groupby(by=[\"row\"]):\n",
    "                rows += 1\n",
    "                p_count = rowdf[(rowdf[\"pseudo_gt\"] == 1)].shape[0]\n",
    "                if p_count > 0:\n",
    "                    pgt_rows += 1\n",
    "                p_recall = rowdf[((rowdf[\"pseudo_gt\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                pgt_recall += p_recall\n",
    "                ignore_0_count = rowdf[rowdf[\"ignore_candidate\"] == 0].shape[0]\n",
    "                if ignore_0_count > 0:\n",
    "                    unignored_rows += 1\n",
    "                unignored_candidates += ignore_0_count\n",
    "                unignored_correct += rowdf[((rowdf[\"ignore_candidate\"] == 0) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                ignored_correct += rowdf[((rowdf[\"ignore_candidate\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                kth_perc_1_count = rowdf[rowdf[\"kth_percenter\"] == 1].shape[0]\n",
    "                if kth_perc_1_count > 0:\n",
    "                    kth_perc_rows += 1\n",
    "                kth_perc_candidates += kth_perc_1_count\n",
    "                kth_perc_correct += rowdf[((rowdf[\"kth_percenter\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "            col_df.append(pd.DataFrame([{\n",
    "                \"filename\":filename,\n",
    "                \"column\": col,\n",
    "                \"rows\": rows,\n",
    "                \"pgt_rows\": pgt_rows,\n",
    "                \"pgt_recall\": pgt_recall,\n",
    "                \"pgt_accuracy\": pgt_recall/pgt_rows if pgt_rows!=0 else 0,\n",
    "                \"unignored_rows\": unignored_rows,\n",
    "                \"unignored_candidates\": unignored_candidates,\n",
    "                \"unignored_correct\": unignored_correct,\n",
    "                \"ignored_correct\": ignored_correct,\n",
    "                \"ignore_candidate_accuracy\": unignored_correct/unignored_rows if unignored_rows != 0 else 0,\n",
    "                \"kth_percenter_rows\": kth_perc_rows,\n",
    "                \"kth_percenter_candidates\": kth_perc_candidates,\n",
    "                \"kth_percenter_correct\": kth_perc_correct,\n",
    "                \"kth_percenter_accuracy\": kth_perc_correct/kth_perc_rows if kth_perc_rows != 0 else 0\n",
    "            }]))\n",
    "        df_list.append(pd.concat(col_df))\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fad2408-0dbd-4142-b9ec-4b701776c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/v15_reverse_context/features /Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/v15_reverse_context/dev_predictions_top_k /Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/v15_reverse_context/saved_models/epoch_18_loss_0.14357593655586243_top1_0.7958579881656804.pth siamese_prediction /Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/temp/training_data/normalization_factor.pkl\n"
     ]
    }
   ],
   "source": [
    "print(dev_feature_path, dev_predictions_top_k, best_model_path, final_score_column, min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unavailable-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4PKEEJU4.csv\n",
      "4DC5O5I4.csv\n",
      "58E7A5WL.csv\n",
      "0FQOOJPU.csv\n",
      "5IXA0RAI.csv\n",
      "0LF7RI6N.csv\n",
      "2PKG4E2V.csv\n",
      "50NWQJ1T.csv\n",
      "44NDHWR1.csv\n",
      "39W7XXTI.csv\n",
      "3WXFYEAX.csv\n",
      "28D7RBJT.csv\n",
      "1GUVMENF.csv\n",
      "0QWF60VG.csv\n",
      "3OCW1LDZ.csv\n",
      "4V4O0CTS.csv\n",
      "29D1VZHF.csv\n",
      "4FG1UN8O.csv\n",
      "0LZ0M8W4.csv\n",
      "3B54GZSX.csv\n",
      "1ZFRQBQS.csv\n",
      "0WBTX8LY.csv\n",
      "1NS33P8C.csv\n",
      "2TGNKH1P.csv\n",
      "53OUTCE4.csv\n",
      "5TJI4XTK.csv\n",
      "4DPRZWVL.csv\n",
      "4SOL8H0M.csv\n",
      "080HU8A5.csv\n",
      "3OAZEVOY.csv\n",
      "13BLTPJD.csv\n",
      "3M5QXPWN.csv\n",
      "0G9YPQC0.csv\n",
      "1YPLVLS9.csv\n",
      "0CETTKTA.csv\n",
      "1GOKLC0K.csv\n",
      "2QFYH2N9.csv\n",
      "00ECUL14.csv\n",
      "2E6QBLCA.csv\n",
      "1T91CHXV.csv\n",
      "59W76Q0Y.csv\n",
      "5PKTGQ6Q.csv\n",
      "1XIWQBSF.csv\n",
      "2YCSL7OH.csv\n",
      "2FSRG0OI.csv\n",
      "0P8H49LQ.csv\n",
      "2FXR6BX7.csv\n",
      "1PTL0CX1.csv\n",
      "0TQXSY28.csv\n",
      "0JSF530F.csv\n"
     ]
    }
   ],
   "source": [
    "dev_prediction(dev_feature_path, dev_predictions_top_k, best_model_path, final_score_column, min_max_scaler_path, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "advanced-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4PKEEJU4.csv\n",
      "4DC5O5I4.csv\n",
      "58E7A5WL.csv\n",
      "0FQOOJPU.csv\n",
      "5IXA0RAI.csv\n",
      "0LF7RI6N.csv\n",
      "2PKG4E2V.csv\n",
      "50NWQJ1T.csv\n",
      "44NDHWR1.csv\n",
      "39W7XXTI.csv\n",
      "3WXFYEAX.csv\n",
      "28D7RBJT.csv\n",
      "1GUVMENF.csv\n",
      "0QWF60VG.csv\n",
      "3OCW1LDZ.csv\n",
      "4V4O0CTS.csv\n",
      "29D1VZHF.csv\n",
      "4FG1UN8O.csv\n",
      "0LZ0M8W4.csv\n",
      "3B54GZSX.csv\n",
      "1ZFRQBQS.csv\n",
      "0WBTX8LY.csv\n",
      "1NS33P8C.csv\n",
      "2TGNKH1P.csv\n",
      "53OUTCE4.csv\n",
      "5TJI4XTK.csv\n",
      "4DPRZWVL.csv\n",
      "4SOL8H0M.csv\n",
      "080HU8A5.csv\n",
      "3OAZEVOY.csv\n",
      "13BLTPJD.csv\n",
      "3M5QXPWN.csv\n",
      "0G9YPQC0.csv\n",
      "1YPLVLS9.csv\n",
      "0CETTKTA.csv\n",
      "1GOKLC0K.csv\n",
      "2QFYH2N9.csv\n",
      "00ECUL14.csv\n",
      "2E6QBLCA.csv\n",
      "1T91CHXV.csv\n",
      "59W76Q0Y.csv\n",
      "5PKTGQ6Q.csv\n",
      "1XIWQBSF.csv\n",
      "2YCSL7OH.csv\n",
      "2FSRG0OI.csv\n",
      "0P8H49LQ.csv\n",
      "2FXR6BX7.csv\n",
      "1PTL0CX1.csv\n",
      "0TQXSY28.csv\n",
      "0JSF530F.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_df = compute_custom_metrics(dev_predictions_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b6ee6-8268-4cb4-b247-83ef923c461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = compute_metrics(dev_predictions_top_k, dev_metrics_path, 'pseudo_gt', k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "thermal-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>column</th>\n",
       "      <th>rows</th>\n",
       "      <th>pgt_rows</th>\n",
       "      <th>pgt_recall</th>\n",
       "      <th>pgt_accuracy</th>\n",
       "      <th>unignored_rows</th>\n",
       "      <th>unignored_candidates</th>\n",
       "      <th>unignored_correct</th>\n",
       "      <th>ignored_correct</th>\n",
       "      <th>ignore_candidate_accuracy</th>\n",
       "      <th>kth_percenter_rows</th>\n",
       "      <th>kth_percenter_candidates</th>\n",
       "      <th>kth_percenter_correct</th>\n",
       "      <th>kth_percenter_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4PKEEJU4.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4PKEEJU4.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4PKEEJU4.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4DC5O5I4.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58E7A5WL.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9</td>\n",
       "      <td>22</td>\n",
       "      <td>110</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2FXR6BX7.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1PTL0CX1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1PTL0CX1.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0TQXSY28.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0JSF530F.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  column  rows  pgt_rows  pgt_recall  pgt_accuracy  \\\n",
       "0   4PKEEJU4.csv       0    20        10          10      1.000000   \n",
       "0   4PKEEJU4.csv       1    20         6           6      1.000000   \n",
       "0   4PKEEJU4.csv       2    20         3           0      0.000000   \n",
       "0   4DC5O5I4.csv       0    20        10          10      1.000000   \n",
       "0   58E7A5WL.csv       0    22        14           5      0.357143   \n",
       "..           ...     ...   ...       ...         ...           ...   \n",
       "0   2FXR6BX7.csv       4    20         9           9      1.000000   \n",
       "0   1PTL0CX1.csv       0    20        10          10      1.000000   \n",
       "0   1PTL0CX1.csv       2    20         8           8      1.000000   \n",
       "0   0TQXSY28.csv       0    20        10          10      1.000000   \n",
       "0   0JSF530F.csv       0    20        10          10      1.000000   \n",
       "\n",
       "    unignored_rows  unignored_candidates  unignored_correct  ignored_correct  \\\n",
       "0               10                    10                 10               10   \n",
       "0                7                    18                  7               13   \n",
       "0                5                    21                  5               15   \n",
       "0               10                    10                 10               10   \n",
       "0               10                    15                  9               13   \n",
       "..             ...                   ...                ...              ...   \n",
       "0               10                    10                 10               10   \n",
       "0               10                    10                 10               10   \n",
       "0               10                    10                 10               10   \n",
       "0               10                    10                 10               10   \n",
       "0               10                    10                 10               10   \n",
       "\n",
       "    ignore_candidate_accuracy  kth_percenter_rows  kth_percenter_candidates  \\\n",
       "0                         1.0                  10                        10   \n",
       "0                         1.0                   7                        11   \n",
       "0                         1.0                   5                        21   \n",
       "0                         1.0                  10                        10   \n",
       "0                         0.9                  22                       110   \n",
       "..                        ...                 ...                       ...   \n",
       "0                         1.0                  10                        10   \n",
       "0                         1.0                  10                        10   \n",
       "0                         1.0                  10                        10   \n",
       "0                         1.0                  10                        10   \n",
       "0                         1.0                  10                        10   \n",
       "\n",
       "    kth_percenter_correct  kth_percenter_accuracy  \n",
       "0                      10                1.000000  \n",
       "0                       6                0.857143  \n",
       "0                       5                1.000000  \n",
       "0                      10                1.000000  \n",
       "0                      22                1.000000  \n",
       "..                    ...                     ...  \n",
       "0                      10                1.000000  \n",
       "0                      10                1.000000  \n",
       "0                       9                0.900000  \n",
       "0                      10                1.000000  \n",
       "0                      10                1.000000  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{dev_metrics_path}/metrics_200_og.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = compute_custom_metrics(dev_predictions_top_k, dev_metrics_path, final_score_column, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499fb64-536a-45e8-b1b3-e5743197c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = compute_metrics(dev_predictions_top_k, dev_metrics_path, 'pseudo_gt', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "covered-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{dev_metrics_path}/metrics_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "collective-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4PKEEJU4.csv\n",
      "add-color Time: 0.9132840633392334s\n",
      "4DC5O5I4.csv\n",
      "add-color Time: 0.17170000076293945s\n",
      "58E7A5WL.csv\n",
      "add-color Time: 0.23033690452575684s\n",
      "0FQOOJPU.csv\n",
      "add-color Time: 0.23967313766479492s\n",
      "5IXA0RAI.csv\n",
      "add-color Time: 0.15819907188415527s\n",
      "0LF7RI6N.csv\n",
      "add-color Time: 0.22588419914245605s\n",
      "2PKG4E2V.csv\n",
      "add-color Time: 0.974827766418457s\n",
      "50NWQJ1T.csv\n",
      "add-color Time: 0.30091190338134766s\n",
      "44NDHWR1.csv\n",
      "add-color Time: 0.2852017879486084s\n",
      "39W7XXTI.csv\n",
      "add-color Time: 0.26947689056396484s\n",
      "3WXFYEAX.csv\n",
      "add-color Time: 0.20303010940551758s\n",
      "28D7RBJT.csv\n",
      "add-color Time: 0.23721718788146973s\n",
      "1GUVMENF.csv\n",
      "add-color Time: 0.6877517700195312s\n",
      "0QWF60VG.csv\n",
      "add-color Time: 0.2218618392944336s\n",
      "3OCW1LDZ.csv\n",
      "add-color Time: 0.8624629974365234s\n",
      "4V4O0CTS.csv\n",
      "add-color Time: 0.8215079307556152s\n",
      "29D1VZHF.csv\n",
      "add-color Time: 0.31464695930480957s\n",
      "4FG1UN8O.csv\n",
      "add-color Time: 0.4611189365386963s\n",
      "0LZ0M8W4.csv\n",
      "add-color Time: 0.2652740478515625s\n",
      "3B54GZSX.csv\n",
      "add-color Time: 0.8788948059082031s\n",
      "1ZFRQBQS.csv\n",
      "add-color Time: 0.9893248081207275s\n",
      "0WBTX8LY.csv\n",
      "add-color Time: 0.27115416526794434s\n",
      "1NS33P8C.csv\n",
      "add-color Time: 0.16216278076171875s\n",
      "2TGNKH1P.csv\n",
      "add-color Time: 0.24827909469604492s\n",
      "53OUTCE4.csv\n",
      "add-color Time: 0.2075638771057129s\n",
      "5TJI4XTK.csv\n",
      "add-color Time: 0.8463480472564697s\n",
      "4DPRZWVL.csv\n",
      "add-color Time: 0.12128305435180664s\n",
      "4SOL8H0M.csv\n",
      "add-color Time: 0.3261439800262451s\n",
      "080HU8A5.csv\n",
      "add-color Time: 0.127028226852417s\n",
      "3OAZEVOY.csv\n",
      "add-color Time: 0.30057406425476074s\n",
      "13BLTPJD.csv\n",
      "add-color Time: 0.8336269855499268s\n",
      "3M5QXPWN.csv\n",
      "add-color Time: 0.40714025497436523s\n",
      "0G9YPQC0.csv\n",
      "add-color Time: 0.8978390693664551s\n",
      "1YPLVLS9.csv\n",
      "add-color Time: 0.26162195205688477s\n",
      "0CETTKTA.csv\n",
      "add-color Time: 0.20987582206726074s\n",
      "1GOKLC0K.csv\n",
      "add-color Time: 1.0142698287963867s\n",
      "2QFYH2N9.csv\n",
      "add-color Time: 0.23941612243652344s\n",
      "00ECUL14.csv\n",
      "add-color Time: 0.2211771011352539s\n",
      "2E6QBLCA.csv\n",
      "add-color Time: 0.2033061981201172s\n",
      "1T91CHXV.csv\n",
      "add-color Time: 0.25600194931030273s\n",
      "59W76Q0Y.csv\n",
      "add-color Time: 0.17255711555480957s\n",
      "5PKTGQ6Q.csv\n",
      "add-color Time: 0.1221928596496582s\n",
      "1XIWQBSF.csv\n",
      "add-color Time: 0.21798491477966309s\n",
      "2YCSL7OH.csv\n",
      "add-color Time: 0.276580810546875s\n",
      "2FSRG0OI.csv\n",
      "add-color Time: 0.1146848201751709s\n",
      "0P8H49LQ.csv\n",
      "add-color Time: 0.2389669418334961s\n",
      "2FXR6BX7.csv\n",
      "add-color Time: 0.28305578231811523s\n",
      "1PTL0CX1.csv\n",
      "add-color Time: 0.20452475547790527s\n",
      "0TQXSY28.csv\n",
      "add-color Time: 0.16903185844421387s\n",
      "0JSF530F.csv\n",
      "add-color Time: 0.1530170440673828s\n"
     ]
    }
   ],
   "source": [
    "add_color(dev_predictions_top_k, dev_colorized_path, final_score_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_correct_candidates(candidates_path, missing_correct_candidates_path):\n",
    "     for file in tqdm(glob.glob(candidates_path + '/*.csv')):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        missing_file = f\"{missing_correct_candidates_path}/{filename}\"\n",
    "        !tl check-candidates \"$file\" > \"$missing_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_correct_candidates(dev_candidate_path, dev_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(files_path):\n",
    "    df_list = []\n",
    "    for file in glob.glob(files_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        df = pd.read_csv(file)\n",
    "        df['filename'] = filename\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = concat_files(dev_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.to_csv(f\"{dev_missing_candidates_path}/missing_concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_correct_candidates(train_candidates_path, train_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df = concat_files(train_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df.to_csv(f\"{train_missing_candidates_path}/missing_concatenated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
