{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented-08'\n",
    "\n",
    "# Input Paths\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/train1-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/dev-canonical\"\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/round4_gt_with_labels\"\n",
    "\n",
    "\n",
    "# OUTPUT PATHS\n",
    "output_path = \"/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker\"\n",
    "train_output_path = f'{output_path}/train1-output/tbl-pipeline-nn'\n",
    "dev_output_path = f'{output_path}/dev-output/tbl-pipeline-nn'\n",
    "\n",
    "# increase version to create a new folder for an experiment\n",
    "PREVIOUS_VERSION = \"v25\"\n",
    "VERSION = \"v26\"\n",
    "\n",
    "train_candidate_path = f'{train_output_path}/{VERSION}/candidates'\n",
    "train_trigram_candidate_path = f'{train_output_path}/{VERSION}/trigram_candidates'\n",
    "train_features1_path = f'{train_output_path}/{VERSION}/features1'\n",
    "train_trigram_features1_path = f'{train_output_path}/{VERSION}/trigram_features1'\n",
    "train_features1_joined_path = f'{train_output_path}/{VERSION}/features1_joined'\n",
    "train_features2_path = f'{train_output_path}/{VERSION}/features2'\n",
    "train_features3_path = f'{train_output_path}/{VERSION}/features3'\n",
    "train_feature_path = f'{train_output_path}/{VERSION}/features'\n",
    "train_missing_candidates_path = f'{train_output_path}/{VERSION}/train_missing_candidates_path'\n",
    "\n",
    "dev_candidate_path = f'{dev_output_path}/{VERSION}/candidates'\n",
    "dev_trigram_candidate_path = f'{dev_output_path}/{VERSION}/trigram_candidates'\n",
    "dev_feature_path = f'{dev_output_path}/{VERSION}/features'\n",
    "dev_features1_path = f'{dev_output_path}/{VERSION}/features1'\n",
    "dev_trigram_features1_path = f'{dev_output_path}/{VERSION}/trigram_features1'\n",
    "dev_features1_joined_path = f'{dev_output_path}/{VERSION}/features1_joined'\n",
    "dev_features2_path = f'{dev_output_path}/{VERSION}/features2'\n",
    "dev_features3_path = f'{dev_output_path}/{VERSION}/features3'\n",
    "dev_output_predictions = f'{dev_output_path}/{VERSION}/dev_predictions'\n",
    "dev_predictions_top_k = f'{dev_output_path}/{VERSION}/dev_predictions_top_k'\n",
    "dev_colorized_path = f'{dev_output_path}/{VERSION}/dev_predictions_colorized'\n",
    "dev_metrics_path = f'{dev_output_path}/{VERSION}/dev_predictions_metrics'\n",
    "dev_missing_candidates_path = f'{dev_output_path}/{VERSION}/dev_missing_candidates_path'\n",
    "\n",
    "aux_field = 'graph_embedding_complex,class_count,property_count,context'\n",
    "\n",
    "\n",
    "train_prop_count = f'{train_output_path}/{VERSION}/train_prop_count' \n",
    "train_trigram_prop_count = f'{train_output_path}/{VERSION}/train_trigram_prop_count'\n",
    "train_joined_prop_count = f'{train_output_path}/{VERSION}/train_joined_prop_count'\n",
    "\n",
    "train_class_count = f'{train_output_path}/{VERSION}/train_class_count'\n",
    "train_trigram_class_count = f'{train_output_path}/{VERSION}/train_trigram_class_count'\n",
    "train_joined_class_count = f'{train_output_path}/{VERSION}/train_joined_class_count'\n",
    "\n",
    "train_context_path = f'{train_output_path}/{VERSION}/train_context'\n",
    "train_trigram_context_path = f'{train_output_path}/{VERSION}/train_trigram_context'\n",
    "train_joined_context_path = f'{train_output_path}/{VERSION}/train_joined_context'\n",
    "\n",
    "train_graph_embedding = f'{train_output_path}/{VERSION}/train_graph_embedding'\n",
    "train_trigram_graph_embedding = f'{train_output_path}/{VERSION}/train_trigram_graph_embedding'\n",
    "train_joined_graph_embedding = f'{train_output_path}/{VERSION}/train_joined_graph_embedding'\n",
    "\n",
    "train_context_property_path = f'{train_output_path}/{VERSION}/train_context_properties'\n",
    "\n",
    "dev_prop_count = f'{dev_output_path}/{VERSION}/dev_prop_count'\n",
    "dev_trigram_prop_count = f'{dev_output_path}/{VERSION}/dev_trigram_prop_count'\n",
    "dev_joined_prop_count = f'{dev_output_path}/{VERSION}/dev_joined_prop_count'\n",
    "\n",
    "dev_class_count = f'{dev_output_path}/{VERSION}/dev_class_count'\n",
    "dev_trigram_class_count = f'{dev_output_path}/{VERSION}/dev_trigram_class_count'\n",
    "dev_joined_class_count = f'{dev_output_path}/{VERSION}/dev_joined_class_count'\n",
    "\n",
    "dev_context_path = f'{dev_output_path}/{VERSION}/dev_context'\n",
    "dev_trigram_context_path = f'{dev_output_path}/{VERSION}/dev_trigram_context'\n",
    "dev_joined_context_path = f'{dev_output_path}/{VERSION}/dev_joined_context'\n",
    "\n",
    "dev_graph_embedding = f'{dev_output_path}/{VERSION}/dev_graph_embedding'\n",
    "dev_trigram_graph_embedding = f'{dev_output_path}/{VERSION}/dev_trigram_graph_embedding'\n",
    "dev_joined_graph_embedding = f'{dev_output_path}/{VERSION}/dev_joined_graph_embedding'\n",
    "\n",
    "dev_context_property_path = f'{dev_output_path}/{VERSION}/dev_context_properties'\n",
    "dev_incorrect_answers_path = f'{dev_output_path}/{VERSION}/incorrect_answers.csv'\n",
    "\n",
    "temp_dir = f'{output_path}/temp'\n",
    "\n",
    "pseudo_gt_model = '/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/dev-output/pseudo-gt-nn/v19/saved_models/epoch_1_loss_0.534353494644165_top1_0.7883487007544007.pth'\n",
    "pseudo_gt_min_max_scaler_path = '/Users/amandeep/Github/table-linker/data/SemTab2020/Round4/table-linker/temp/training_data/normalization_factor.pkl'\n",
    "\n",
    "tl_log_file =f'{temp_dir}/tl_log.txt'\n",
    "\n",
    "training_data_path = f'{dev_output_path}/{VERSION}/training_data'\n",
    "pos_output = f'{training_data_path}/tl_pipeline_pos_features.pkl'\n",
    "neg_output = f'{training_data_path}/tl_pipeline_neg_features.pkl'\n",
    "min_max_scaler_path = f'{training_data_path}/tl_pipeline_normalization_factor.pkl'\n",
    "\n",
    "final_score_column = 'siamese_prediction'\n",
    "threshold = final_score_column+\":median\"\n",
    "\n",
    "model_save_path = f'{dev_output_path}/{VERSION}/saved_models'\n",
    "best_model_path = ''\n",
    "\n",
    "copy_candidates_from_previous_version = False\n",
    "copy_features1_from_previous_version = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sensitive-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$temp_dir\"\n",
    "\n",
    "!mkdir -p \"$train_trigram_candidate_path\"\n",
    "!mkdir -p \"$train_trigram_features1_path\"\n",
    "!mkdir -p \"$dev_trigram_candidate_path\"\n",
    "!mkdir -p \"$dev_trigram_features1_path\"\n",
    "\n",
    "!mkdir -p \"$train_prop_count\"\n",
    "!mkdir -p \"$train_trigram_prop_count\"\n",
    "!mkdir -p \"$dev_prop_count\"\n",
    "!mkdir -p \"$dev_trigram_prop_count\"\n",
    "!mkdir -p \"$train_class_count\"\n",
    "!mkdir -p \"$train_trigram_class_count\"\n",
    "!mkdir -p \"$dev_class_count\"\n",
    "!mkdir -p \"$dev_trigram_class_count\"\n",
    "!mkdir -p \"$train_graph_embedding\"\n",
    "!mkdir -p \"$train_trigram_graph_embedding\"\n",
    "!mkdir -p \"$dev_graph_embedding\"\n",
    "!mkdir -p \"$dev_trigram_graph_embedding\"\n",
    "!mkdir -p \"$train_context_path\"\n",
    "!mkdir -p \"$train_trigram_context_path\"\n",
    "!mkdir -p \"$dev_context_path\"\n",
    "!mkdir -p \"$dev_trigram_context_path\"\n",
    "\n",
    "!mkdir -p \"$train_candidate_path\"\n",
    "!mkdir -p \"$dev_candidate_path\"\n",
    "\n",
    "!mkdir -p \"$train_feature_path\"\n",
    "!mkdir -p \"$train_features1_path\"\n",
    "!mkdir -p \"$train_features2_path\"\n",
    "!mkdir -p \"$train_features3_path\"\n",
    "!mkdir -p \"$dev_feature_path\"\n",
    "!mkdir -p \"$dev_features1_path\"\n",
    "!mkdir -p \"$dev_features2_path\"\n",
    "!mkdir -p \"$dev_features3_path\"\n",
    "\n",
    "!mkdir -p \"$temp_dir/training_data\"\n",
    "!mkdir -p \"$dev_output_predictions\"\n",
    "!mkdir -p \"$model_save_path\"\n",
    "!mkdir -p \"$dev_predictions_top_k\"\n",
    "!mkdir -p \"$dev_colorized_path\"\n",
    "!mkdir -p \"$dev_metrics_path\"\n",
    "!mkdir -p \"$dev_missing_candidates_path\"\n",
    "!mkdir -p \"$train_missing_candidates_path\"\n",
    "!mkdir -p \"$train_context_property_path\"\n",
    "!mkdir -p \"$dev_context_property_path\"\n",
    "\n",
    "!mkdir -p \"$training_data_path\"\n",
    "\n",
    "!mkdir -p \"$train_features1_joined_path\"\n",
    "!mkdir -p \"$dev_features1_joined_path\"\n",
    "!mkdir -p \"$train_joined_prop_count\"\n",
    "!mkdir -p \"$train_joined_class_count\"\n",
    "!mkdir -p \"$train_joined_context_path\"\n",
    "!mkdir -p \"$train_joined_graph_embedding\"\n",
    "!mkdir -p \"$dev_joined_prop_count\"\n",
    "!mkdir -p \"$dev_joined_class_count\"\n",
    "!mkdir -p \"$dev_joined_context_path\"\n",
    "!mkdir -p \"$dev_joined_graph_embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romance-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "if copy_candidates_from_previous_version:\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/candidates/*csv $dev_output_path/$VERSION/candidates\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_prop_count/* $dev_output_path/$VERSION/dev_prop_count\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_class_count/* $dev_output_path/$VERSION/dev_class_count\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_context/* $dev_output_path/$VERSION/dev_context\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/dev_graph_embedding/* $dev_output_path/$VERSION/dev_graph_embedding\n",
    "    \n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/candidates/*csv $train_output_path/$VERSION/candidates\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_prop_count/* $train_output_path/$VERSION/train_prop_count\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_class_count/* $train_output_path/$VERSION/train_class_count\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_context/* $train_output_path/$VERSION/train_context\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/train_graph_embedding/* $train_output_path/$VERSION/train_graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e96e759-e94d-40a6-8e77-7412edaf22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_gt_features = [\"monge_elkan\",\"monge_elkan_aliases\",\"jaro_winkler\",\"levenshtein\",\"singleton\",\"pgr_rts\",\"context_score\",\"smc_class_score\",\"smc_property_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-tradition",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "planned-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    file_list = glob.glob(path + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        gt_file = f\"{ground_truth_files}/{filename}\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "        !tl --log-file $tl_log_file clean -c label -o label_clean \"$file\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" / \\\n",
    "        / get-ngram-matches -c label_clean  \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" \\\n",
    "        / get-trigram-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" \\\n",
    "        / ground-truth-labeler --gt-file \"$gt_file\" > \"$output_file\"\n",
    "\n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            if field == 'context':\n",
    "                file_list = glob.glob(f'{temp_dir}/*{field}.jl')\n",
    "                o_f = open(f\"{context_path}/{filename[:-4]}_context.jl\", 'w')\n",
    "                \n",
    "                for i_f_P in file_list:\n",
    "                    i_f = open(i_f_P)\n",
    "                    for line in i_f:\n",
    "                        o_f.write(line)\n",
    "                    i_f.close()\n",
    "                o_f.close()\n",
    "                \n",
    "            else:\n",
    "                for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                    aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "                aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "                if field == 'class_count':\n",
    "                    class_count_file = f\"{class_count_path}/{filename[:-4]}_class_count.tsv\"\n",
    "                    aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "                elif field == 'property_count':\n",
    "                    prop_count_file = f\"{prop_count_path}/{filename[:-4]}_prop_count.tsv\"\n",
    "                    aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "                else:\n",
    "                    graph_embedding_file = f\"{graph_embedding}/{filename[:-4 ]}_graph_embedding_complex.tsv\"\n",
    "                    aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [09:22, 39.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [\"167]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [\"167]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'\"167\\': Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [\"167]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'\"167\\': Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 5.  Encountered: <EOF> after : \"\\\\\"167\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [13:04, 35.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [Forces\"Commaod]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Forces\"Commaod]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'Forces\"Commaod\\': Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"Commaod\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"Commaod\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"commaod\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [forces\"commaod]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'forces\"commaod\\': Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"commaod\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"commaod\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 15.  Encountered: <EOF> after : \"\\\\\"commaod\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [24:00, 50.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 500!\n",
      "{'error': {'root_cause': [{'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 10598 states and 21399 transitions would result in more than 10000 states.'}, {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: follow-up study of safety and efficacy of pneumostem® in premature infants with bronchopulmonary dysplasia'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'fuzzy_terms_exception: Term too complex: follow-up study of safety and efficacy of pneumostem® in premature infants with bronchopulmonary dysplasia', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 10598 states and 21399 transitions would result in more than 10000 states.'}}}, {'shard': 1, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: follow-up study of safety and efficacy of pneumostem® in premature infants with bronchopulmonary dysplasia', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 10598 states and 21399 transitions would result in more than 10000 states.'}}}], 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 10598 states and 21399 transitions would result in more than 10000 states.'}}, 'status': 500}\n",
      "entered except\n",
      "Command: get-fuzzy-augmented-matches\n",
      "Error Message: Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-fuzzy-augmented-matches.py\", line 74, in run\n",
      "    odf = em.get_matches(column=kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_fuzzy_augmented_matches.py\", line 44, in get_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 32, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 378, in search_term_candidates\n",
      "    hits = self.create_fuzzy_augmented_union(fuzzy_augmented_hits, fuzzy_augmented_keyword_lower_hits)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 318, in create_fuzzy_augmented_union\n",
      "    for item in fuzzy_augmented_keyword_lower_hits:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "Command: get-exact-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-exact-matches.py\", line 68, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 65, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [24:38, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [138 East 50th\"Street]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [138 East 50th\"Street]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'138 East 50th\"Street\\': Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Street\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Street\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"street\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [138 east 50th\"street]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'138 east 50th\"street\\': Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"street\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"street\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"street\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [26:49, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der Franziskaner\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Missionsznurale\"der Franziskaner]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Missionsznurale\"der Franziskaner\\': Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der Franziskaner\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der Franziskaner\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der Franziskaner\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [missionsznurale\"der franziskaner]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [missionsznurale\"der franziskaner]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'missionsznurale\"der franziskaner\\': Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der franziskaner\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"der franziskaner\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [34:58, 37.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Rowing Cup III\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2007 World\"Rowing Cup III]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'2007 World\"Rowing Cup III\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Rowing Cup III\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Rowing Cup III\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Rowing Cup III\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"rowing cup iii\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2007 world\"rowing cup iii]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'2007 world\"rowing cup iii\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"rowing cup iii\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"rowing cup iii\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"rowing cup iii\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [38:59, 40.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"L\\\\u00e1zaro Psychiatric Hospital\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [San\"Lázaro Psychiatric Hospital]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'San\"Lázaro Psychiatric Hospital\\': Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"L\\\\u00e1zaro Psychiatric Hospital\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"L\\\\u00e1zaro Psychiatric Hospital\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"L\\\\u00e1zaro Psychiatric Hospital\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"l\\\\u00e1zaro psychiatric hospital\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [san\"lázaro psychiatric hospital]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'san\"lázaro psychiatric hospital\\': Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"l\\\\u00e1zaro psychiatric hospital\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"l\\\\u00e1zaro psychiatric hospital\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 32.  Encountered: <EOF> after : \"\\\\\"l\\\\u00e1zaro psychiatric hospital\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [42:22, 48.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"Vuelta a Andaluc\\\\u00eda\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2010\"Vuelta a Andalucía]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'2010\"Vuelta a Andalucía\\': Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"Vuelta a Andaluc\\\\u00eda\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"Vuelta a Andaluc\\\\u00eda\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"Vuelta a Andaluc\\\\u00eda\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [2011 Tour du Haut\"Var]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2011 Tour du Haut\"Var]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'2011 Tour du Haut\"Var\\': Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"Var\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"Var\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [2010\"vuelta a andalucía]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2010\"vuelta a andalucía]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'2010\"vuelta a andalucía\\': Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"vuelta a andaluc\\\\u00eda\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 24.  Encountered: <EOF> after : \"\\\\\"vuelta a andaluc\\\\u00eda\"'}}}}]}, 'status': 400}\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"var\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2011 tour du haut\"var]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'2011 tour du haut\"var\\': Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"var\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"var\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 22.  Encountered: <EOF> after : \"\\\\\"var\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [53:25, 38.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 500!\n",
      "{'error': {'root_cause': [{'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: adresse des habitants des trois faubourgs\"de la ville de montréal à richard montgomery, brigadier général des forces du continent'}, {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 13350 states and 26687 transitions would result in more than 10000 states.'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: adresse des habitants des trois faubourgs\"de la ville de montréal à richard montgomery, brigadier général des forces du continent', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 13350 states and 26687 transitions would result in more than 10000 states.'}}}, {'shard': 3, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'fuzzy_terms_exception: Term too complex: adresse des habitants des trois faubourgs\"de la ville de montréal à richard montgomery, brigadier général des forces du continent', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 13350 states and 26687 transitions would result in more than 10000 states.'}}}], 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: adresse des habitants des trois faubourgs\"de la ville de montréal à richard montgomery, brigadier général des forces du continent', 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: adresse des habitants des trois faubourgs\"de la ville de montréal à richard montgomery, brigadier général des forces du continent', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 13350 states and 26687 transitions would result in more than 10000 states.'}}}}, 'status': 500}\n",
      "entered except\n",
      "Command: get-fuzzy-augmented-matches\n",
      "Error Message: Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-fuzzy-augmented-matches.py\", line 74, in run\n",
      "    odf = em.get_matches(column=kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_fuzzy_augmented_matches.py\", line 44, in get_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 32, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 378, in search_term_candidates\n",
      "    hits = self.create_fuzzy_augmented_union(fuzzy_augmented_hits, fuzzy_augmented_keyword_lower_hits)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 318, in create_fuzzy_augmented_union\n",
      "    for item in fuzzy_augmented_keyword_lower_hits:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "Command: get-exact-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-exact-matches.py\", line 68, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 65, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [1:24:41, 42.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [1989 Prueba Villafranca de\"Ordizia]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [1989 Prueba Villafranca de\"Ordizia]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'1989 Prueba Villafranca de\"Ordizia\\': Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"Ordizia\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"Ordizia\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"ordizia\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [1989 prueba villafranca de\"ordizia]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'1989 prueba villafranca de\"ordizia\\': Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"ordizia\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"ordizia\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"ordizia\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [1:30:51, 43.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 500!\n",
      "{'error': {'root_cause': [{'type': 'fuzzy_terms_exception', 'reason': \"Term too complex: données d'observations pour la reconnaissance et l'identification de la faune et de la flore subaquatiques\"}, {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 10658 states and 21419 transitions would result in more than 10000 states.'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'fuzzy_terms_exception', 'reason': \"Term too complex: données d'observations pour la reconnaissance et l'identification de la faune et de la flore subaquatiques\", 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 10658 states and 21419 transitions would result in more than 10000 states.'}}}, {'shard': 3, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'fuzzy_terms_exception', 'reason': \"fuzzy_terms_exception: Term too complex: données d'observations pour la reconnaissance et l'identification de la faune et de la flore subaquatiques\", 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 10658 states and 21419 transitions would result in more than 10000 states.'}}}], 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': \"Term too complex: données d'observations pour la reconnaissance et l'identification de la faune et de la flore subaquatiques\", 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': \"Term too complex: données d'observations pour la reconnaissance et l'identification de la faune et de la flore subaquatiques\", 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 10658 states and 21419 transitions would result in more than 10000 states.'}}}}, 'status': 500}\n",
      "entered except\n",
      "Command: get-fuzzy-augmented-matches\n",
      "Error Message: Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-fuzzy-augmented-matches.py\", line 74, in run\n",
      "    odf = em.get_matches(column=kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_fuzzy_augmented_matches.py\", line 44, in get_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 32, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 378, in search_term_candidates\n",
      "    hits = self.create_fuzzy_augmented_union(fuzzy_augmented_hits, fuzzy_augmented_keyword_lower_hits)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 318, in create_fuzzy_augmented_union\n",
      "    for item in fuzzy_augmented_keyword_lower_hits:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "Command: get-exact-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-exact-matches.py\", line 68, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 65, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [1:31:50, 36.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de Barber\\\\u00e0\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Conca\"de Barberà]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Conca\"de Barberà\\': Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de Barber\\\\u00e0\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de Barber\\\\u00e0\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de Barber\\\\u00e0\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [conca\"de barberà]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [conca\"de barberà]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'conca\"de barberà\\': Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de barber\\\\u00e0\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"de barber\\\\u00e0\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [1:44:16, 36.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [MSC\"Julia R]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [MSC\"Julia R]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'MSC\"Julia R\\': Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"Julia R\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"Julia R\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"julia r\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [msc\"julia r]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'msc\"julia r\\': Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"julia r\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"julia r\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 12.  Encountered: <EOF> after : \"\\\\\"julia r\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [1:47:00, 39.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1240, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1286, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1235, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1006, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 946, in send\n",
      "    self.connect()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1192e8a00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1192e8a00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 73, in run\n",
      "    odf = em.get_ngram_matches(kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/ngram_matches.py\", line 49, in get_ngram_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 56, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 368, in search_term_candidates\n",
      "    hits = self.search_es(self.create_ngram_query(search_term, size=size, extra_musts=extra_musts))\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 41, in search_es\n",
      "    response = requests.post(es_search_url, json=query)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 117, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1192e8a00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [2:13:54, 42.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Eisenb\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Griethausener\"Eisenb]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Griethausener\"Eisenb\\': Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Eisenb\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Eisenb\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 21.  Encountered: <EOF> after : \"\\\\\"Eisenb\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [griethausener\"eisenbahnbrücke]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [griethausener\"eisenbahnbrücke]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'griethausener\"eisenbahnbrücke\\': Lexical error at line 1, column 30.  Encountered: <EOF> after : \"\\\\\"eisenbahnbr\\\\u00fccke\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 30.  Encountered: <EOF> after : \"\\\\\"eisenbahnbr\\\\u00fccke\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [2:18:07, 41.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Bucharest\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Arcul de Triumf\"Bucharest]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Arcul de Triumf\"Bucharest\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Bucharest\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Bucharest\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Bucharest\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [arcul de triumf\"bucharest]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [arcul de triumf\"bucharest]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'arcul de triumf\"bucharest\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"bucharest\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"bucharest\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [2:27:23, 45.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1240, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1286, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1235, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1006, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 946, in send\n",
      "    self.connect()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1169653d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1169653d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 80, in run\n",
      "    odf = tgm.get_trigram_matches(kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_trigram_matches.py\", line 141, in get_trigram_matches\n",
      "    self.utility.create_candidates_df(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 56, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 360, in search_term_candidates\n",
      "    hits = self.search_es(self.create_trigram_query(search_term,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 41, in search_es\n",
      "    response = requests.post(es_search_url, json=query)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 117, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1169653d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "215it [2:28:15, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 500!\n",
      "{'error': {'root_cause': [{'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: ennoblement act of jan januszowski , publisher and manager of the oficyna łazarzowa in kraków, graqted by king sigismund iii vasa'}, {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 13130 states and 26480 transitions would result in more than 10000 states.'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: ennoblement act of jan januszowski , publisher and manager of the oficyna łazarzowa in kraków, graqted by king sigismund iii vasa', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 13130 states and 26480 transitions would result in more than 10000 states.'}}}, {'shard': 1, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'fuzzy_terms_exception', 'reason': 'fuzzy_terms_exception: Term too complex: ennoblement act of jan januszowski , publisher and manager of the oficyna łazarzowa in kraków, graqted by king sigismund iii vasa', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'too_complex_to_determinize_exception: Determinizing automaton with 13130 states and 26480 transitions would result in more than 10000 states.'}}}], 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: ennoblement act of jan januszowski , publisher and manager of the oficyna łazarzowa in kraków, graqted by king sigismund iii vasa', 'caused_by': {'type': 'fuzzy_terms_exception', 'reason': 'Term too complex: ennoblement act of jan januszowski , publisher and manager of the oficyna łazarzowa in kraków, graqted by king sigismund iii vasa', 'caused_by': {'type': 'too_complex_to_determinize_exception', 'reason': 'Determinizing automaton with 13130 states and 26480 transitions would result in more than 10000 states.'}}}}, 'status': 500}\n",
      "entered except\n",
      "Command: get-fuzzy-augmented-matches\n",
      "Error Message: Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-fuzzy-augmented-matches.py\", line 74, in run\n",
      "    odf = em.get_matches(column=kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_fuzzy_augmented_matches.py\", line 44, in get_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 32, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 378, in search_term_candidates\n",
      "    hits = self.create_fuzzy_augmented_union(fuzzy_augmented_hits, fuzzy_augmented_keyword_lower_hits)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 318, in create_fuzzy_augmented_union\n",
      "    for item in fuzzy_augmented_keyword_lower_hits:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "Command: get-exact-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-exact-matches.py\", line 68, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 65, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233it [2:39:24, 38.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"Theological Seminary\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Tbilisi\"Theological Seminary]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Tbilisi\"Theological Seminary\\': Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"Theological Seminary\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"Theological Seminary\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"Theological Seminary\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Theological Academy\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Tbilisi\"Theological Academy]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Tbilisi\"Theological Academy\\': Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Theological Academy\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Theological Academy\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Theological Academy\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [tbilisi\"theological academy]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [tbilisi\"theological academy]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'tbilisi\"theological academy\\': Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"theological academy\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"theological academy\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"theological seminary\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [tbilisi\"theological seminary]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'tbilisi\"theological seminary\\': Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"theological seminary\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"theological seminary\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"theological seminary\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [2:59:18, 35.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"Gordian I\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Arch of\"Gordian I]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Arch of\"Gordian I\\': Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"Gordian I\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"Gordian I\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"Gordian I\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [porte\"Sainte-Croix]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [porte\"Sainte-Croix]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'porte\"Sainte-Croix\\': Lexical error at line 1, column 19.  Encountered: <EOF> after : \"\\\\\"Sainte-Croix\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 19.  Encountered: <EOF> after : \"\\\\\"Sainte-Croix\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [Rua Augusta\"Arch]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Rua Augusta\"Arch]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'Rua Augusta\"Arch\\': Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"Arch\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"Arch\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [arch of\"gordian i]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [arch of\"gordian i]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'arch of\"gordian i\\': Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"gordian i\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 18.  Encountered: <EOF> after : \"\\\\\"gordian i\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [porte\"sainte-croix]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [porte\"sainte-croix]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'porte\"sainte-croix\\': Lexical error at line 1, column 19.  Encountered: <EOF> after : \"\\\\\"sainte-croix\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 19.  Encountered: <EOF> after : \"\\\\\"sainte-croix\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"arch\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [rua augusta\"arch]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'rua augusta\"arch\\': Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"arch\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"arch\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 17.  Encountered: <EOF> after : \"\\\\\"arch\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [3:04:03, 37.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Republic of Poland\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [People\\'s\"Republic of Poland]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'People\\'s\"Republic of Poland\\': Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Republic of Poland\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Republic of Poland\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"Republic of Poland\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [people\\'s\"republic of poland]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [people\\'s\"republic of poland]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'people\\'s\"republic of poland\\': Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"republic of poland\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 28.  Encountered: <EOF> after : \"\\\\\"republic of poland\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [3:11:56, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"Mary\\\\\\'s Academy and College\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Saint\"Mary\\'s Academy and College]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Saint\"Mary\\'s Academy and College\\': Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"Mary\\\\\\'s Academy and College\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"Mary\\\\\\'s Academy and College\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"Mary\\\\\\'s Academy and College\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [saint\"mary\\'s academy and college]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [saint\"mary\\'s academy and college]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'saint\"mary\\'s academy and college\\': Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"mary\\\\\\'s academy and college\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 33.  Encountered: <EOF> after : \"\\\\\"mary\\\\\\'s academy and college\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [3:22:04, 40.28s/it]\n"
     ]
    }
   ],
   "source": [
    "if not copy_candidates_from_previous_version:\n",
    "    candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_context_path,train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [11:41, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Park\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Vélodsome de Queen\\'s\"Park]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Vélodsome de Queen\\'s\"Park\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Park\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Park\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"Park\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de Vincennes\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Vélodrome\"de Vincennes]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'Vélodrome\"de Vincennes\\': Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de Vincennes\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de Vincennes\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de Vincennes\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"park\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [vélodsome de queen\\'s\"park]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'vélodsome de queen\\'s\"park\\': Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"park\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"park\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 26.  Encountered: <EOF> after : \"\\\\\"park\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de vincennes\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [vélodrome\"de vincennes]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'vélodrome\"de vincennes\\': Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de vincennes\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de vincennes\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 23.  Encountered: <EOF> after : \"\\\\\"de vincennes\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [21:01, 47.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: get-exact-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1240, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1286, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1235, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 1006, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\", line 946, in send\n",
      "    self.connect()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connection.py\", line 181, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x117bf1400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x117bf1400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-exact-matches.py\", line 72, in run\n",
      "    odf = em.get_exact_matches(kwargs['column'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/get_exact_matches.py\", line 49, in get_exact_matches\n",
      "    return self.utility.create_candidates_df(df,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 56, in create_candidates_df\n",
      "    for _candidates_format, candidates_aux_dict in executor.map(\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 611, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/Cellar/python@3.8/3.8.3_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/utility.py\", line 90, in create_candidates\n",
      "    candidate_dict, candidate_aux_dict = self.es.search_term_candidates(\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 346, in search_term_candidates\n",
      "    hits = self.search_es(self.create_exact_match_query(search_term, lower_case, size, properties,\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/candidate_generation/es_search.py\", line 41, in search_es\n",
      "    response = requests.post(es_search_url, json=query)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 117, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='ckg07', port=9200): Max retries exceeded with url: /wikidatadwd-augmented-08/_search (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x117bf1400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "Command: get-ngram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-ngram-matches.py\", line 65, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: get-trigram-matches\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/get-trigram-matches.py\", line 72, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: ground-truth-labeler\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/ground-truth-labeler.py\", line 34, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 69, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 549, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [31:17, 40.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [2015 Ultramort Town Council\"election]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2015 Ultramort Town Council\"election]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'2015 Ultramort Town Council\"election\\': Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [2015 ultramort town council\"election]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'2015 ultramort town council\"election\\': Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 37.  Encountered: <EOF> after : \"\\\\\"election\"'}}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [35:05, 40.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [Sweden\"national footballteam]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Sweden\"national footballteam]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'Sweden\"national footballteam\\': Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [Netherlands\"national football team]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [Netherlands\"national football team]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'Netherlands\"national football team\\': Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"national football team\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"national football team\"'}}}}]}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': '_1cSOPZbS42KxMr93lgE6Q', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [sweden\"national footballteam]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'parse_exception: Cannot parse \\'sweden\"national footballteam\\': Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"'}}}}], 'caused_by': {'type': 'token_mgr_error', 'reason': 'token_mgr_error: Lexical error at line 1, column 29.  Encountered: <EOF> after : \"\\\\\"national footballteam\"'}}, 'status': 400}\n",
      "Query ES error with response 400!\n",
      "{'error': {'root_cause': [{'type': 'query_shard_exception', 'reason': 'Failed to parse query [netherlands\"national football team]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08'}], 'type': 'search_phase_execution_exception', 'reason': 'all shards failed', 'phase': 'query', 'grouped': True, 'failed_shards': [{'shard': 0, 'index': 'wikidatadwd-augmented-08', 'node': 'u7ow2JmvTsGBLyDnBPiVbQ', 'reason': {'type': 'query_shard_exception', 'reason': 'Failed to parse query [netherlands\"national football team]', 'index_uuid': 'pJOiXAWGRsy4IhbeHIcG5Q', 'index': 'wikidatadwd-augmented-08', 'caused_by': {'type': 'parse_exception', 'reason': 'Cannot parse \\'netherlands\"national football team\\': Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"national football team\"', 'caused_by': {'type': 'token_mgr_error', 'reason': 'Lexical error at line 1, column 35.  Encountered: <EOF> after : \"\\\\\"national football team\"'}}}}]}, 'status': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [37:31, 44.14s/it]\n"
     ]
    }
   ],
   "source": [
    "if not copy_candidates_from_previous_version:\n",
    "    candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_context_path, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-visitor",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_1(candidate_dir, output_path, context_path, class_count_dir, property_count_dir, pseudo_gt_model, pseudo_gt_min_max_scaler_path):\n",
    "    file_list = glob.glob(candidate_dir + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "#         if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "#             continue\n",
    "        \n",
    "        context_file = f\"{context_path}/{filename[:-4]}_context.tsv\"\n",
    "        class_count_file = f\"{class_count_dir}/{filename[:-4]}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}/{filename[:-4]}_prop_count.tsv\"\n",
    "        feature_str =  \",\".join(pseudo_gt_features)\n",
    "\n",
    "        !tl --log-file $tl_log_file deduplicate-candidates -c kg_id $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan --threshold 0.5 \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases --threshold 0.5 \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler --threshold 0.5 \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein --threshold 0.5 \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            / pick-hc-candidates -o ignore_candidate --string-similarity-label-columns monge_elkan,jaro_winkler,levenshtein --string-similarity-alias-columns monge_elkan_aliases \\\n",
    "            / context-match --debug --context-file $context_file --ignore-column-name ignore_candidate -o context_score \\\n",
    "            / kth-percentile -c context_score -o kth_percenter --ignore-column ignore_candidate --k-percentile 0.75  --minimum-cells 10 \\\n",
    "            / pgt-semantic-tf-idf \\\n",
    "            -o smc_class_score \\\n",
    "            --pagerank-column pagerank \\\n",
    "            --retrieval-score-column retrieval_score \\\n",
    "            --feature-file \"$class_count_file\" \\\n",
    "            --feature-name class_count \\\n",
    "            --high-confidence-column kth_percenter \\\n",
    "            / pgt-semantic-tf-idf \\\n",
    "            -o smc_property_score \\\n",
    "            --pagerank-column pagerank \\\n",
    "            --retrieval-score-column retrieval_score \\\n",
    "            --feature-file \"$property_count_file\" \\\n",
    "            --feature-name property_count \\\n",
    "            --high-confidence-column kth_percenter \\\n",
    "            / predict-using-model -o pseudo_gt_prediction \\\n",
    "            --features $feature_str \\\n",
    "            --ranking-model $pseudo_gt_model \\\n",
    "            --ignore-column ignore_candidate \\\n",
    "            --normalization-factor $pseudo_gt_min_max_scaler_path \\\n",
    "            / create-pseudo-gt -o pseudo_gt \\\n",
    "            --column-thresholds pseudo_gt_prediction:mean \\\n",
    "            --filter smc_class_score:0 > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4384ae2-5ebc-4774-aa28-16cf973076c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not copy_features1_from_previous_version:\n",
    "    features_1(train_candidate_path, train_features1_path, train_context_path, train_class_count, train_prop_count,pseudo_gt_model, pseudo_gt_min_max_scaler_path)\n",
    "else:\n",
    "    !cp $train_output_path/$PREVIOUS_VERSION/features1/*csv $train_output_path/$VERSION/features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657676b-d125-4898-96c7-a5def6183212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not copy_features1_from_previous_version:\n",
    "    features_1(dev_candidate_path, dev_features1_path, dev_context_path, dev_class_count, dev_prop_count,pseudo_gt_model, pseudo_gt_min_max_scaler_path)\n",
    "else:\n",
    "    !cp $dev_output_path/$PREVIOUS_VERSION/features1/*csv $dev_output_path/$VERSION/features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53382461-51e1-439f-a58e-606bbf42cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_candidate_generation(path, gt_path, output_path, class_count_path, prop_count_path, context_path, graph_embedding):\n",
    "    file_list = glob.glob(path + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        filename = file.split('/')[-1]\n",
    "        gt_file = f\"{ground_truth_files}/{filename}\"\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        \n",
    "        os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        \n",
    "        !tl --log-file $tl_log_file --url $es_url --index $es_index \\\n",
    "        get-trigram-matches \"$file\" -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder \"$temp_dir\" \\\n",
    "        --pseudo-gt-column pseudo_gt \\\n",
    "        / ground-truth-labeler --gt-file \"$gt_file\" > \"$output_file\"\n",
    "\n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/trigram_matches_*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = f\"{class_count_path}/{filename.strip('.csv')}_class_count.tsv\"\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = f\"{prop_count_path}/{filename.strip('.csv')}_prop_count.tsv\"\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            elif field == 'context':\n",
    "                context_file = f\"{context_path}/{filename.strip('.csv')}_context.tsv\"\n",
    "                aux_df.to_csv(context_file, sep='\\t', index=False)\n",
    "            else:\n",
    "                graph_embedding_file = f\"{graph_embedding}/{filename.strip('.csv')}_graph_embedding_complex.tsv\"\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99babe1-14d4-4246-9c53-abdf8488755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_candidate_generation(train_features1_path, ground_truth_files, train_trigram_candidate_path, train_trigram_class_count, train_trigram_prop_count, train_trigram_context_path, train_trigram_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce5aea-ed73-4dc7-847d-e0f84c16315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_candidate_generation(dev_features1_path, ground_truth_files, dev_trigram_candidate_path, dev_trigram_class_count, dev_trigram_prop_count, dev_trigram_context_path, dev_trigram_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f356b7-6623-4527-9582-fee39c550ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1(train_trigram_candidate_path, train_trigram_features1_path, train_trigram_context_path, train_trigram_class_count, train_trigram_prop_count,pseudo_gt_model, pseudo_gt_min_max_scaler_path)getsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84286a65-0034-4240-aed8-ad78ff04fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1(dev_trigram_candidate_path, dev_trigram_features1_path, dev_trigram_context_path, dev_trigram_class_count, dev_trigram_prop_count,pseudo_gt_model, pseudo_gt_min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65b8e0-3843-4ea6-87b0-5d509d6ba176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_all_files(path1, path2, output_path, file_type='csv'):\n",
    "    file_list = glob.glob(f'{path1}/*.{file_type}')\n",
    "    sep='\\t' if file_type == 'tsv' else ','\n",
    "                          \n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "         \n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        out = []\n",
    "        if os.path.getsize(file) > 0:\n",
    "            out.append(pd.read_csv(file, sep=sep))\n",
    "        file2 = f\"{path2}/{filename}\"\n",
    "        if os.path.exists(file2) and os.path.getsize(file2) > 0:\n",
    "            out.append(pd.read_csv(file2, sep=sep))\n",
    "\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        out_df = pd.concat(out)\n",
    "        if len(out_df) > 0:\n",
    "            out_df.to_csv(output_file, index=False, sep=sep)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334050c1-d1d2-451d-b467-88c898a86afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(train_prop_count,train_trigram_prop_count, train_joined_prop_count, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519a61d-c6cb-43e3-aaf1-41a50f1eadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(train_class_count,train_trigram_class_count, train_joined_class_count, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fd19c-6201-40ed-b31e-ce77a0a3028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(train_context_path,train_trigram_context_path, train_joined_context_path, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d25e1-e94f-475b-b7e1-4215109d0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(train_graph_embedding,train_trigram_graph_embedding, train_joined_graph_embedding, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5466c13-b40e-4a9b-a639-8f72ba3da276",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(dev_prop_count,dev_trigram_prop_count, dev_joined_prop_count, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10e8c9-4254-4a4c-858f-f847c90664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(dev_class_count,dev_trigram_class_count, dev_joined_class_count, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c511d-d5bb-4c93-b941-9b3f0e2c4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(dev_context_path,dev_trigram_context_path, dev_joined_context_path, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c487363-078c-4bbe-bacc-dc3cc70d6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(dev_graph_embedding,dev_trigram_graph_embedding, dev_joined_graph_embedding, file_type='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40a6b5-abc0-4e46-a5a4-240cd58bad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(train_features1_path, train_trigram_features1_path,train_features1_joined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d1a30-578a-4062-b1cf-58a60d711fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_files(dev_features1_path, dev_trigram_features1_path,dev_features1_joined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7e2e6-2bae-4495-b80f-5ff08d741d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc3cf1-e8d5-4827-ace9-09507f496d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2(features1_dir, output_path, context_path, context_property_path):\n",
    "    file_list = glob.glob(features1_dir + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        context_file = f\"{context_path}/{filename[:-4]}_context.tsv\"\n",
    "        context_property_file = f\"{context_property_path}/{filename[:-4]}_context_properties.csv\"\n",
    "\n",
    "        !tl --log-file $tl_log_file context-match $file -o context_score_2 \\\n",
    "        --context-file $context_file --save-property-scores-path $context_property_file \\\n",
    "        --pseudo-gt-column-name pseudo_gt > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259af2d4-26af-4971-9518-40644b555657",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2(train_features1_joined_path, train_features2_path, train_joined_context_path, train_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89733ec8-ebaf-4c7a-baca-57f3aafa936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2(train_features1_path, train_features2_path, train_context_path, train_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630fa59-b8cd-4afd-ad52-3ee763bc0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2(dev_features1_joined_path, dev_features2_path, dev_joined_context_path, dev_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14f055-1669-4a55-a211-1c8486b90c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2(dev_features1_path, dev_features2_path, dev_context_path, dev_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e614c-378d-48e0-b10d-bef9af099d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2_1(features2_dir, output_path, context_path, context_property_path):\n",
    "    file_list = glob.glob(features2_dir + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        \n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        context_file = f\"{context_path}/{filename[:-4]}_context.tsv\"\n",
    "        context_property_file = f\"{context_property_path}/{filename[:-4]}_context_properties.csv\"\n",
    "            \n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "            continue\n",
    "\n",
    "        !tl --log-file $tl_log_file context-match $file --context-file $context_file -o context_score_3 \\\n",
    "        --use-saved-property-scores $context_property_file > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356222b-0727-49d9-a55d-8d97361bc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2_1(train_features2_path, train_features3_path, train_joined_context_path, train_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474f450-2b0a-454d-b248-48a56dbf94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2_1(train_features2_path, train_features3_path, train_context_path, train_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b5d50-1a18-4eee-aa59-79984bef79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2_1(dev_features2_path, dev_features3_path, dev_joined_context_path, dev_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf26e6-2b58-4f8a-833a-55c7be6c147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2_1(dev_features2_path, dev_features3_path, dev_context_path, dev_context_property_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ba515-01aa-4519-853d-93552c423dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_3(features3_dir, output_path, ge_path, class_count_dir, property_count_dir):\n",
    "    file_list = glob.glob(features3_dir + '/*.csv')\n",
    "    for i, file in tqdm(enumerate(file_list)):\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        filename = file.split('/')[-1]\n",
    "        output_file = f\"{output_path}/{filename}\"\n",
    "        embedding_file = f\"{ge_path}/{filename[:-4]}_graph_embedding_complex.tsv\"\n",
    "        class_count_file = f\"{class_count_dir}/{filename[:-4]}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}/{filename[:-4]}_prop_count.tsv\"\n",
    "\n",
    "        !tl mosaic-features -c kg_labels --num-char --num-tokens $file \\\n",
    "        / score-using-embedding \\\n",
    "        --column-vector-strategy centroid-of-lof \\\n",
    "        --lof-strategy pseudo-gt \\\n",
    "        -o pgt_centroid_score \\\n",
    "        --embedding-file $embedding_file \\\n",
    "        / compute-tf-idf  \\\n",
    "        --feature-file $class_count_file \\\n",
    "        --feature-name class_count \\\n",
    "        --singleton-column pseudo_gt \\\n",
    "        -o pgt_class_count_tf_idf_score \\\n",
    "        / compute-tf-idf \\\n",
    "        --feature-file $property_count_file \\\n",
    "        --feature-name property_count \\\n",
    "        --singleton-column pseudo_gt \\\n",
    "        -o pgt_property_count_tf_idf_score > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20e216-1a43-4e06-8bb2-9141842d63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3(train_features3_path, train_feature_path, train_joined_graph_embedding, train_joined_class_count, train_joined_prop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa70c9f-5e21-4319-8fca-ac1739356a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3(train_features3_path, train_feature_path, train_graph_embedding, train_class_count, train_prop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d3388-59ae-478b-8b1c-dc2fd7da96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3(dev_features3_path, dev_feature_path, dev_joined_graph_embedding, dev_joined_class_count, dev_joined_prop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ce81c-b199-45a9-9a22-1fb8dcb14cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3(dev_features3_path, dev_feature_path, dev_graph_embedding, dev_class_count, dev_prop_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-carrier",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b084e84-ab19-4215-8da2-3683a118e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\n",
    "#             \"monge_elkan\",\"monge_elkan_aliases\",\"jaro_winkler\",\n",
    "#             \"levenshtein\",\"singleton\",\"context_score_3\",\n",
    "#             \"pgt_centroid_score\",\"pgt_class_count_tf_idf_score\",\n",
    "#             \"pgt_property_count_tf_idf_score\", \"num_tokens\", \"num_char\"\n",
    "#             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30a0df-6040-4071-acf0-b1b2a99e59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"monge_elkan\",\"monge_elkan_aliases\",\"jaro_winkler\",\n",
    "            \"levenshtein\",\"singleton\",\"context_score_3\",\"pgt_centroid_score\",\"pgt_class_count_tf_idf_score\",\n",
    "            \"pgt_property_count_tf_idf_score\", \"num_occurences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    df_list  = []\n",
    "    for fn in glob.glob(f\"{datapath}/*csv\"):\n",
    "        if os.path.getsize(fn) == 0:\n",
    "            continue\n",
    "        fid = fn.split('/')[-1][:-4]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df['context_score'].fillna(0.0, inplace=True)\n",
    "        df_list.append(df)            \n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cebe14-8fd3-4c52-bf00-79f9a96752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['evaluation_label']\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(glob.glob(args.train_path + '/*.csv')):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "        d_sample = pd.read_csv(file)\n",
    "#         d_sample['context_score'].fillna(0.0, inplace=True)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            pos_rows = cell[1][cell[1]['evaluation_label'].astype(int) == 1][features].to_numpy()\n",
    "            for i in range(len(pos_rows)):\n",
    "                pos_features.append(pos_rows[i])\n",
    "            neg_rows = cell[1][cell[1]['evaluation_label'].astype(int) == -1][features].to_numpy()\n",
    "            for i in range(min(100,len(neg_rows))):\n",
    "                neg_features.append(neg_rows[i])\n",
    "            random.shuffle(pos_features)\n",
    "            random.shuffle(neg_features)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "            \n",
    "    print(len(positive_features_final), len(positive_features_final[3]))\n",
    "    print(len(negative_features_final), len(negative_features_final[3]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_feature_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ebe59-63d5-4516-bdc9-334a3e0b48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb42916-c088-4de8-ae14-29fe3f78d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26866d65-3a57-49d9-808d-024d9c6427b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_train_data(gen_training_data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aaa0d-754f-481f-8913-dde5b9ea0dd0",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-lying",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    normalize_features = features\n",
    "    for file in glob.glob(input_table_path + '/*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                continue\n",
    "                \n",
    "        d_sample = pd.read_csv(file)\n",
    "#         d_sample['context_score'].fillna(0.0, inplace=True)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        new_df_list = []\n",
    "        pred = []\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            sorted_df = cell[1].sort_values('context_score', ascending=False)\n",
    "            sorted_df_features = sorted_df[normalize_features]\n",
    "            new_df_list.append(sorted_df)\n",
    "            arr = sorted_df_features.to_numpy()\n",
    "            test_inp = []\n",
    "            for a in arr:\n",
    "                test_inp.append(a)\n",
    "            test_tensor = torch.tensor(test_inp).float()\n",
    "            scores = model.predict(test_tensor)\n",
    "            scores_list = torch.squeeze(scores).tolist()\n",
    "            if not type(scores_list) is list:\n",
    "                pred.append(scores_list)\n",
    "            else:\n",
    "                pred.extend(scores_list)\n",
    "        test_df = pd.concat(new_df_list)\n",
    "        test_df[final_score_column] = pred\n",
    "        test_df.to_csv(f\"{output_table_path}/{file_name}\", index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, final_score_column)\n",
    "        top1_precision = res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            best_model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = os.path.join(dirpath, fn)\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "                    \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_feature_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call Training\n",
    "best_model_path = train(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-holocaust",
   "metadata": {},
   "source": [
    "## Dev Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_prediction(dev_feature_path, dev_predictions_top_k, saved_model, output_column, min_max_scaler_path, k=5):\n",
    "    for file in glob.glob(dev_feature_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        feature_str =  \",\".join(features)\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        # location where the output generated by the predictions wil be stored.\n",
    "        dev_output = f\"{dev_predictions_top_k}/{filename}\"\n",
    "        !tl --log-file $tl_log_file predict-using-model $file -o $output_column \\\n",
    "            --features $feature_str \\\n",
    "            --ranking-model $saved_model \\\n",
    "            --normalization-factor $min_max_scaler_path \\\n",
    "            / get-kg-links -c $final_score_column -k $k --k-rows \\\n",
    "            > $dev_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_color(dev_predictions_top_k, dev_colorized_path, score_column, k=5):\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_color_file = f\"{dev_colorized_path}/{filename[:-4]}.xlsx\"\n",
    "        !tl add-color $file -c \"$score_column,evaluation_label\" -k $k --output $dev_color_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dev_predictions_top_k, dev_predictions_metrics, score_column, k=5):\n",
    "    df_list = []\n",
    "    for file in glob.glob(dev_predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "                \n",
    "        dev_metrics_file = f\"{dev_predictions_metrics}/{filename}\"\n",
    "        !tl --log-file $tl_log_file metrics $file -k $k -c $score_column --tag $filename> $dev_metrics_file\n",
    "        df_list.append(pd.read_csv(dev_metrics_file))\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686c3e4-a7c0-49aa-9a7e-f9d80a2f6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_custom_metrics(dev_predictions_top_k):\n",
    "    df_list = []\n",
    "    for file in glob.glob(dev_predictions_top_k+\"/*.csv\"):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        df = pd.read_csv(file)\n",
    "        col_df = []\n",
    "        for col, coldf in df.groupby(by=[\"column\"]):\n",
    "            rows = 0\n",
    "            pgt_rows = 0\n",
    "            pgt_recall = 0\n",
    "            unignored_rows = 0\n",
    "            unignored_candidates = 0\n",
    "            unignored_correct = 0\n",
    "            ignored_correct = 0\n",
    "            kth_perc_rows = 0\n",
    "            kth_perc_correct = 0\n",
    "            kth_perc_candidates = 0\n",
    "            for row, rowdf in coldf.groupby(by=[\"row\"]):\n",
    "                rows += 1\n",
    "                p_count = rowdf[(rowdf[\"pseudo_gt\"] == 1)].shape[0]\n",
    "                if p_count > 0:\n",
    "                    pgt_rows += 1\n",
    "                p_recall = rowdf[((rowdf[\"pseudo_gt\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                pgt_recall += p_recall\n",
    "                ignore_0_count = rowdf[rowdf[\"ignore_candidate\"] == 0].shape[0]\n",
    "                if ignore_0_count > 0:\n",
    "                    unignored_rows += 1\n",
    "                unignored_candidates += ignore_0_count\n",
    "                unignored_correct += rowdf[((rowdf[\"ignore_candidate\"] == 0) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                ignored_correct += rowdf[((rowdf[\"ignore_candidate\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "                kth_perc_1_count = rowdf[rowdf[\"kth_percenter\"] == 1].shape[0]\n",
    "                if kth_perc_1_count > 0:\n",
    "                    kth_perc_rows += 1\n",
    "                kth_perc_candidates += kth_perc_1_count\n",
    "                kth_perc_correct += rowdf[((rowdf[\"kth_percenter\"] == 1) & (rowdf[\"evaluation_label\"] == 1))].shape[0]\n",
    "            unique_rows = coldf[\"label\"].nunique()\n",
    "            col_df.append(pd.DataFrame([{\n",
    "                \"filename\":filename,\n",
    "                \"column\": col,\n",
    "                \"rows\": rows,\n",
    "                \"unique_rows\": unique_rows,\n",
    "                \"pgt_rows\": pgt_rows,\n",
    "                \"pgt_recall\": pgt_recall,\n",
    "                \"pgt_accuracy\": pgt_recall/pgt_rows if pgt_rows!=0 else 0,\n",
    "                \"unignored_rows\": unignored_rows,\n",
    "                \"unignored_candidates\": unignored_candidates,\n",
    "                \"unignored_correct\": unignored_correct,\n",
    "                \"ignored_correct\": ignored_correct,\n",
    "                \"ignore_candidate_accuracy\": unignored_correct/unignored_rows if unignored_rows != 0 else 0,\n",
    "                \"kth_percenter_rows\": kth_perc_rows,\n",
    "                \"kth_percenter_candidates\": kth_perc_candidates,\n",
    "                \"kth_percenter_correct\": kth_perc_correct,\n",
    "                \"kth_percenter_accuracy\": kth_perc_correct/kth_perc_rows\n",
    "            }]))\n",
    "        df_list.append(pd.concat(col_df))\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad2408-0dbd-4142-b9ec-4b701776c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_feature_path, dev_predictions_top_k, best_model_path, final_score_column, min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prediction(dev_feature_path, dev_predictions_top_k, best_model_path, final_score_column, min_max_scaler_path, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = compute_metrics(dev_predictions_top_k, dev_metrics_path, final_score_column, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{dev_metrics_path}/metrics_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e251-61fa-445f-9802-923fbbbce018",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcd8be-dd14-4eec-aeb4-0efe88917997",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1b790-1174-4cf7-888d-88bb82d5070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_color(dev_predictions_top_k, dev_colorized_path, final_score_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_correct_candidates(candidates_path, missing_correct_candidates_path):\n",
    "     for file in tqdm(glob.glob(candidates_path + '/*.csv')):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        missing_file = f\"{missing_correct_candidates_path}/{filename}\"\n",
    "        !tl check-candidates \"$file\" > \"$missing_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_correct_candidates(dev_candidate_path, dev_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(files_path):\n",
    "    df_list = []\n",
    "    for file in glob.glob(files_path + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        df = pd.read_csv(file)\n",
    "        df['filename'] = filename\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = concat_files(dev_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.to_csv(f\"{dev_missing_candidates_path}/missing_concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_correct_candidates(train_candidate_path, train_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df = concat_files(train_missing_candidates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing_df.to_csv(f\"{train_missing_candidates_path}/missing_concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93413abb-6d55-48b0-875e-921f8cc05b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(predictions_top_k):\n",
    "    analysis = []\n",
    "    for file in glob.glob(predictions_top_k + '/*.csv'):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "                    continue\n",
    "        df = pd.read_csv(file)\n",
    "        df.fillna(0.0, inplace=True)\n",
    "        col_grpd = df.groupby(by=['column'])\n",
    "        for col, cgdf in col_grpd:\n",
    "            cs_avg = cgdf[cgdf['evaluation_label'] == 1]['context_score_3'].mean()\n",
    "            pgt_centroid_score_avg = cgdf[cgdf['evaluation_label'] == 1]['pgt_centroid_score'].mean()\n",
    "            pgt_class_count_tf_idf_avg = cgdf[cgdf['evaluation_label'] == 1]['pgt_class_count_tf_idf_score'].mean()\n",
    "            pgt_prop_count_tf_idf_avg = cgdf[cgdf['evaluation_label'] == 1]['pgt_property_count_tf_idf_score'].mean()\n",
    "            row_grpd = cgdf.groupby(by=['row'])\n",
    "            for row, rgdf in row_grpd:\n",
    "                rgdf = rgdf[rgdf['rank'].astype(int) == 1]\n",
    "                if filename == '0QWF60VG.csv' and col == 0 and row == 0:\n",
    "                    print(rgdf['kg_id'], rgdf['GT_kg_id'], rgdf['evaluation_label'])\n",
    "                for i, r in rgdf.iterrows():\n",
    "                    if r['evaluation_label'] == -1 and r['kg_id'] != r['GT_kg_id']:\n",
    "                        analysis.append({\n",
    "                            'filename': filename,\n",
    "                            'column': col,\n",
    "                            'row': row,\n",
    "                            'kg_id': r['kg_id'],\n",
    "                            'label': r['label'],\n",
    "                            'GT_kg_id': r['GT_kg_id'],\n",
    "                            'GT_kg_label': r['GT_kg_label'],\n",
    "                            'context_score_3_average': cs_avg,\n",
    "                            'context_score_3': r['context_score_3'],\n",
    "                            'pgt_centroid_score_avg': pgt_centroid_score_avg,\n",
    "                            'pgt_centroid_score': r['pgt_centroid_score'],\n",
    "                            'pgt_class_count_tf_idf_avg': pgt_class_count_tf_idf_avg,\n",
    "                            'pgt_class_count_tf_idf_score': r['pgt_class_count_tf_idf_score'],\n",
    "                            'pgt_property_count_tf_idf_avg': pgt_prop_count_tf_idf_avg,\n",
    "                            'pgt_property_count_tf_idf_score': r['pgt_property_count_tf_idf_score'],\n",
    "                            'siamese_prediction': r['siamese_prediction'],\n",
    "                            'best_str_similarity': r['best_str_similarity'],\n",
    "                            'ignore_candidate': r['ignore_candidate'],\n",
    "                            'kth_percenter': r['kth_percenter'],\n",
    "                            'pseudo_gt': r['pseudo_gt']\n",
    "                        })\n",
    "    return pd.DataFrame(analysis).sort_values(by=['filename', 'column', 'row'], ascending=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da81686-ee36-4383-8f21-1c633d42ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis(dev_predictions_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c0f37-4fe8-43d3-afbf-93f62b82b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5cf7c8-30ab-4267-8739-66e96da08c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.to_csv(dev_incorrect_answers_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d52250-6ed0-4c0a-b51d-fba05866dd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
