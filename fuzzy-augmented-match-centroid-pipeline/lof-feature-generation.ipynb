{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = '/Users/summ7t/dev/novartis/table-linker/t2dv2-candidates-april-28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate lof-graph-embedding-score for any table\n",
    "\n",
    "Required datasets\n",
    "- candidate file\n",
    "- candidate feature file\n",
    "- graph_embedding_complex.tsv (generated and stored during candidate generation)\n",
    "\n",
    "Script used `lof-script.sh`\n",
    "\n",
    "```\n",
    "filename=$1\n",
    "tsv_postfix=_graph_embedding_complex\n",
    "\n",
    "tl smallest-qnode-number train-candidates/candidates-$filename.csv \\\n",
    "/ align-page-rank \\\n",
    "/ string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan \\\n",
    "/ string-similarity -i --method jaccard:tokenizer=word -c kg_descriptions context -o des_cont_jaccard \\\n",
    "/ normalize-scores -c des_cont_jaccard \\\n",
    "/ vote-by-classifier --prob-threshold 0.995 --model weighted_lr.pkl \\\n",
    "> model-voted/$filename.csv\n",
    "\n",
    "tl score-using-embedding model-voted/$filename.csv \\\n",
    "--column-vector-strategy centroid-of-lof \\\n",
    "--lof-strategy ems-mv \\\n",
    "-o graph-embedding-score \\\n",
    "--embedding-file train-graph-embeddings/$filename$tsv_postfix.tsv \\\n",
    "--embedding-url http://ckg07:9200/wikidatadwd-augmented/ \\\n",
    "> lof-score/$filename.csv\n",
    "```\n",
    "\n",
    "cmd: `bash {HOME_DIR}/lof-script.sh {fid}`\n",
    "\n",
    "output: lof-score/$filename.csv contains `is_lof` and `graph-embedding-score` (centroid-of-lof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $HOME_DIR/model-voted\n",
    "!mkdir -p $HOME_DIR/lof-score\n",
    "!mkdir -p $HOME_DIR/merged-lof-score\n",
    "!mkdir -p $HOME_DIR/final-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in candidates dir\n",
    "file_names = []\n",
    "file_ids = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(f'{HOME_DIR}/train-candidates/'):\n",
    "    for fn in filenames:\n",
    "        if \"csv\" not in fn:\n",
    "            continue\n",
    "        abs_fn = dirpath + fn\n",
    "        assert os.path.isfile(abs_fn)\n",
    "        if os.path.getsize(abs_fn) == 0:\n",
    "            continue\n",
    "        file_names.append(abs_fn)\n",
    "        file_ids.append(fn.split('.csv')[0].split('candidates-')[1])\n",
    "len(file_names), file_ids[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, fid in enumerate(file_ids):\n",
    "    print(f\"Generating score for {idx}th file: {fid}...\")\n",
    "    os.system(f'bash {HOME_DIR}/lof-script.sh {fid}')\n",
    "    assert os.path.isfile(f'{HOME_DIR}/model-voted/{fid}.csv'), f\"Something wrong with model-voted result: {idx}th file: {fid}\"\n",
    "    assert os.path.isfile(f'{HOME_DIR}/lof-score/{fid}.csv'), f\"Something wrong with lof-score result: {idx}th file: {fid}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model-voted and lof-score files\n",
    "fid = '88523363_0_8180214313099580515'\n",
    "model_voted_df = pd.read_csv(f'{HOME_DIR}/model-voted/{fid}.csv')\n",
    "model_voted_df[model_voted_df['vote_by_classifier'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = '88523363_0_8180214313099580515'\n",
    "score_df = pd.read_csv(f'{HOME_DIR}/lof-score/{fid}.csv')\n",
    "score_df.sort_values(by=['graph-embedding-score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lof candidate (graph-embedding-score) with candidate feature file\n",
    "for idx, fid in enumerate(file_ids):\n",
    "    print(f\"Merging embedding score for {idx}th file: {fid}...\")\n",
    "    train_features_df = pd.read_csv(f'{HOME_DIR}/train-features/{fid}.csv')\n",
    "    lof_score_df = pd.read_csv(f'{HOME_DIR}/lof-score/{fid}.csv')\n",
    "    lof_score_df.rename(columns = {'graph-embedding-score':'lof-graph-embedding-score'}, inplace = True)\n",
    "    trimmed_lof_score_df = lof_score_df.loc[:, ['column', 'row', 'kg_id', 'lof-graph-embedding-score']]\n",
    "    \n",
    "    # merge two df on row, column, kg_id\n",
    "    final_df = pd.merge(train_features_df, trimmed_lof_score_df, left_on=['column', 'row', 'kg_id'], right_on = ['column', 'row', 'kg_id'])\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "    assert len(final_df) == len(train_features_df), f\"{len(train_features_df)}, {len(final_df)}\"\n",
    "    \n",
    "    final_df.to_csv(f\"{HOME_DIR}/merged-lof-score/{fid}.csv\", index=False)\n",
    "    assert os.path.isfile(f'{HOME_DIR}/merged-lof-score/{fid}.csv'), f\"Something wrong with merged score result: {idx}th file: {fid}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check merged train feature files\n",
    "fid = '88523363_0_8180214313099580515'\n",
    "merged_score_df = pd.read_csv(f'{HOME_DIR}/merged-lof-score/{fid}.csv')\n",
    "merged_score_df.sort_values(by=['lof-graph-embedding-score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lof-reciprocal-rank\n",
    "for idx, fid in enumerate(file_ids):\n",
    "    print(f\"generating final feature for {idx}th file: {fid}\")\n",
    "    merged_lof_f = f'{HOME_DIR}/merged-lof-score/{fid}.csv'\n",
    "    final_features_f = f'{HOME_DIR}/final-features/{fid}.csv'\n",
    "    script = f\"\"\"\n",
    "    tl generate-reciprocal-rank {merged_lof_f} \\\n",
    "    -c lof-graph-embedding-score \\\n",
    "    -o lof-reciprocal-rank \\\n",
    "    > {final_features_f}\n",
    "    \"\"\"\n",
    "    os.system(script)\n",
    "    assert os.path.isfile(final_features_f), f\"Something wrong with final feature result: {idx}th file: {fid}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final feature files\n",
    "fid = '88523363_0_8180214313099580515'\n",
    "final_feature_df = pd.read_csv(f'{HOME_DIR}/final-features/{fid}.csv')\n",
    "final_feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of lof-graph-embedding-score\n",
    "- baseline: graph-embedding-score (centroid-of-singleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use top 1/5 accuracy\n",
    "def embedding_eval(eval_file):\n",
    "    assert \"graph-embedding-score\" in eval_file\n",
    "    assert \"lof-graph-embedding-score\" in eval_file\n",
    "    \n",
    "    cos_top1_count = 0\n",
    "    cos_top5_count = 0\n",
    "    lof_top1_count = 0\n",
    "    lof_top5_count = 0\n",
    "    all_count = 0\n",
    "    \n",
    "    for ((col, row), group) in eval_file.groupby(['column', 'row']):\n",
    "        all_count += 1\n",
    "        \n",
    "        # sort by centroid-of-singleton embedding score\n",
    "        eval_labels = group.sort_values(by=['graph-embedding-score'], ascending=False)['evaluation_label']\n",
    "        if eval_labels.iloc[0] == 1:\n",
    "            cos_top1_count += 1\n",
    "        if 1 in eval_labels.iloc[:5].values:\n",
    "            cos_top5_count += 1\n",
    "            \n",
    "        # sort by centroid-of-lof embedding score\n",
    "        eval_labels = group.sort_values(by=['lof-graph-embedding-score'], ascending=False)['evaluation_label']\n",
    "        if eval_labels.iloc[0] == 1:\n",
    "            lof_top1_count += 1\n",
    "        if 1 in eval_labels.iloc[:5].values:\n",
    "            lof_top5_count += 1\n",
    "    \n",
    "    return {\n",
    "        'cos_top1_accuracy': cos_top1_count / all_count, \n",
    "        'cos_top5_accuracy': cos_top5_count / all_count, \n",
    "        'lof_top1_accuracy': lof_top1_count / all_count, \n",
    "        'lof_top5_accuracy': lof_top5_count / all_count,\n",
    "        'all_count': all_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_top_accuracy = {}\n",
    "for fid in file_ids:\n",
    "    final_df = pd.read_csv(f\"{HOME_DIR}/merged-lof-score/{fid}.csv\")\n",
    "    res_top_accuracy[fid] = embedding_eval(final_df)\n",
    "res_top_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_accuracy_df = pd.DataFrame(res_top_accuracy)\n",
    "top_accuracy_df = top_accuracy_df.transpose()\n",
    "len(top_accuracy_df[top_accuracy_df['lof_top1_accuracy'] < top_accuracy_df['cos_top1_accuracy']]), \\\n",
    "len(top_accuracy_df[top_accuracy_df['lof_top5_accuracy'] < top_accuracy_df['cos_top5_accuracy']]), \\\n",
    "len(top_accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embedding-score difference\n",
    "def highlight_greaterthan_1(x):\n",
    "    if x.lof_top1_accuracy < x.cos_top1_accuracy:\n",
    "        return ['background-color: yellow']*5\n",
    "    else:\n",
    "        return ['background-color: white']*5\n",
    "    \n",
    "top_accuracy_df.style.apply(highlight_greaterthan_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
